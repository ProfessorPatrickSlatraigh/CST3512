{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CST3512/blob/main/CST3512_NLP_Intro_Spring_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Natural Language Process    \n",
        "\n",
        "*CST3512 Data and Information Management - II*\n",
        "\n",
        "The content of this notebook is derived from several sources including:\n",
        "*  **Basic Concepts of Natural Language Processing (NLP) Models and Python Implementation** by Prasun Biswas in [Toward Data Science](https://towardsdatascience.com/basic-concepts-of-natural-language-processing-nlp-models-and-python-implementation-88a589ce1fc0), 01-Jul-2021   \n",
        "*  **What Is the Difference Between Stemming and Lemmatization?** from [StackOverflow](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)    \n",
        "*  **Beginners Guide to Stemming in Python NLTK** from [MachineLearningKnowldge.ai](https://machinelearningknowledge.ai/beginners-guide-to-stemming-in-python-nltk/)    \n"
      ],
      "metadata": {
        "id": "OUmE5DGjvJa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a human it’s pretty easy to understand language but machines are not capable to recognize it easily.  Natural Language Processing (NLP) enables computers to interpret and to understand the way humans communicate using language.  But NLP does not interpret language the same way humans understand language."
      ],
      "metadata": {
        "id": "LtMik5YvvYv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. This notebook steps through the basics of NLP using the NLTK in Python."
      ],
      "metadata": {
        "id": "EX7Agag9w7Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Housekeeping    \n",
        "\n",
        "First, import requisite libraries and access desired data sources    \n"
      ],
      "metadata": {
        "id": "TZptTrUb118i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup wordcloud and nltk\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk"
      ],
      "metadata": {
        "id": "UIrY026I2Bgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and import book sources from nltk\n",
        "nltk.download('book') \n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "27ggUk4D2Sbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stopwords for English language from NLTK\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "3bB7WyYKTQ7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get WordTokenize to split text into word tokens \n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "7lVKq3sevg4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Lemmatizer and stemmere (Porter, Snowball, Lancaster, Regexp) from NLTK\n",
        "# The Snowball Stemmer may be preferred as it is more modern\n",
        "\n",
        "# Get PorterStemmer from NLTK\n",
        "porter_stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# Get SnowballStemmer from NLTK\n",
        "# SnowballStemmer needs a language parameter set\n",
        "snowball_stemmer = nltk.stem.SnowballStemmer(language='english')\n",
        "\n",
        "# Get LancasterStemmer from NLTK\n",
        "lancaster_stemmer = nltk.stem.LancasterStemmer()\n",
        "\n",
        "# Get the RegexpStemmer from NLTK\n",
        "from nltk.stem import RegexpStemmer\n",
        "# Instances of RegexpStemmer require a regex as a positional argument\n",
        "# RegexpStemmer calls may also provide a kwarg for min(imum) length\n",
        "regexp_stemmer = nltk.stem.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "# Get Lemmatizer from NLTK\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "GG3E04-c2h2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get tagging capability for parts of speech using spacy library\n",
        "import spacy\n",
        "# Get Tokenizer class from spacy module\n",
        "from spacy.tokenizer import Tokenizer\n",
        "# Instantiate an nlp object with \"en_core_web_sm\" argument\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "41nUi-3GTjz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import regular expressions for searches and filtering\n",
        "import re"
      ],
      "metadata": {
        "id": "ZoXe9SAs4Qmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural languages are a free form of text which are unstructured in nature. Cleaning and preparing the text data to extract features is very important for an NLP approach to develop any model(s). This notebook covers the basic but important steps and shows how to implement them in Python using NLTK and other packages with the goal of developing an NLP-based classification model."
      ],
      "metadata": {
        "id": "oC21TftYxMMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following phases of the approach will be described:\n",
        "\n",
        "A. Data Cleaning and Tokenization        \n",
        "B. Vectorization/Word Embedding    \n",
        "C. Model Development"
      ],
      "metadata": {
        "id": "tAvsJ4eVxyXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Data Cleaning    \n",
        "\n",
        "Data cleaning is a basic but very important step in NLP. This section includes a few steps in an approach for data cleaning.  Depending on the source and nature of the text data being analyzed, new steps may be added and the steps considered may be refined.  As with any data science analysis, a continuos improvement approach is recommended where learnings are applied to every step in the approach and not just to final analysis and presentation.     \n"
      ],
      "metadata": {
        "id": "ghlJwIb_yy7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Line of Text**\n",
        "\n",
        "For this exercise a simple line of text is created through the assignment of a string to the variable `line`"
      ],
      "metadata": {
        "id": "YQmNweQD4goW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line = 'Reaching out for HELP. Please meet me in LONDON at 6 a.m xyz@abc.com #urgent!'"
      ],
      "metadata": {
        "id": "E63DGFOg4nm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line = \"\"\"  Four score and seven years ago our fathers brought forth on this\n",
        "continent a new nation, conceived in liberty and dedicated to the\n",
        "proposition that all men are created equal. Now we are engaged in\n",
        "a great civil war, testing whether that nation or any nation so\n",
        "conceived and so dedicated can long endure. We are met on a great\n",
        "battlefield of that war. We have come to dedicate a portion of\n",
        "that field as a final resting-place for those who here gave their\n",
        "lives that that nation might live. It is altogether fitting and\n",
        "proper that we should do this. But in a larger sense, we cannot\n",
        "dedicate, we cannot consecrate, we cannot hallow this ground.\n",
        "The brave men, living and dead who struggled here have consecrated\n",
        "it far above our poor power to add or detract. The world will\n",
        "little note nor long remember what we say here, but it can never\n",
        "forget what they did here. It is for us the living rather to be\n",
        "dedicated here to the unfinished work which they who fought here\n",
        "have thus far so nobly advanced. It is rather for us to be here\n",
        "dedicated to the great task remaining before us--that from these\n",
        "honored dead we take increased devotion to that cause for which\n",
        "they gave the last full measure of devotion--that we here highly\n",
        "resolve that these dead shall not have died in vain, that this\n",
        "nation under God shall have a new birth of freedom, and that\n",
        "government of the people, by the people, for the people shall\n",
        "not perish from the earth. \"\"\""
      ],
      "metadata": {
        "id": "iUAi9r7R5h29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Transform to lower case    \n",
        "\n",
        "Convert all the text to lowercase to maintain uniformity in subsequent analysis.  Of course, all text could just as easily be converted to upper case, but it is generally accepted practice to work with text all in lower case.    \n"
      ],
      "metadata": {
        "id": "5FWWXOSP6KRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display text before transformation to lower case\n",
        "print(line)\n",
        "# Transform text to lower case\n",
        "line = line.lower()\n",
        "# Display text after transformation to lower case\n",
        "print(line)"
      ],
      "metadata": {
        "id": "E5gn2Z--7DFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code example beloew works with a different example text that the preview `line` variable. A lower case version of the text to be analyzed is generated using the `.lower()` method in the following example.  If the data cleaning steps were to be performed in a logical sequence, transforming to lower case would be done first.  The two may, of course be combined as they are in the code example below.    \n"
      ],
      "metadata": {
        "id": "j-YAuBf-9tf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Tokenization    \n",
        "\n"
      ],
      "metadata": {
        "id": "tAjphGQSMBfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking text down into words (which are representative of concepts) and can be achieved through several methods.  The particular method applied can be selected and refined based upon the nature of the source of the text and the use case for the analysis.    \n",
        "\n",
        "In other data science work the fundamental task of tokenization has been performed using something as straightforward as the `split()` string method.  Three methods are presented here:    \n",
        "1. **split()**    \n",
        "2. **Regex**    \n",
        "3. **NLTK**    "
      ],
      "metadata": {
        "id": "jBlaLPg2N4Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization Using `split()` Method**  \n",
        "\n",
        "The following example assigns the text from the string `line` to a new string `split_tokenized_line` and then applies the `split()` method to the new string."
      ],
      "metadata": {
        "id": "fZs9pJF8Pqfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the split() method to tokenize a string of text\n",
        "\n",
        "# Display the text string before tokenization\n",
        "print(line)\n",
        "# Tokenize the string into a list using the .split() method\n",
        "split_tokenized_line = line.split()\n",
        "# Display the list of tokenized words from the .split() method\n",
        "print(split_tokenized_line)\n"
      ],
      "metadata": {
        "id": "XyRcrEIoN4uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice in the example above that the `split()` method is simply parsing the string into a list of distrinct strings based on occurrences of the delimiter character, which by default is a space (' ').  The `split()` [method may be given a different character as a delimiter](https://www.w3schools.com/python/ref_string_split.asp).  Regardless of the delimiter character used, parsing based on a single character results in a list of strings which may contain punctuation and other noise. "
      ],
      "metadata": {
        "id": "hqfzKM8nRPd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization Using Regular Expression (Regex)**        \n",
        "\n",
        "A bit more sophistication may be achievable by employing the use of regular expressions (regex) to perform tokenization of text into a list of word strings as the following code demonstrates.    \n"
      ],
      "metadata": {
        "id": "UBA2Oso2R5Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using regular expressions to tokenize a string of text\n",
        "\n",
        "# Display the text string before tokenization\n",
        "print(line)\n",
        "# Tokenize the string into a list using the Regexp method\n",
        "regex_tokenized_line = re.findall(\"[\\w]+\",line)\n",
        "# Display the list of tokenized words from the Regex method\n",
        "print(regex_tokenized_line)\n"
      ],
      "metadata": {
        "id": "UJW6dJn0Sdgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenized example above results in a list which more accurately repersents the words in the line of text though some content such as the time of day and email address have been fragmented into pieces which have lost the original meaning. "
      ],
      "metadata": {
        "id": "sjHZBL_NTQ0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization Using NLTK**      \n",
        "\n",
        "Because of the complexities of working with natural language it can be seen that a simple method of tokenization using the `split()` method is not adequate, nor is a somewhat more advanced method of using regular expressions.    \n",
        "\n",
        "Fortunately, the NLTK toolkit provides some more sophisticated means of tokenizaion of natural language text as the code in the following examples demonstrate.  There are three methods of tokenization available in NLTK:    \n",
        "1. NLTK Word Tokenizer    \n",
        "2. NLTK Regexp Tokenizer    \n",
        "3. NLTK WhiteSpace Tokenizer    \n"
      ],
      "metadata": {
        "id": "hvWdyBXiTmsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Word Tokenizer**    \n",
        "\n",
        "The following code provides an example of the tokenization of a string of text using the NLTK Word Tokenizer method. "
      ],
      "metadata": {
        "id": "fTPbk2zTYP2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK `word_tokenize()` to tokenize a string of text\n",
        "# This method requires the following import\n",
        "# from nltk import word_tokenize\n",
        "\n",
        "# Display the text string before tokenization\n",
        "print(line)\n",
        "# Tokenize the string into a list using the word_tokenize() method\n",
        "nltkword_tokenized_line = word_tokenize(line)\n",
        "# Display the list of tokenized words from the Regex method\n",
        "print(nltkword_tokenized_line)"
      ],
      "metadata": {
        "id": "slVgIdRiVBar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Regexp Tokenizer**    \n",
        "\n",
        "The following code provides an example of the tokenization of a string of text using the NLTK Regexp Tokenizer method. "
      ],
      "metadata": {
        "id": "fkeWSjizYlxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK RegexpTokenizer to tokenize a string of text\n",
        "\n",
        "# Display the text string before tokenization\n",
        "print(line)\n",
        "# Tokenize the string into a list using the RegexpTokenizer method\n",
        "reg_tokenizer = nltk.tokenize.RegexpTokenizer(\"[A-Za-z]+\")\n",
        "nltkregexp_tokenized_line = reg_tokenizer.tokenize(line)\n",
        "# Display the list of tokenized words from the Regex method\n",
        "print(nltkregexp_tokenized_line)"
      ],
      "metadata": {
        "id": "ywmQJScVY1Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK WhiteSpace Tokenizer**    \n",
        "\n",
        "The following code provides an example of the tokenization of a string of text using the NLTK WhiteSpace Tokenizer method. "
      ],
      "metadata": {
        "id": "xu1YDhVIZn8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK WhiteSpaceTokenizer to tokenize a string of text\n",
        "\n",
        "# Display the text string before tokenization\n",
        "print(line)\n",
        "# Tokenize the string into a list using the WhiteSpace Tokenizer method\n",
        "# Using NLTK RegexpTokenizer to tokenize a string of text\n",
        "space_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "nltkwspace_tokenized_line = space_tokenizer.tokenize(line)\n",
        "# Display the list of tokenized words from the Regex method\n",
        "print(nltkwspace_tokenized_line)"
      ],
      "metadata": {
        "id": "V-HdvWjuZtwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing the  methods of tokenization**"
      ],
      "metadata": {
        "id": "fycqeRtgVnbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original line of text:')\n",
        "print(line, '\\n')\n",
        "print('Text tokenized with split():')\n",
        "print(split_tokenized_line, '\\n')\n",
        "print('Text tokenized with Regex:')\n",
        "print(regex_tokenized_line, '\\n')\n",
        "print('Text tokenized with NLTK Word:')\n",
        "print(nltkword_tokenized_line, '\\n')\n",
        "print('Text tokenized with NLTK Regexp:')\n",
        "print(nltkregexp_tokenized_line, '\\n')\n",
        "print('Text tokenized with NLTK WhiteSpace:')\n",
        "print(nltkwspace_tokenized_line, '\\n')"
      ],
      "metadata": {
        "id": "qmkIH9RIVtTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Remove stopwords    \n",
        "\n",
        "There are words which are very commonly used but have little meaning when humans use language, those words do not add much value. Depending on the source of data and analysis to be performed, there may be words which are not required for the natural language processing approach. It is best to delete these words from the data.  The set of words to be deleted because they add little value is referred to as **stopwords** in NLP.    \n",
        "\n",
        "**Stop Words** -  a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.    \n",
        " \n",
        "We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words.     \n",
        "\n",
        "NLTK(Natural Language Toolkit) in Python has a list of stopwords stored in 16 different languages. Find them in the nltk_data directory: home/pratima/nltk_data/corpora/stopwords is the directory address.(Do not forget to change your home directory name)    \n",
        "\n",
        "\n",
        "This example focuses on English-language stopwords but, of course, every language and use case may have its own set of stopwords.\n"
      ],
      "metadata": {
        "id": "V6QBwzb44LHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using NLTK to Remove Stopwords**\n",
        "\n"
      ],
      "metadata": {
        "id": "HTG0hROCuwih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import stopwords             # already done\n",
        "# from nltk.tokenize import word_tokenize       # already done\n",
        " \n",
        "example_sentence = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        " \n",
        "# stop_words = set(stopwords.words('english'))  # deprecate for stopwords\n",
        " \n",
        "word_tokens = word_tokenize(example_sentence)\n",
        " \n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stopwords]\n",
        " \n",
        "filtered_sentence = []\n",
        " \n",
        "for w in word_tokens:\n",
        "    if w not in stopwords:\n",
        "        filtered_sentence.append(w)\n",
        " \n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "E5PO5nLGvWcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The following code extends the list of stopwords to use.*"
      ],
      "metadata": {
        "id": "pBbRzYL4xmiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional stopwords\n",
        "extra_list = ['let', 'may', 'might', 'must', 'need', 'apologies', 'meet']\n",
        "# Extending the list of stopwords\n",
        "stopwords.extend(extra_list)\n",
        "# Display the extended list of stopwords\n",
        "print('Stopwords to use:', stopwords)"
      ],
      "metadata": {
        "id": "r0gayW-v29pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the extended list of stopwords to the list."
      ],
      "metadata": {
        "id": "pDK0EcW9xvfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization and Stemming**\n",
        "\n",
        "Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
        "\n",
        "Stemming is the process of reducing the word to its word stem that affixes to suffixes and prefixes or to roots of words known as a lemma. In simple words stemming is reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem. For example – The words care, cared and caring lie under the same stem ‘care’. Stemming is important in NLP as an alternative to Lemmatizing.\n",
        "\n",
        "\n",
        "Sometimes, the same word can have multiple different Lemmas. We should identify the Part of Speech (POS) tag for the word in that specific context. Here are the examples to illustrate all the differences and use cases:\n",
        "\n",
        "1. If you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous.\n",
        "2. If you lemmatize the word 'Stripes' in verb context, it would return 'Strip'. If you lemmatize it in noun context, it would return 'Stripe'. If you just stem it, it would just return 'Strip'.\n",
        "3. You would get same results whether you lemmatize or stem words such as walking, running, swimming... to walk, run, swim etc.\n",
        "4. Lemmatization is computationally expensive since it involves look-up tables and what not.     \n",
        "\n",
        "If you have a large dataset and performance is an issue, go with Stemming. Remember you can also add your own rules to Stemming. If accuracy is paramount and dataset isn't humongous, go with Lemmatization.\n",
        "\n",
        "*source: StackOverflow, [What Is the Difference Betweem Stemming and Lemmatization?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)*\n"
      ],
      "metadata": {
        "id": "rLs1PwBs8auT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Glossary**    \n",
        "\n",
        "* **whitespace tokenizer** - a tokenizer that splits on and discards only whitespace characters returning individual words or terms as tokens.    \n",
        "\n",
        "* **Porter Stemming** - The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the most common morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.    \n",
        "\n",
        "* **Snowball Stemmer** - This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer.    \n",
        "\n",
        "* **Lancaster Stemmer** - is simple but it tends to produce results with over stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning.    \n",
        "\n",
        "* **Regexp Stemmer** - uses regular expressions (regex) to identify morphological affixes. Any substrings that match the regular expressions will be removed.\n"
      ],
      "metadata": {
        "id": "qZxRAqc-AIX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Lemmatization    \n",
        "\n",
        "Lemmatization helps to reduce the words into a single form taking the context of the use of the word into consideration.  The resultant term from Lemmatization is called a lemma. The following example defines a function to use a Lemmatization method from NLTK.  Other methods may prove more effective depending on the data source and use case."
      ],
      "metadata": {
        "id": "dRU0xXYS9J4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to Lemmatize text and to tokenize based on whitespace\n",
        "def lemmatize_text(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "# Display the line without tokenization and Lemmatization\n",
        "print(line)\n",
        "# Invoke the function defined above and display result\n",
        "lemma_line = lemmatize_text(line) \n",
        "print(lemma_line)\n"
      ],
      "metadata": {
        "id": "e1jEgSBV_UTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Stemming    \n",
        "\n",
        "Stemming is a faster, more efficient way to attempto to reduce words into their root form. As mentioned earlier, there is a trade-off of accuracy in return for the efficiency of stemming vs. Lemmatization.  The following example defines a function to perform whitespace tokenization and stemming of a line of text then invokes that function.  In this example the NLTK Porter method of stemming is used.  Depending on the nature of the source text and the use case(s) for the analysis, different methods may be considered for stemming. "
      ],
      "metadata": {
        "id": "-AEd9W1OBuOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Stemming**"
      ],
      "metadata": {
        "id": "yUM61ARbHeOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Porter stemming of text tokenized for whitespaces\n",
        "def stem_porter(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    ps = nltk.PorterStemmer()\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Display the original line\n",
        "print(line)\n",
        "# Invoke the method defined for Porter stemming and display the result\n",
        "porter_stem_line = stem_porter(line)\n",
        "print(porter_stem_line)"
      ],
      "metadata": {
        "id": "uq0X8IGJBvrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemming**    \n",
        "\n",
        "A few common rules of Snowball stemming are:    \n",
        "* ILY  -----> ILI    \n",
        "* LY   -----> Nill    \n",
        "* SS   -----> SS    \n",
        "* S    -----> Nill    \n",
        "* ED   -----> E,Nill    \n",
        "\n",
        "**Nill** means the suffix is replaced with nothing and is just removed.    \n",
        "\n",
        "There may be cases where these rules vary depending on the words. As in the case of the suffix ‘ed’ if the words are ‘cared’ and ‘bumped’ they will be stemmed as ‘care‘ and ‘bump‘. Hence, here in cared the suffix is considered as ‘d’ only and not ‘ed’. \n",
        "\n",
        "The word ‘stemmed‘ is replaced with the word ‘stem‘ and not ‘stemmed‘. Therefore, the suffix depends on the word.\n",
        "\n",
        "Here are a few examples:-\n",
        "```\n",
        "WORD           STEM\n",
        "cared          care\n",
        "university     univers\n",
        "fairly         fair\n",
        "easily         easili\n",
        "singing        sing\n",
        "sings          sing\n",
        "sung           sung\n",
        "singer         singer\n",
        "sportingly     sport\n",
        "```"
      ],
      "metadata": {
        "id": "_15H2iHrBDP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Snowball stemming of text tokenized for whitespaces\n",
        "def stem_snowball(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Snowball stemmer needs language defined\n",
        "    ps = nltk.SnowballStemmer(language='english')\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Display the original line\n",
        "print(line)\n",
        "# Invoke the method defined for Snowball stemming and display the result\n",
        "snowball_stem_line = stem_snowball(line)\n",
        "print(snowball_stem_line)"
      ],
      "metadata": {
        "id": "pXGICkO4IiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**    \n",
        "\n",
        "There is a third method of stemming available, Lancaster Stemming. \n",
        "\n",
        "Lancaster Stemmer is simple but it tends to produce results with over stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning.\n",
        "\n",
        "In NLTK, there is a module LancasterStemmer() that supports the Lancaster stemming algorithm.  The following code shows an example of the application of Lancaster Stemming along with whitespace tokenization.    \n"
      ],
      "metadata": {
        "id": "jfAgw67fJxy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Lancaster stemming of text tokenized for whitespaces\n",
        "def stem_lancaster(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Lancaster stemmer needs language defined\n",
        "    ps = nltk.LancasterStemmer()\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Display the original line\n",
        "print(line)\n",
        "# Invoke the method defined for Lancaster stemming and display the result\n",
        "lancaster_stem_line = stem_lancaster(line)\n",
        "print(lancaster_stem_line)"
      ],
      "metadata": {
        "id": "mUPFNR_AKki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer**\n",
        "\n",
        "Regex stemmer uses regular expressions to identify morphological affixes. Any substrings that match the regular expressions will be removed.\n",
        "\n",
        "In NLTK, there is a module `RegexpStemmer()` that supports the Regex stemming algorithm."
      ],
      "metadata": {
        "id": "7NJQFwOYMHaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Regexp stemming of text tokenized for whitespaces\n",
        "def stem_regexp(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Regexp stemmer needs a regex defined\n",
        "    ps = nltk.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Display the original line\n",
        "print(line)\n",
        "# Invoke the method defined for Regexp stemming and display the result\n",
        "regexp_stem_line = stem_regexp(line)\n",
        "print(regexp_stem_line)"
      ],
      "metadata": {
        "id": "Z1gGR-iVNVoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Display of the result of Lemmatization vs. the four methods of stemming compared with the original text follow.  The following code requires the execution of each of the five snippets above to create the variables to print.*"
      ],
      "metadata": {
        "id": "Ip8pUTTwNv_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatization:\")\n",
        "print(lemma_line, \"\\n\")\n",
        "print(\"Porter Stemming:\")\n",
        "print(porter_stem_line, \"\\n\")\n",
        "print(\"Snowball Stemming:\")\n",
        "print(snowball_stem_line, \"\\n\")\n",
        "print(\"Lancaster Stemming:\")\n",
        "print(lancaster_stem_line, \"\\n\")\n",
        "print(\"Regexp Stemming with `'ing$|s$|e$|able$', min=4'`:\")\n",
        "print(regexp_stem_line, \"\\n\")\n"
      ],
      "metadata": {
        "id": "yT7oueLROFbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More Comparative Examples of Stemming**    "
      ],
      "metadata": {
        "id": "1bDMPq05PsGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING EXAMPLE 1\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "id": "oDvh_Z5wPxe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING EXAMPLE 2\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "word_list = ['run','runs','running','runner','ran','easily','fairly']\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "id": "W-e38bAzP7Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Difference Between Porter Stemmer and Snowball Stemmer**\n",
        "\n",
        "Snowball Stemmer is more aggressive than Porter Stemmer.\n",
        "Some issues in Porter Stemmer were fixed in Snowball Stemmer.\n",
        "There is only a little difference in the working of these two.\n",
        "Words like ‘fairly‘ and ‘sportingly‘ were stemmed to ‘fair’ and ‘sport’ in the snowball stemmer but when you use the porter stemmer they are stemmed to ‘fairli‘ and ‘sportingli‘.    \n",
        "\n",
        "The difference between the two algorithms can be clearly seen in the way the word ‘Sportingly’ in stemmed by both. Clearly Snowball Stemmer stems it to a more accurate stem.\n",
        "\n",
        "**Drawbacks of Stemming**    \n",
        "\n",
        "Issues of over stemming and under stemming may lead to not so meaningful or inappropriate stems.    \n",
        "\n",
        "Stemming does not consider how the word is being used. For example – the word ‘saw‘ will be stemmed to ‘saw‘ itself but it won’t be considered whether the word is being used as a noun or a verb in the context. For this reason, Lemmatization is used as it keeps this fact in consideration and will return either ‘see’ or ‘saw’ depending on whether the word ‘saw’ was used as a verb or a noun."
      ],
      "metadata": {
        "id": "X_nbuYzzQcqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Removal of Regular Expressions   \n",
        "\n",
        "Regular expressions (regex) help to identify and to get rid of different patterns which are not required in the text.\n"
      ],
      "metadata": {
        "id": "-VugsKUTQv3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Line:\", line)\n",
        "print(\"...cleaning...\\n\")\n",
        "clean0 = line\n",
        "clean1 = re.sub('\\S*@\\S*\\s?',\" \",clean0)            # email removal\n",
        "clean2 = re.sub('\\s+',\" \",clean1)                   # new line character removal\n",
        "clean3 = re.sub(\"\\’\",\" \",clean2)                    # single quote removal\n",
        "clean4 = re.sub('_',\" \",clean3)                     # underscore removal\n",
        "clean5 = re.sub('http\\S*\\s?',\" \",clean4)            # link removal\n",
        "clean = ' '.join([i for i in clean5.split() if i.find('#') < 0]) #remove hashtag\n",
        "print(\"Clean line:\", clean)"
      ],
      "metadata": {
        "id": "4L8SUll2RAjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Parts-of-Speech (POS) tagging\n",
        "\n",
        "Parts-of-speech tagging helps to identify the parts of speech. Based on the use case one can keep or remove some of them.  Parts-of-speech may be localized by dialect and other cultural factors (profession, age, etc.)\n",
        "\n",
        "The following code is an example of parts-of-speech tagging using the `spacy` library.\n"
      ],
      "metadata": {
        "id": "raRKGOoJUR6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_spacy = nlp(line)\n",
        "for token in tokens_spacy:\n",
        "    print(token.text, ': ', token.pos_, ': ', token.is_stop)"
      ],
      "metadata": {
        "id": "b409l28CU0UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Named-Entity-Recognition (NER)    \n",
        "\n",
        "Named-entity-recognition helps to identify and categorize the different groups which includes names, places, currency etc.\n",
        "\n",
        "The following code is an example of the use of named-entity-recognition using the `spacy` module. \n",
        "\n",
        "*The following code requires execution of the prior snippet of code to create the variable `tokens_spacy`.*"
      ],
      "metadata": {
        "id": "lnWSZkUUVgIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in tokens_spacy.ents:\n",
        "    print(ent.text, ': ', ent.label_)"
      ],
      "metadata": {
        "id": "v5PZWuV-Vhkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##B. Vectorization/Word-Embedding    \n",
        "\n",
        "*More to come on this subject*    \n",
        "\n"
      ],
      "metadata": {
        "id": "Y3r4-Hpqa1L_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##C. NLP Model Development    \n",
        "\n",
        "*More to come on this subject*    \n",
        "\n"
      ],
      "metadata": {
        "id": "-WUo2NDka_O2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tsj8i_RybXme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TbwdBQDhV3bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK Chapter 1 "
      ],
      "metadata": {
        "id": "X2KvxLuwbU9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OE5PCBn9vXCz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqcb9--fNVE0"
      },
      "source": [
        "## Let us see how we can use LISTs in NLP\n",
        "\n",
        "[NLTK Chapter 1](https://www.nltk.org/book/ch01.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###setup\n",
        "\n",
        "*The following setup and imports are already done at the top of this notebook.*    \n"
      ],
      "metadata": {
        "id": "8tQgEsdQbbFa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElVkFGzHQiU"
      },
      "source": [
        "# Setup wordcloud and nltk\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdqCrNTNKbUK"
      },
      "source": [
        "nltk.download('book') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qKxEu4KOQzt"
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMS1oJzJKbIf"
      },
      "source": [
        "# Get stopwords, stemmer and lemmatizer\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ld9BaJNbjzA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkUsB2KDKbOa"
      },
      "source": [
        "type(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDxgXFxLKbRM"
      },
      "source": [
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCtu_5qsxZYn"
      },
      "source": [
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMOnL3KuKsRu"
      },
      "source": [
        "stopwords[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCgbcs7pKbWP"
      },
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']\n",
        "sent1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXI-1OdxxnH"
      },
      "source": [
        "text1[100:110]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1[0:50]"
      ],
      "metadata": {
        "id": "64FL5BPzkNxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-gJFqwZKbcx"
      },
      "source": [
        "len(sent1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4IxtzszOQmu"
      },
      "source": [
        "sent2 = ['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']\n",
        "sent3 = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRpOEOLyOQrE"
      },
      "source": [
        "sent2+sent3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZB_FSxhOQt6"
      },
      "source": [
        "sent1.append(\"Some\")\n",
        "sent1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Bj67QHOQx3"
      },
      "source": [
        "len(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrr1IqO-xFkM"
      },
      "source": [
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e_FK-bjPYkW"
      },
      "source": [
        "text1[100:110]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zl_kxLI1ln0"
      },
      "source": [
        "##[NLTK concordance](http://www.nltk.org/api/nltk.html?highlight=concordance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4"
      ],
      "metadata": {
        "id": "awnAG-PAMgrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgSt9w9Ps3-"
      },
      "source": [
        "text4.concordance(\"nation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"son\")"
      ],
      "metadata": {
        "id": "VCkeg3Xvm8M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"king\")"
      ],
      "metadata": {
        "id": "2fQqXkYqolyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"ship\")"
      ],
      "metadata": {
        "id": "jDz_aj4aooJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"knife\")"
      ],
      "metadata": {
        "id": "EkK0nk8Sop92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"monster\")"
      ],
      "metadata": {
        "id": "GccaIaPQovuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using .similar()"
      ],
      "metadata": {
        "id": "WIwkGzsVn9eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text1)"
      ],
      "metadata": {
        "id": "oo92GzTQp6KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"monster\")"
      ],
      "metadata": {
        "id": "Dkg8Fut_n_gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"king\")"
      ],
      "metadata": {
        "id": "ZmlZNkVEoFjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"knife\")"
      ],
      "metadata": {
        "id": "Yi0qqLtooNxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"son\")"
      ],
      "metadata": {
        "id": "a8z0RhYaoPBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"ship\")"
      ],
      "metadata": {
        "id": "RqVlSeM7oR6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DCMlA8U3m7ap"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PqlJAKK0JKc"
      },
      "source": [
        "print(text4)\n",
        "len(text4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc05kKoa1UOe"
      },
      "source": [
        "print(text2)\n",
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx2fI346fnPD"
      },
      "source": [
        "text2.dispersion_plot([\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\", \"Brandon\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziYklSVdf_dD"
      },
      "source": [
        "len(set(text4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Z8Gg-iiajT"
      },
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"liberty\", \"is\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eHJd7-icgR"
      },
      "source": [
        "text2.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"is\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXY-SyXdiiMC"
      },
      "source": [
        "text2.dispersion_plot([\"society\", \"husband\", \"feeling\", \"marriage\", \"independent\", \"Dashwood\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H364Zrc61uQA"
      },
      "source": [
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sil_VPA71yAm"
      },
      "source": [
        "len(set(text2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANRzeLmb13Ec"
      },
      "source": [
        "len(set(text2))/len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi4KTZHYjDyH"
      },
      "source": [
        "len(set(text3)) / len(text3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN-VR57AjGp-"
      },
      "source": [
        "len(set(text3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1zOz0cjjXi"
      },
      "source": [
        "len(set(text3)-set(stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_T3BbrymeKz"
      },
      "source": [
        "fdist1 = FreqDist(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_oqEJh28Dv"
      },
      "source": [
        "fdist1.most_common(50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Qcf8OT3DE0"
      },
      "source": [
        "fdist2 = FreqDist(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYYuUHg3tJc"
      },
      "source": [
        "len(fdist2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0rqLgN3F4H"
      },
      "source": [
        "fdist2.most_common(50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yzQUowg3PuL"
      },
      "source": [
        "Find 50 most frequent words for Sense and Sensibility that are not stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWwLAiyB3Wze"
      },
      "source": [
        "fdist2[',']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49eASpO76cSW"
      },
      "source": [
        "counter = 0\n",
        "for item in fdist2.most_common():\n",
        "  if item[0] not in stopwords:\n",
        "    print(item)\n",
        "    counter += 1\n",
        "    if (counter == 49):\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSK7Yr77NRM"
      },
      "source": [
        "170/len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "EnZk49GLnyTZ"
      }
    }
  ]
}