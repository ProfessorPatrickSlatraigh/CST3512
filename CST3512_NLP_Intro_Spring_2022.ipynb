{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CST3512_NLP-Intro_Spring-2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CST3512/blob/main/CST3512_NLP_Intro_Spring_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Natural Language Process    \n",
        "\n",
        "*CST3512 Data and Information Management - II*\n",
        "\n",
        "The content of this notebook is derived from several sources including:\n",
        "*  **Basic Concepts of Natural Language Processing (NLP) Models and Python Implementation** by Prasun Biswas in Toward Data Science, 01-Jul-2021   \n",
        "*  **What Is the Difference Between Stemming and Lemmatization?** from [StackOverflow](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)    \n",
        "*  **Beginners Guide to Stemming in Python NLTK** from [MachineLearningKnowldge.ai](https://machinelearningknowledge.ai/beginners-guide-to-stemming-in-python-nltk/)    \n"
      ],
      "metadata": {
        "id": "OUmE5DGjvJa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a human it’s pretty easy to understand language but machines are not capable to recognize it easily.  Natural Language Processing (NLP) enables computers to interpret and to understand the way humans communicate using language.  But NLP does not interpret language the same way humans understand language."
      ],
      "metadata": {
        "id": "LtMik5YvvYv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. This notebook steps through the basics of NLP using the NLTK in Python."
      ],
      "metadata": {
        "id": "EX7Agag9w7Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Housekeeping    \n",
        "\n",
        "First, import requisite libraries and access desired data sources    \n"
      ],
      "metadata": {
        "id": "TZptTrUb118i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup wordcloud and nltk\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk"
      ],
      "metadata": {
        "id": "UIrY026I2Bgs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and import book sources from nltk\n",
        "nltk.download('book') \n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "27ggUk4D2Sbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f872438-3afe-4d27-a11b-606a25077bf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n",
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stopwords for English language from NLTK\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "3bB7WyYKTQ7X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get WordTokenize to split text into word tokens \n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "7lVKq3sevg4a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Lemmatizer and stemmere (Porter, Snowball, Lancaster, Regexp) from NLTK\n",
        "# The Snowball Stemmer may be preferred as it is more modern\n",
        "\n",
        "# Get PorterStemmer from NLTK\n",
        "porter_stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# Get SnowballStemmer from NLTK\n",
        "# SnowballStemmer needs a language parameter set\n",
        "snowball_stemmer = nltk.stem.SnowballStemmer(language='english')\n",
        "\n",
        "# Get LancasterStemmer from NLTK\n",
        "lancaster_stemmer = nltk.stem.LancasterStemmer()\n",
        "\n",
        "# Get the RegexpStemmer from NLTK\n",
        "from nltk.stem import RegexpStemmer\n",
        "# Instances of RegexpStemmer require a regex as a positional argument\n",
        "# RegexpStemmer calls may also provide a kwarg for min(imum) length\n",
        "regexp_stemmer = nltk.stem.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "# Get Lemmatizer from NLTK\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "GG3E04-c2h2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get tagging capability for parts of speech using spacy library\n",
        "import spacy\n",
        "# Get Tokenizer class from spacy module\n",
        "from spacy.tokenizer import Tokenizer\n",
        "# Instantiate an nlp object with \"en_core_web_sm\" argument\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "41nUi-3GTjz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import regular expressions for searches and filtering\n",
        "import re"
      ],
      "metadata": {
        "id": "ZoXe9SAs4Qmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural languages are a free form of text which are unstructured in nature. Cleaning and preparing the text data to extract features is very important for an NLP approach to develop any model(s). This notebook covers the basic but important steps and shows how to implement them in Python using NLTK and other packages with the goal of developing an NLP-based classification model."
      ],
      "metadata": {
        "id": "oC21TftYxMMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following phases of the approach will be described:\n",
        "\n",
        "A. Data Cleaning    \n",
        "B. Tokenization    \n",
        "C. Vectorization/Word Embedding    \n",
        "D. Model Development"
      ],
      "metadata": {
        "id": "tAvsJ4eVxyXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Data Cleaning    \n",
        "\n",
        "Data cleaning is a basic but very important step in NLP. This section includes a few steps in an approach for data cleaning.  Depending on the source and nature of the text data being analyzed, new steps may be added and the steps considered may be refined.  As with any data science analysis, a continuos improvement approach is recommended where learnings are applied to every step in the approach and not just to final analysis and presentation.     \n"
      ],
      "metadata": {
        "id": "ghlJwIb_yy7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this exercise a simple line of text is created through the assignment of a string to the variable `line`"
      ],
      "metadata": {
        "id": "YQmNweQD4goW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line = 'Reaching out for HELP. Please meet me in LONDON at 6 a.m xyz@abc.com #urgent!'"
      ],
      "metadata": {
        "id": "E63DGFOg4nm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Remove stopwords    \n",
        "\n",
        "There are words which are very commonly used but have little meaning when humans use language, those words do not add much value. Depending on the source of data and analysis to be performed, there may be words which are not required for the natural language processing approach. It is best to delete these words from the data.  The set of words to be deleted because they add little value is referred to as **stopwords** in NLP.    \n",
        "\n",
        "**Stop Words** -  a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.    \n",
        " \n",
        "We would not want these words to take up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words.     \n",
        "\n",
        "NLTK(Natural Language Toolkit) in Python has a list of stopwords stored in 16 different languages. Find them in the nltk_data directory: home/pratima/nltk_data/corpora/stopwords is the directory address.(Do not forget to change your home directory name)    \n",
        "\n",
        "\n",
        "This example focuses on English-language stopwords but, of course, every language and use case may have its own set of stopwords.\n"
      ],
      "metadata": {
        "id": "V6QBwzb44LHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional stopwords\n",
        "extra_list = ['let', 'may', 'might', 'must', 'need', 'apologies', 'meet']\n",
        "# Extending the list of stopwords\n",
        "stopwords.extend(extra_list)\n",
        "# Display the extended list of stopwords\n",
        "print('Stopwords to use:', stopwords)"
      ],
      "metadata": {
        "id": "r0gayW-v29pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using NLTK to Remove Stopwords**\n",
        "\n"
      ],
      "metadata": {
        "id": "HTG0hROCuwih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from nltk.corpus import stopwords             # already done\n",
        "# from nltk.tokenize import word_tokenize       # already done\n",
        " \n",
        "example_sent = \"\"\"This is a sample sentence,\n",
        "                  showing off the stop words filtration.\"\"\"\n",
        " \n",
        "# stop_words = set(stopwords.words('english'))  # deprecate for stopwords\n",
        " \n",
        "word_tokens = word_tokenize(example_sent)\n",
        " \n",
        "filtered_sentence = [w for w in word_tokens if not w.lower() in stopwords]\n",
        " \n",
        "filtered_sentence = []\n",
        " \n",
        "for w in word_tokens:\n",
        "    if w not in stopwords:\n",
        "        filtered_sentence.append(w)\n",
        " \n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5PO5nLGvWcP",
        "outputId": "972feed3-b6ce-496a-aee6-3dc39d1c7fab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Transform to lower case    \n",
        "\n",
        "Convert all the text to lowercase to maintain uniformity in subsequent analysis.  Of course, all text could just as easily be converted to upper case, but it is generally accepted practice to work with text all in lower case.    \n"
      ],
      "metadata": {
        "id": "5FWWXOSP6KRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform text to lower case\n",
        "line = line.lower()\n",
        "# Display text after transformation to lower case\n",
        "print(line)"
      ],
      "metadata": {
        "id": "E5gn2Z--7DFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization and Stemming**\n",
        "\n",
        "Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
        "\n",
        "Stemming is the process of reducing the word to its word stem that affixes to suffixes and prefixes or to roots of words known as a lemma. In simple words stemming is reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem. For example – The words care, cared and caring lie under the same stem ‘care’. Stemming is important in NLP as an alternative to Lemmatizing.\n",
        "\n",
        "\n",
        "Sometimes, the same word can have multiple different Lemmas. We should identify the Part of Speech (POS) tag for the word in that specific context. Here are the examples to illustrate all the differences and use cases:\n",
        "\n",
        "1. If you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous.\n",
        "2. If you lemmatize the word 'Stripes' in verb context, it would return 'Strip'. If you lemmatize it in noun context, it would return 'Stripe'. If you just stem it, it would just return 'Strip'.\n",
        "3. You would get same results whether you lemmatize or stem words such as walking, running, swimming... to walk, run, swim etc.\n",
        "4. Lemmatization is computationally expensive since it involves look-up tables and what not.     \n",
        "\n",
        "If you have a large dataset and performance is an issue, go with Stemming. Remember you can also add your own rules to Stemming. If accuracy is paramount and dataset isn't humongous, go with Lemmatization.\n",
        "\n",
        "*source: StackOverflow, [What Is the Difference Betweem Stemming and Lemmatization?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)*\n"
      ],
      "metadata": {
        "id": "rLs1PwBs8auT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Glossary**    \n",
        "\n",
        "* **whitespace tokenizer** - a tokenizer that splits on and discards only whitespace characters returning individual words or terms as tokens.    \n",
        "\n",
        "* **Porter Stemming** - The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the most common morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.    \n",
        "\n",
        "* **Snowball Stemmer** - This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer.    \n",
        "\n",
        "* **Lancaster Stemmer** - is simple but it tends to produce results with over stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning.    \n",
        "\n",
        "* **Regexp Stemmer** - uses regular expressions (regex) to identify morphological affixes. Any substrings that match the regular expressions will be removed.\n"
      ],
      "metadata": {
        "id": "qZxRAqc-AIX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Lemmatization    \n",
        "\n",
        "Lemmatization helps to reduce the words into a single form taking the context of the use of the word into consideration.  The resultant term from Lemmatization is called a lemma. The following example defines a function to use a Lemmatization method from NLTK.  Other methods may prove more effective depending on the data source and use case."
      ],
      "metadata": {
        "id": "dRU0xXYS9J4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to Lemmatize text and to tokenize based on whitespace\n",
        "def lemmatize_text(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "# Invoke the function defined above and display result\n",
        "lemma_line = lemmatize_text(line) \n",
        "print(lemma_line)\n",
        "# Display the line without tokenization and Lemmatization\n",
        "print(line)"
      ],
      "metadata": {
        "id": "e1jEgSBV_UTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Stemming    \n",
        "\n",
        "Stemming is a faster, more efficient way to attempto to reduce words into their root form. As mentioned earlier, there is a trade-off of accuracy in return for the efficiency of stemming vs. Lemmatization.  The following example defines a function to perform whitespace tokenization and stemming of a line of text then invokes that function.  In this example the NLTK Porter method of stemming is used.  Depending on the nature of the source text and the use case(s) for the analysis, different methods may be considered for stemming. "
      ],
      "metadata": {
        "id": "-AEd9W1OBuOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Stemming**"
      ],
      "metadata": {
        "id": "yUM61ARbHeOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Porter stemming of text tokenized for whitespaces\n",
        "def stem_porter(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    ps = nltk.PorterStemmer()\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Porter stemming and display the result\n",
        "porter_stem_line = stem_porter(line)\n",
        "print(porter_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "uq0X8IGJBvrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemming**    \n",
        "\n",
        "A few common rules of Snowball stemming are:    \n",
        "* ILY  -----> ILI    \n",
        "* LY   -----> Nill    \n",
        "* SS   -----> SS    \n",
        "* S    -----> Nill    \n",
        "* ED   -----> E,Nill    \n",
        "\n",
        "**Nill** means the suffix is replaced with nothing and is just removed.    \n",
        "\n",
        "There may be cases where these rules vary depending on the words. As in the case of the suffix ‘ed’ if the words are ‘cared’ and ‘bumped’ they will be stemmed as ‘care‘ and ‘bump‘. Hence, here in cared the suffix is considered as ‘d’ only and not ‘ed’. \n",
        "\n",
        "The word ‘stemmed‘ is replaced with the word ‘stem‘ and not ‘stemmed‘. Therefore, the suffix depends on the word.\n",
        "\n",
        "Here are a few examples:-\n",
        "```\n",
        "WORD           STEM\n",
        "cared          care\n",
        "university     univers\n",
        "fairly         fair\n",
        "easily         easili\n",
        "singing        sing\n",
        "sings          sing\n",
        "sung           sung\n",
        "singer         singer\n",
        "sportingly     sport\n",
        "```"
      ],
      "metadata": {
        "id": "_15H2iHrBDP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Snowball stemming of text tokenized for whitespaces\n",
        "def stem_snowball(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Snowball stemmer needs language defined\n",
        "    ps = nltk.SnowballStemmer(language='english')\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Snowball stemming and display the result\n",
        "snowball_stem_line = stem_snowball(line)\n",
        "print(snowball_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "pXGICkO4IiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**    \n",
        "\n",
        "There is a third method of stemming available, Lancaster Stemming. \n",
        "\n",
        "Lancaster Stemmer is simple but it tends to produce results with over stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning.\n",
        "\n",
        "In NLTK, there is a module LancasterStemmer() that supports the Lancaster stemming algorithm.  The following code shows an example of the application of Lancaster Stemming along with whitespace tokenization.    \n"
      ],
      "metadata": {
        "id": "jfAgw67fJxy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Lancaster stemming of text tokenized for whitespaces\n",
        "def stem_lancaster(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Lancaster stemmer needs language defined\n",
        "    ps = nltk.LancasterStemmer()\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Lancaster stemming and display the result\n",
        "lancaster_stem_line = stem_lancaster(line)\n",
        "print(lancaster_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "mUPFNR_AKki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer**\n",
        "\n",
        "Regex stemmer uses regular expressions to identify morphological affixes. Any substrings that match the regular expressions will be removed.\n",
        "\n",
        "In NLTK, there is a module `RegexpStemmer()` that supports the Regex stemming algorithm."
      ],
      "metadata": {
        "id": "7NJQFwOYMHaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Regexp stemming of text tokenized for whitespaces\n",
        "def stem_regexp(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Regexp stemmer needs a regex defined\n",
        "    ps = nltk.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Regexp stemming and display the result\n",
        "regexp_stem_line = stem_regexp(line)\n",
        "print(regexp_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "Z1gGR-iVNVoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Display of the result of Lemmatization vs. the four methods of stemming compared with the original text follow.  The following code requires the execution of each of the five snippets above to create the variables to print.*"
      ],
      "metadata": {
        "id": "Ip8pUTTwNv_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lematization:\")\n",
        "print(lemma_line, \"\\n\")\n",
        "print(\"Porter Stemming:\")\n",
        "print(porter_stem_line, \"\\n\")\n",
        "print(\"Snowball Stemming:\")\n",
        "print(snowball_stem_line, \"\\n\")\n",
        "print(\"Lancaster Stemming:\")\n",
        "print(lancaster_stem_line, \"\\n\")\n",
        "print(\"Regexp Stemming with `'ing$|s$|e$|able$', min=4'`:\")\n",
        "print(regexp_stem_line, \"\\n\")\n"
      ],
      "metadata": {
        "id": "yT7oueLROFbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More Comparative Examples of Stemming**    "
      ],
      "metadata": {
        "id": "1bDMPq05PsGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING EXAMPLE 1\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "id": "oDvh_Z5wPxe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING EXAMPLE 2\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "word_list = ['run','runs','running','runner','ran','easily','fairly']\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "id": "W-e38bAzP7Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Difference Between Porter Stemmer and Snowball Stemmer**\n",
        "\n",
        "Snowball Stemmer is more aggressive than Porter Stemmer.\n",
        "Some issues in Porter Stemmer were fixed in Snowball Stemmer.\n",
        "There is only a little difference in the working of these two.\n",
        "Words like ‘fairly‘ and ‘sportingly‘ were stemmed to ‘fair’ and ‘sport’ in the snowball stemmer but when you use the porter stemmer they are stemmed to ‘fairli‘ and ‘sportingli‘.    \n",
        "\n",
        "The difference between the two algorithms can be clearly seen in the way the word ‘Sportingly’ in stemmed by both. Clearly Snowball Stemmer stems it to a more accurate stem.\n",
        "\n",
        "**Drawbacks of Stemming**    \n",
        "\n",
        "Issues of over stemming and under stemming may lead to not so meaningful or inappropriate stems.    \n",
        "\n",
        "Stemming does not consider how the word is being used. For example – the word ‘saw‘ will be stemmed to ‘saw‘ itself but it won’t be considered whether the word is being used as a noun or a verb in the context. For this reason, Lemmatization is used as it keeps this fact in consideration and will return either ‘see’ or ‘saw’ depending on whether the word ‘saw’ was used as a verb or a noun."
      ],
      "metadata": {
        "id": "X_nbuYzzQcqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Removal of Regular Expressions   \n",
        "\n",
        "Regular expressions (regex) help to identify and to get rid of different patterns which are not required in the text.\n"
      ],
      "metadata": {
        "id": "-VugsKUTQv3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Line:\", line)\n",
        "print(\"...cleaning...\\n\")\n",
        "clean0 = line\n",
        "clean1 = re.sub('\\S*@\\S*\\s?',\" \",clean0)            # email removal\n",
        "clean2 = re.sub('\\s+',\" \",clean1)                   # new line character removal\n",
        "clean3 = re.sub(\"\\’\",\" \",clean2)                    # single quote removal\n",
        "clean4 = re.sub('_',\" \",clean3)                     # underscore removal\n",
        "clean5 = re.sub('http\\S*\\s?',\" \",clean4)            # link removal\n",
        "clean = ' '.join([i for i in clean5.split() if i.find('#') < 0]) #remove hashtag\n",
        "print(\"Clean line:\", clean)"
      ],
      "metadata": {
        "id": "4L8SUll2RAjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Parts-of-Speech (POS) tagging\n",
        "\n",
        "Parts-of-speech tagging helps to identify the parts of speech. Based on the use case one can keep or remove some of them.  Parts-of-speech may be localized by dialect and other cultural factors (profession, age, etc.)\n",
        "\n",
        "The following code is an example of parts-of-speech tagging using the `spacy` library.\n"
      ],
      "metadata": {
        "id": "raRKGOoJUR6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_spacy = nlp(line)\n",
        "for token in tokens_spacy:\n",
        "    print(token.text, ': ', token.pos_, ': ', token.is_stop)"
      ],
      "metadata": {
        "id": "b409l28CU0UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Named-Entity-Recognition (NER)    \n",
        "\n",
        "Named-entity-recognition helps to identify and categorize the different groups which includes names, places, currency etc.\n",
        "\n",
        "The following code is an example of the use of named-entity-recognition using the `spacy` module. \n",
        "\n",
        "*The following code requires execution of the prior snippet of code to create the variable `tokens_spacy`."
      ],
      "metadata": {
        "id": "lnWSZkUUVgIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in tokens_spacy.ents:\n",
        "    print(ent.text, ': ', ent.label_)"
      ],
      "metadata": {
        "id": "v5PZWuV-Vhkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TbwdBQDhV3bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OE5PCBn9vXCz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqcb9--fNVE0"
      },
      "source": [
        "## Let us see how we can use LISTs in NLP\n",
        "\n",
        "[NLTK Chapter 1](https://www.nltk.org/book/ch01.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElVkFGzHQiU"
      },
      "source": [
        "# Setup wordcloud and nltk\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdqCrNTNKbUK"
      },
      "source": [
        "nltk.download('book') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qKxEu4KOQzt"
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMS1oJzJKbIf"
      },
      "source": [
        "# Get stopwords, stemmer and lemmatizer\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkUsB2KDKbOa"
      },
      "source": [
        "type(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDxgXFxLKbRM"
      },
      "source": [
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCtu_5qsxZYn"
      },
      "source": [
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMOnL3KuKsRu"
      },
      "source": [
        "stopwords[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCgbcs7pKbWP"
      },
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']\n",
        "sent1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXI-1OdxxnH"
      },
      "source": [
        "text1[100:110]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1[0:50]"
      ],
      "metadata": {
        "id": "64FL5BPzkNxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-gJFqwZKbcx"
      },
      "source": [
        "len(sent1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4IxtzszOQmu"
      },
      "source": [
        "sent2 = ['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']\n",
        "sent3 = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRpOEOLyOQrE"
      },
      "source": [
        "sent2+sent3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZB_FSxhOQt6"
      },
      "source": [
        "sent1.append(\"Some\")\n",
        "sent1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Bj67QHOQx3"
      },
      "source": [
        "len(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrr1IqO-xFkM"
      },
      "source": [
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e_FK-bjPYkW"
      },
      "source": [
        "text1[100:110]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zl_kxLI1ln0"
      },
      "source": [
        "##[NLTK concordance](http://www.nltk.org/api/nltk.html?highlight=concordance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4"
      ],
      "metadata": {
        "id": "awnAG-PAMgrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgSt9w9Ps3-"
      },
      "source": [
        "text4.concordance(\"nation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"son\")"
      ],
      "metadata": {
        "id": "VCkeg3Xvm8M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"king\")"
      ],
      "metadata": {
        "id": "2fQqXkYqolyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"ship\")"
      ],
      "metadata": {
        "id": "jDz_aj4aooJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"knife\")"
      ],
      "metadata": {
        "id": "EkK0nk8Sop92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"monster\")"
      ],
      "metadata": {
        "id": "GccaIaPQovuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using .similar()"
      ],
      "metadata": {
        "id": "WIwkGzsVn9eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text1)"
      ],
      "metadata": {
        "id": "oo92GzTQp6KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"monster\")"
      ],
      "metadata": {
        "id": "Dkg8Fut_n_gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"king\")"
      ],
      "metadata": {
        "id": "ZmlZNkVEoFjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"knife\")"
      ],
      "metadata": {
        "id": "Yi0qqLtooNxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"son\")"
      ],
      "metadata": {
        "id": "a8z0RhYaoPBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"ship\")"
      ],
      "metadata": {
        "id": "RqVlSeM7oR6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DCMlA8U3m7ap"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PqlJAKK0JKc"
      },
      "source": [
        "print(text4)\n",
        "len(text4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc05kKoa1UOe"
      },
      "source": [
        "print(text2)\n",
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx2fI346fnPD"
      },
      "source": [
        "text2.dispersion_plot([\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\", \"Brandon\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziYklSVdf_dD"
      },
      "source": [
        "len(set(text4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Z8Gg-iiajT"
      },
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"liberty\", \"is\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eHJd7-icgR"
      },
      "source": [
        "text2.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"is\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXY-SyXdiiMC"
      },
      "source": [
        "text2.dispersion_plot([\"society\", \"husband\", \"feeling\", \"marriage\", \"independent\", \"Dashwood\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H364Zrc61uQA"
      },
      "source": [
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sil_VPA71yAm"
      },
      "source": [
        "len(set(text2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANRzeLmb13Ec"
      },
      "source": [
        "len(set(text2))/len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi4KTZHYjDyH"
      },
      "source": [
        "len(set(text3)) / len(text3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN-VR57AjGp-"
      },
      "source": [
        "len(set(text3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1zOz0cjjXi"
      },
      "source": [
        "len(set(text3)-set(stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_T3BbrymeKz"
      },
      "source": [
        "fdist1 = FreqDist(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_oqEJh28Dv"
      },
      "source": [
        "fdist1.most_common(50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Qcf8OT3DE0"
      },
      "source": [
        "fdist2 = FreqDist(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYYuUHg3tJc"
      },
      "source": [
        "len(fdist2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0rqLgN3F4H"
      },
      "source": [
        "fdist2.most_common(50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yzQUowg3PuL"
      },
      "source": [
        "Find 50 most frequent words for Sense and Sensibility that are not stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWwLAiyB3Wze"
      },
      "source": [
        "fdist2[',']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49eASpO76cSW"
      },
      "source": [
        "counter = 0\n",
        "for item in fdist2.most_common():\n",
        "  if item[0] not in stopwords:\n",
        "    print(item)\n",
        "    counter += 1\n",
        "    if (counter == 49):\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSK7Yr77NRM"
      },
      "source": [
        "170/len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}