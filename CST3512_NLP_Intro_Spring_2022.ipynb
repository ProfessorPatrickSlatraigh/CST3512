{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CST3512_NLP-Intro_Spring-2022.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oqcb9--fNVE0",
        "8Zl_kxLI1ln0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CST3512/blob/main/CST3512_NLP_Intro_Spring_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Natural Language Process    \n",
        "\n",
        "*CST3512 Data and Information Management - II*\n",
        "\n",
        "The content of this notebook is derived from several sources including:\n",
        "*  **Basic Concepts of Natural Language Processing (NLP) Models and Python Implementation** by Prasun Biswas in Toward Data Science, 01-Jul-2021   \n",
        "*  **What Is the Difference Between Stemming and Lemmatization?** from [StackOverflow](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)    \n",
        "*  **Beginners Guide to Stemming in Python NLTK** from [MachineLearningKnowldge.ai](https://machinelearningknowledge.ai/beginners-guide-to-stemming-in-python-nltk/)    \n"
      ],
      "metadata": {
        "id": "OUmE5DGjvJa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a human itâ€™s pretty easy to understand language but machines are not capable to recognize it easily.  Natural Language Processing (NLP) enables computers to interpret and to understand the way humans communicate using language.  But NLP does not interpret language the same way humans understand language."
      ],
      "metadata": {
        "id": "LtMik5YvvYv8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. This notebook steps through the basics of NLP using the NLTK in Python."
      ],
      "metadata": {
        "id": "EX7Agag9w7Az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Housekeeping    \n",
        "\n",
        "First, import requisite libraries and access desired data sources    \n"
      ],
      "metadata": {
        "id": "TZptTrUb118i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup wordcloud and nltk\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk"
      ],
      "metadata": {
        "id": "UIrY026I2Bgs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and import book sources from nltk\n",
        "nltk.download('book') \n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "27ggUk4D2Sbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get stopwords for English language from NLTK\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "3bB7WyYKTQ7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Lemmatizer and stemmere (Porter, Snowball, Lancaster, Regexp) from NLTK\n",
        "# The Snowball Stemmer may be preferred as it is more modern\n",
        "\n",
        "# Get PorterStemmer from NLTK\n",
        "porter_stemmer = nltk.stem.PorterStemmer()\n",
        "\n",
        "# Get SnowballStemmer from NLTK\n",
        "# SnowballStemmer needs a language parameter set\n",
        "snowball_stemmer = nltk.stem.SnowballStemmer(language='english')\n",
        "\n",
        "# Get LancasterStemmer from NLTK\n",
        "lancaster_stemmer = nltk.stem.LancasterStemmer()\n",
        "\n",
        "# Get the RegexpStemmer from NLTK\n",
        "from nltk.stem import RegexpStemmer\n",
        "# Instances of RegexpStemmer require a regex as a positional argument\n",
        "# RegexpStemmer calls may also provide a kwarg for min(imum) length\n",
        "regexp_stemmer = nltk.stem.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "# Get Lemmatizer from NLTK\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "GG3E04-c2h2P"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get tagging capability for parts of speech using spacy library\n",
        "import spacy\n",
        "# Get Tokenizer class from spacy module\n",
        "from spacy.tokenizer import Tokenizer\n",
        "# Instantiate an nlp object with \"en_core_web_sm\" argument\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "41nUi-3GTjz-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import regular expressions for searches and filtering\n",
        "import re"
      ],
      "metadata": {
        "id": "ZoXe9SAs4Qmz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Natural languages are a free form of text which are unstructured in nature. Cleaning and preparing the text data to extract features is very important for an NLP approach to develop any model(s). This notebook covers the basic but important steps and shows how to implement them in Python using NLTK and other packages with the goal of developing an NLP-based classification model."
      ],
      "metadata": {
        "id": "oC21TftYxMMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following phases of the approach will be described:\n",
        "\n",
        "A. Data Cleaning    \n",
        "B. Tokenization    \n",
        "C. Vectorization/Word Embedding    \n",
        "D. Model Development"
      ],
      "metadata": {
        "id": "tAvsJ4eVxyXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Data Cleaning    \n",
        "\n",
        "Data cleaning is a basic but very important step in NLP. This section includes a few steps in an approach for data cleaning.  Depending on the source and nature of the text data being analyzed, new steps may be added and the steps considered may be refined.  As with any data science analysis, a continuos improvement approach is recommended where learnings are applied to every step in the approach and not just to final analysis and presentation.     \n"
      ],
      "metadata": {
        "id": "ghlJwIb_yy7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this exercise a simple line of text is created through the assignment of a string to the variable `line`"
      ],
      "metadata": {
        "id": "YQmNweQD4goW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line = 'Reaching out for HELP. Please meet me in LONDON at 6 a.m xyz@abc.com #urgent!'"
      ],
      "metadata": {
        "id": "E63DGFOg4nm6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Remove stopwords    \n",
        "\n",
        "There are words which are very commonly used but have little meaning when humans use language, those words do not add much value. Depending on the source of data and analysis to be performed, there may be words which are not required for the natural language processing approach. It is best to delete these words from the data.  The set of words to be deleted because they add little value is referred to as **stopwords** in NLP.    \n",
        "\n",
        "The NLTK package has a defined set of stopwords for different languages like English. This example focuses on English-language stopwords but, of course, every language and use case may have its own set of stopwords.\n"
      ],
      "metadata": {
        "id": "V6QBwzb44LHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional stopwords\n",
        "extra_list = ['let', 'may', 'might', 'must', 'need', 'apologies', 'meet']\n",
        "# Extending the list of stopwords\n",
        "stopwords.extend(extra_list)\n",
        "# Display the extended list of stopwords\n",
        "print('Stopwords to use:', stopwords)"
      ],
      "metadata": {
        "id": "r0gayW-v29pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Transform to lower case    \n",
        "\n",
        "Convert all the text to lowercase to maintain uniformity in subsequent analysis.  Of course, all text could just as easily be converted to upper case, but it is generally accepted practice to work with text all in lower case.    \n"
      ],
      "metadata": {
        "id": "5FWWXOSP6KRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform text to lower case\n",
        "line = line.lower()\n",
        "# Display text after transformation to lower case\n",
        "print(line)"
      ],
      "metadata": {
        "id": "E5gn2Z--7DFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization and Stemming**\n",
        "\n",
        "Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
        "\n",
        "Stemming is the process of reducing the word to its word stem that affixes to suffixes and prefixes or to roots of words known as a lemma. In simple words stemming is reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem. For example â€“ The words care, cared and caring lie under the same stem â€˜careâ€™. Stemming is important in NLP as an alternative to Lemmatizing.\n",
        "\n",
        "\n",
        "Sometimes, the same word can have multiple different Lemmas. We should identify the Part of Speech (POS) tag for the word in that specific context. Here are the examples to illustrate all the differences and use cases:\n",
        "\n",
        "1. If you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous.\n",
        "2. If you lemmatize the word 'Stripes' in verb context, it would return 'Strip'. If you lemmatize it in noun context, it would return 'Stripe'. If you just stem it, it would just return 'Strip'.\n",
        "3. You would get same results whether you lemmatize or stem words such as walking, running, swimming... to walk, run, swim etc.\n",
        "4. Lemmatization is computationally expensive since it involves look-up tables and what not.     \n",
        "\n",
        "If you have a large dataset and performance is an issue, go with Stemming. Remember you can also add your own rules to Stemming. If accuracy is paramount and dataset isn't humongous, go with Lemmatization.\n",
        "\n",
        "*source: StackOverflow, [What Is the Difference Betweem Stemming and Lemmatization?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)*\n"
      ],
      "metadata": {
        "id": "rLs1PwBs8auT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Glossary**    \n",
        "\n",
        "* **whitespace tokenizer** - a tokenizer that splits on and discards only whitespace characters returning individual words or terms as tokens.    \n",
        "\n",
        "* **Porter Stemming** - The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the most common morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.    \n",
        "\n",
        "* **Snowball Stemmer** - This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer.    \n",
        "\n",
        "* **Lancaster Stemmer** - is simple but it tends to produce results with over stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning.    \n",
        "\n",
        "* **Regexp Stemmer** - uses regular expressions (regex) to identify morphological affixes. Any substrings that match the regular expressions will be removed.\n"
      ],
      "metadata": {
        "id": "qZxRAqc-AIX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Lemmatization    \n",
        "\n",
        "Lemmatization helps to reduce the words into a single form taking the context of the use of the word into consideration.  The resultant term from Lemmatization is called a lemma. The following example defines a function to use a Lemmatization method from NLTK.  Other methods may prove more effective depending on the data source and use case."
      ],
      "metadata": {
        "id": "dRU0xXYS9J4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to Lemmatize text and to tokenize based on whitespace\n",
        "def lemmatize_text(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
        "\n",
        "# Invoke the function defined above and display result\n",
        "lemma_line = lemmatize_text(line) \n",
        "print(lemma_line)\n",
        "# Display the line without tokenization and Lemmatization\n",
        "print(line)"
      ],
      "metadata": {
        "id": "e1jEgSBV_UTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Stemming    \n",
        "\n",
        "Stemming is a faster, more efficient way to attempto to reduce words into their root form. As mentioned earlier, there is a trade-off of accuracy in return for the efficiency of stemming vs. Lemmatization.  The following example defines a function to perform whitespace tokenization and stemming of a line of text then invokes that function.  In this example the NLTK Porter method of stemming is used.  Depending on the nature of the source text and the use case(s) for the analysis, different methods may be considered for stemming. "
      ],
      "metadata": {
        "id": "-AEd9W1OBuOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter Stemming**"
      ],
      "metadata": {
        "id": "yUM61ARbHeOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Porter stemming of text tokenized for whitespaces\n",
        "def stem_porter(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    ps = nltk.PorterStemmer()\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Porter stemming and display the result\n",
        "porter_stem_line = stem_porter(line)\n",
        "print(porter_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "uq0X8IGJBvrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemming**    \n",
        "\n",
        "A few common rules of Snowball stemming are:    \n",
        "* ILY  -----> ILI    \n",
        "* LY   -----> Nill    \n",
        "* SS   -----> SS    \n",
        "* S    -----> Nill    \n",
        "* ED   -----> E,Nill    \n",
        "\n",
        "**Nill** means the suffix is replaced with nothing and is just removed.    \n",
        "\n",
        "There may be cases where these rules vary depending on the words. As in the case of the suffix â€˜edâ€™ if the words are â€˜caredâ€™ and â€˜bumpedâ€™ they will be stemmed as â€˜careâ€˜ and â€˜bumpâ€˜. Hence, here in cared the suffix is considered as â€˜dâ€™ only and not â€˜edâ€™. \n",
        "\n",
        "The word â€˜stemmedâ€˜ is replaced with the word â€˜stemâ€˜ and not â€˜stemmedâ€˜. Therefore, the suffix depends on the word.\n",
        "\n",
        "Here are a few examples:-\n",
        "```\n",
        "WORD           STEM\n",
        "cared          care\n",
        "university     univers\n",
        "fairly         fair\n",
        "easily         easili\n",
        "singing        sing\n",
        "sings          sing\n",
        "sung           sung\n",
        "singer         singer\n",
        "sportingly     sport\n",
        "```"
      ],
      "metadata": {
        "id": "_15H2iHrBDP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Snowball stemming of text tokenized for whitespaces\n",
        "def stem_snowball(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Snowball stemmer needs language defined\n",
        "    ps = nltk.SnowballStemmer(language='english')\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Snowball stemming and display the result\n",
        "snowball_stem_line = stem_snowball(line)\n",
        "print(snowball_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "pXGICkO4IiZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lancaster Stemmer**    \n",
        "\n",
        "There is a third method of stemming available, Lancaster Stemming. \n",
        "\n",
        "Lancaster Stemmer is simple but it tends to produce results with over stemming. Over-stemming causes the stems to be not linguistic, or they may have no meaning.\n",
        "\n",
        "In NLTK, there is a module LancasterStemmer() that supports the Lancaster stemming algorithm.  The following code shows an example of the application of Lancaster Stemming along with whitespace tokenization.    \n"
      ],
      "metadata": {
        "id": "jfAgw67fJxy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Lancaster stemming of text tokenized for whitespaces\n",
        "def stem_lancaster(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Lancaster stemmer needs language defined\n",
        "    ps = nltk.LancasterStemmer()\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Lancaster stemming and display the result\n",
        "lancaster_stem_line = stem_lancaster(line)\n",
        "print(lancaster_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "mUPFNR_AKki6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer**\n",
        "\n",
        "Regex stemmer uses regular expressions to identify morphological affixes. Any substrings that match the regular expressions will be removed.\n",
        "\n",
        "In NLTK, there is a module `RegexpStemmer()` that supports the Regex stemming algorithm."
      ],
      "metadata": {
        "id": "7NJQFwOYMHaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for Regexp stemming of text tokenized for whitespaces\n",
        "def stem_regexp(text):\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "    # Bear in mind that the Regexp stemmer needs a regex defined\n",
        "    ps = nltk.RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "    return [ps.stem(w) for w in w_tokenizer.tokenize(text)]\n",
        "# Invoke the method defined for Regexp stemming and display the result\n",
        "regexp_stem_line = stem_regexp(line)\n",
        "print(regexp_stem_line)\n",
        "# Display the original line\n",
        "print(line)"
      ],
      "metadata": {
        "id": "Z1gGR-iVNVoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Display of the result of Lemmatization vs. the four methods of stemming compared with the original text follow.  The following code requires the execution of each of the five snippets above to create the variables to print.*"
      ],
      "metadata": {
        "id": "Ip8pUTTwNv_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lematization:\")\n",
        "print(lemma_line, \"\\n\")\n",
        "print(\"Porter Stemming:\")\n",
        "print(porter_stem_line, \"\\n\")\n",
        "print(\"Snowball Stemming:\")\n",
        "print(snowball_stem_line, \"\\n\")\n",
        "print(\"Lancaster Stemming:\")\n",
        "print(lancaster_stem_line, \"\\n\")\n",
        "print(\"Regexp Stemming with `'ing$|s$|e$|able$', min=4'`:\")\n",
        "print(regexp_stem_line, \"\\n\")\n"
      ],
      "metadata": {
        "id": "yT7oueLROFbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More Comparative Examples of Stemming**    "
      ],
      "metadata": {
        "id": "1bDMPq05PsGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING EXAMPLE 1\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\"]\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "id": "oDvh_Z5wPxe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEMMING EXAMPLE 2\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(language='english')\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "word_list = ['run','runs','running','runner','ran','easily','fairly']\n",
        "print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(\"Word\",\"Porter Stemmer\",\"Snowball Stemmer\",\"Lancaster Stemmer\",'Regexp Stemmer'))\n",
        "for word in word_list:\n",
        "    print(\"{0:20}{1:20}{2:20}{3:30}{4:40}\".format(word,porter.stem(word),snowball.stem(word),lancaster.stem(word),regexp.stem(word)))"
      ],
      "metadata": {
        "id": "W-e38bAzP7Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Difference Between Porter Stemmer and Snowball Stemmer**\n",
        "\n",
        "Snowball Stemmer is more aggressive than Porter Stemmer.\n",
        "Some issues in Porter Stemmer were fixed in Snowball Stemmer.\n",
        "There is only a little difference in the working of these two.\n",
        "Words like â€˜fairlyâ€˜ and â€˜sportinglyâ€˜ were stemmed to â€˜fairâ€™ and â€˜sportâ€™ in the snowball stemmer but when you use the porter stemmer they are stemmed to â€˜fairliâ€˜ and â€˜sportingliâ€˜.    \n",
        "\n",
        "The difference between the two algorithms can be clearly seen in the way the word â€˜Sportinglyâ€™ in stemmed by both. Clearly Snowball Stemmer stems it to a more accurate stem.\n",
        "\n",
        "**Drawbacks of Stemming**    \n",
        "\n",
        "Issues of over stemming and under stemming may lead to not so meaningful or inappropriate stems.    \n",
        "\n",
        "Stemming does not consider how the word is being used. For example â€“ the word â€˜sawâ€˜ will be stemmed to â€˜sawâ€˜ itself but it wonâ€™t be considered whether the word is being used as a noun or a verb in the context. For this reason, Lemmatization is used as it keeps this fact in consideration and will return either â€˜seeâ€™ or â€˜sawâ€™ depending on whether the word â€˜sawâ€™ was used as a verb or a noun."
      ],
      "metadata": {
        "id": "X_nbuYzzQcqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Removal of Regular Expressions   \n",
        "\n",
        "Regular expressions (regex) help to identify and to get rid of different patterns which are not required in the text.\n"
      ],
      "metadata": {
        "id": "-VugsKUTQv3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Line:\", line)\n",
        "print(\"...cleaning...\\n\")\n",
        "clean0 = line\n",
        "clean1 = re.sub('\\S*@\\S*\\s?',\" \",clean0)            # email removal\n",
        "clean2 = re.sub('\\s+',\" \",clean1)                   # new line character removal\n",
        "clean3 = re.sub(\"\\â€™\",\" \",clean2)                    # single quote removal\n",
        "clean4 = re.sub('_',\" \",clean3)                     # underscore removal\n",
        "clean5 = re.sub('http\\S*\\s?',\" \",clean4)            # link removal\n",
        "clean = ' '.join([i for i in clean5.split() if i.find('#') < 0]) #remove hashtag\n",
        "print(\"Clean line:\", clean)"
      ],
      "metadata": {
        "id": "4L8SUll2RAjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Parts-of-Speech (POS) tagging\n",
        "\n",
        "Parts-of-speech tagging helps to identify the parts of speech. Based on the use case one can keep or remove some of them.  Parts-of-speech may be localized by dialect and other cultural factors (profession, age, etc.)\n",
        "\n",
        "The following code is an example of parts-of-speech tagging using the `spacy` library.\n"
      ],
      "metadata": {
        "id": "raRKGOoJUR6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_spacy = nlp(line)\n",
        "for token in tokens_spacy:\n",
        "    print(token.text, ': ', token.pos_, ': ', token.is_stop)"
      ],
      "metadata": {
        "id": "b409l28CU0UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Named-Entity-Recognition (NER)    \n",
        "\n",
        "Named-entity-recognition helps to identify and categorize the different groups which includes names, places, currency etc.\n",
        "\n",
        "The following code is an example of the use of named-entity-recognition using the `spacy` module. \n",
        "\n",
        "*The following code requires execution of the prior snippet of code to create the variable `tokens_spacy`."
      ],
      "metadata": {
        "id": "lnWSZkUUVgIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in tokens_spacy.ents:\n",
        "    print(ent.text, ': ', ent.label_)"
      ],
      "metadata": {
        "id": "v5PZWuV-Vhkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TbwdBQDhV3bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OE5PCBn9vXCz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqcb9--fNVE0"
      },
      "source": [
        "## Let us see how we can use LISTs in NLP\n",
        "\n",
        "[NLTK Chapter 1](https://www.nltk.org/book/ch01.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dElVkFGzHQiU"
      },
      "source": [
        "# Setup wordcloud and nltk\n",
        "!pip install -q wordcloud\n",
        "import wordcloud\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdqCrNTNKbUK"
      },
      "source": [
        "nltk.download('book') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qKxEu4KOQzt"
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMS1oJzJKbIf"
      },
      "source": [
        "# Get stopwords, stemmer and lemmatizer\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = nltk.stem.PorterStemmer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkUsB2KDKbOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2120f97-f49f-42aa-a5b4-9f9e7565da82"
      },
      "source": [
        "type(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDxgXFxLKbRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa61bfa-b783-4715-c4d1-e70120941026"
      },
      "source": [
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCtu_5qsxZYn"
      },
      "source": [
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMOnL3KuKsRu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "73ef0216-4a82-4d81-f4f7-8391320a52a5"
      },
      "source": [
        "stopwords[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i'"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCgbcs7pKbWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d2b999-ca19-48d8-ae39-d2a43f8595b5"
      },
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']\n",
        "sent1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Call', 'me', 'Ishmael', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTXI-1OdxxnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d53fc03-b042-4314-ba03-72ebd1ad6bea"
      },
      "source": [
        "text1[100:110]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1[0:50]"
      ],
      "metadata": {
        "id": "64FL5BPzkNxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-gJFqwZKbcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fe5553-57a0-4d89-f2c0-4179f5dddfba"
      },
      "source": [
        "len(sent1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4IxtzszOQmu"
      },
      "source": [
        "sent2 = ['The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex', '.']\n",
        "sent3 = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRpOEOLyOQrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995a516e-753d-4f4d-b38f-49bad1b7d2a3"
      },
      "source": [
        "sent2+sent3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'family',\n",
              " 'of',\n",
              " 'Dashwood',\n",
              " 'had',\n",
              " 'long',\n",
              " 'been',\n",
              " 'settled',\n",
              " 'in',\n",
              " 'Sussex',\n",
              " '.',\n",
              " 'In',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'God',\n",
              " 'created',\n",
              " 'the',\n",
              " 'heaven',\n",
              " 'and',\n",
              " 'the',\n",
              " 'earth',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZB_FSxhOQt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1094931d-bdb1-40f1-e351-cc9c8bf32a5c"
      },
      "source": [
        "sent1.append(\"Some\")\n",
        "sent1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Call', 'me', 'Ishmael', '.', 'Some']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Bj67QHOQx3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0aaff4-96d0-4c06-ee33-e249d10bedf2"
      },
      "source": [
        "len(text1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260819"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrr1IqO-xFkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166373ef-1ebe-493a-92de-d67eecd3d7d4"
      },
      "source": [
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141576"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e_FK-bjPYkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "568296e7-a076-46c3-9d68-3ecdda037ad2"
      },
      "source": [
        "text1[100:110]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['and', 'to', 'teach', 'them', 'by', 'what', 'name', 'a', 'whale', '-']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zl_kxLI1ln0"
      },
      "source": [
        "##[NLTK concordance](http://www.nltk.org/api/nltk.html?highlight=concordance)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awnAG-PAMgrL",
        "outputId": "7b388276-22ff-4b67-d1a3-37321d8c92e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Text: Inaugural Address Corpus>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOgSt9w9Ps3-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f90e25-69d7-43f3-933b-9643adb4ec7c"
      },
      "source": [
        "text4.concordance(\"nation\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 25 of 330 matches:\n",
            " to the character of an independent nation seems to have been distinguished by\n",
            "f Heaven can never be expected on a nation that disregards the eternal rules o\n",
            "first , the representatives of this nation , then consisting of little more th\n",
            ", situation , and relations of this nation and country than any which had ever\n",
            ", prosperity , and happiness of the nation I have acquired an habitual attachm\n",
            "an be no spectacle presented by any nation more pleasing , more noble , majest\n",
            "party for its own ends , not of the nation for the national good . If that sol\n",
            "tures and the people throughout the nation . On this subject it might become m\n",
            "if a personal esteem for the French nation , formed in a residence of seven ye\n",
            "f our fellow - citizens by whatever nation , and if success can not be obtaine\n",
            "y , continue His blessing upon this nation and its Government and give it all \n",
            "powers so justly inspire . A rising nation , spread over a wide and fruitful l\n",
            "ing now decided by the voice of the nation , announced according to the rules \n",
            "ars witness to the fact that a just nation is trusted on its word when recours\n",
            "e union of opinion which gives to a nation the blessing of harmony and the ben\n",
            "uil suffrage of a free and virtuous nation , would under any circumstances hav\n",
            "d spirit and united councils of the nation will be safeguards to its honor and\n",
            "iction that the war with a powerful nation , which forms so prominent a featur\n",
            "out breaking down the spirit of the nation , destroying all confidence in itse\n",
            "ed on the military resources of the nation . These resources are amply suffici\n",
            "the war to an honorable issue . Our nation is in number more than half that of\n",
            "ndividually have been happy and the nation prosperous . Under this Constitutio\n",
            "rights , and is able to protect the nation against injustice from foreign powe\n",
            " great agricultural interest of the nation prospers under its protection . Loc\n",
            "ak our Union , and demolish us as a nation . Our distance from Europe and the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"son\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCkeg3Xvm8M_",
        "outputId": "dbd31490-3824-4f7d-efdc-ea7a9071a1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Moby Dick by Herman Melville 1851>\n",
            "Displaying 20 of 20 matches:\n",
            "l sinners among men , the sin of this son of Amittai was in his wilful disobedi\n",
            "r his naked wrists ; Queequeg was the son of a King , and Queequeg budged not .\n",
            " cabin , ye canting , drab - coloured son of a wooden gun -- a straight wake wi\n",
            " He must show that he ' s converted . Son of darkness ,\" he added , turning to \n",
            " and all of us , and every mother ' s son and soul of us belong ; the great and\n",
            "arnestly into his eyes , and said , \" Son of darkness , I must do my duty by th\n",
            "the sea ; the unerring harpoon of the son fitly replacing the infallible arrow \n",
            "f - believed this wild Indian to be a son of the Prince of the Powers of the Ai\n",
            "or little Flask , he was the youngest son , and little boy of this weary family\n",
            "narrative ; I have conversed with his son ; and all this within a few miles of \n",
            " from his girdle ; \" every mother ' s son of ye draw his knife , and pull with \n",
            "elkilt Charlemagne , had he been born son to Charlemagne ' s father . But Radne\n",
            " festooning their limbs . Nor can any son of mortal woman , for the first time \n",
            " fraternity . The gallant Perseus , a son of Jupiter , was the first whaleman ;\n",
            " may reveal of the divine love in the Son , the soft , curled , hermaphroditica\n",
            "or worshipping which , King Asa , her son , did depose her , and destroyed the \n",
            " and you SHALL do this thing .\" \" His son !\" cried Stubb , \" oh , it ' s his so\n",
            "on !\" cried Stubb , \" oh , it ' s his son he ' s lost ! I take back the coat an\n",
            " chase , there had been still another son ; as that for a time , the wretched f\n",
            ", that Nantucket captains will send a son of such tender age away from them , f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"king\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fQqXkYqolyH",
        "outputId": "d543b8b6-a82b-4141-c0af-f183995c4f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Moby Dick by Herman Melville 1851>\n",
            "Displaying 25 of 64 matches:\n",
            "th , of which he brought some to the king . ... The best whales were catched i\n",
            "RRATIVE TAKEN DOWN FROM HIS MOUTH BY KING ALFRED , A . D . 890 . \" And whereas\n",
            "armacetti for an inward bruise .\" -- KING HENRY . \" Very like a whale .\" -- HA\n",
            "SOMEWHERE .) \" A tenth branch of the king ' s ordinary revenue , said to be gr\n",
            " the coast , are the property of the king .\" -- BLACKSTONE . \" Soon to the spo\n",
            " Io ! sing . To the finny people ' s king . Not a mightier whale than this In \n",
            "n might , where might is right , And King of the boundless sea .\" -- WHALE SON\n",
            "wo . His father was a High Chief , a King ; his uncle a High Priest ; and on t\n",
            ", spurned his suit ; and not all the King his father ' s influence could preva\n",
            "d wrists ; Queequeg was the son of a King , and Queequeg budged not . Struck b\n",
            " the High Priest and his majesty the King , Queequeg ' s father . Grace being \n",
            " plain precedence over a mere island King , especially in the King ' s own hou\n",
            "mere island King , especially in the King ' s own house -- the Captain coolly \n",
            " , not unworthy a Scandinavian sea - king , or a poetical Pagan Roman . And wh\n",
            "f old , thou knowest , was a crowned king !\" \" And a very vile one . When that\n",
            "d a very vile one . When that wicked king was slain , the dogs , did they not \n",
            " great feast given by his father the king , on the gaining of a great battle w\n",
            "settees and sofas of all sorts , the king , chiefs , and great people generall\n",
            "ws ? Certain I am , however , that a king ' s head is solemnly oiled at his co\n",
            "ight and last long . They called him King - Post on board of the Pequod ; beca\n",
            "ed ? For a Khan of the plank , and a king of the sea , and a great lord of Lev\n",
            "osted Flask . \" Such a queer dream , King - Post , I never had . You know the \n",
            "dent , hilarious little Flask enters King Ahab ' s presence , in the character\n",
            "haps not . To have been Belshazzar , King of Babylon ; and to have been Belsha\n",
            "e , the great gods mock that captive king ; so like a Caryatid , he patient si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"ship\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDz_aj4aooJ4",
        "outputId": "8216568a-a131-48d4-8e0c-733c33f34440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Moby Dick by Herman Melville 1851>\n",
            "Displaying 25 of 518 matches:\n",
            "hale is floating at the stern of the ship , they cut off his head , and tow it\n",
            "ution for fear they should run their ship upon them .\" -- SCHOUTEN ' S SIXTH C\n",
            " from the Elbe , wind N . E . in the ship called The Jonas - in - the - Whale \n",
            "RATIVE OF THE SHIPWRECK OF THE WHALE SHIP ESSEX OF NANTUCKET , WHICH WAS ATTAC\n",
            "HALING CRUIZE . 1846 . \" The Whale - ship Globe , on board of which vessel occ\n",
            "OCK . ANOTHER VERSION OF THE WHALE - SHIP GLOBE NARRATIVE . \" The voyages of t\n",
            "\" It is impossible to meet a whale - ship on the ocean without being struck by\n",
            "e whales , that the whites saw their ship in bloody possession of the savages \n",
            "E TAKING AND RETAKING OF THE WHALE - SHIP HOBOMACK . \" It is generally well kn\n",
            "on his sword ; I quietly take to the ship . There is nothing surprising in thi\n",
            " , when first told that you and your ship were now out of sight of land ? Why \n",
            " , a cook being a sort of officer on ship - board -- yet , somehow , I never f\n",
            "eat hurricane ; the half - foundered ship weltering there with its three disma\n",
            "s legs . He was trying his hand at a ship under full sail , but he didn ' t ma\n",
            " a three years ' voyage , and a full ship . Hurrah , boys ; now we ' ll have t\n",
            "fore the idol ; then laying a bit of ship biscuit on top and applying the flam\n",
            " blacksmiths , and harpooneers , and ship keepers ; a brown and brawny company\n",
            "ming one of the boats ' crews OF THE SHIP ELIZA Who were towed out of sight by\n",
            "dder , like those used in mounting a ship from a boat at sea . The wife of a w\n",
            "ped me that however convenient for a ship , these joints in the present instan\n",
            "arge painting representing a gallant ship beating against a terrible storm off\n",
            "a distinct spot of radiance upon the ship ' s tossed deck , something like tha\n",
            "ank where Nelson fell . \" Ah , noble ship ,\" the angel seemed to say , \" beat \n",
            "y , \" beat on , beat on , thou noble ship , and bear a hardy helm ; for lo ! t\n",
            "elled front was in the likeness of a ship ' s bluff bows , and the Holy Bible \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"knife\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkK0nk8Sop92",
        "outputId": "3ad7954f-b1e7-4163-ee2c-b23a8af082ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Moby Dick by Herman Melville 1851>\n",
            "Displaying 25 of 31 matches:\n",
            " further adorning it with his jack - knife , stooping over and diligently worki\n",
            "ed with a sailor - belt and sheath - knife . Here comes another with a sou '- w\n",
            "rd into its face , and with a jack - knife gently whittling away at its nose , \n",
            "trying to mend a pen with his jack - knife , old Bildad , to my no small surpri\n",
            " mend that pen , will ye . My jack - knife here needs the grindstone . That ' s\n",
            "es all fastened upon the old man ' s knife , as he carved the chief dish before\n",
            "her . No ! And when reaching out his knife and fork , between which the slice o\n",
            " little started if , perchance , the knife grazed against the plate ; and chewe\n",
            "y wooden trencher , while Tashtego , knife in hand , began laying out the circl\n",
            "him ; tows me with a cable I have no knife to cut . Horrible old man ! Who ' s \n",
            "er ! SPANISH SAILOR ( MEETING HIM ). Knife thee heartily ! big frame , small sp\n",
            " Fair play ! Snatch the Spaniard ' s knife ! A ring , a ring ! OLD MANX SAILOR \n",
            "s ; one captain , seizing the line - knife from his broken prow , had dashed at\n",
            ". Then , in darting at the monster , knife in hand , he had but given loose to \n",
            "out ! Here !\" whipping out the sharp knife from his girdle ; \" every mother ' s\n",
            " every mother ' s son of ye draw his knife , and pull with the blade between hi\n",
            "s it would tip for an instant on the knife - like edge of the sharper waves , t\n",
            "ng of the boat , quickly brought his knife to the line . He cut it ; and the wh\n",
            "k ' s tooth , of his one poor jack - knife , he will carve you a bit of bone sc\n",
            "ult to injury , is it ? Look at your knife - handle , there , my civilized and \n",
            " cutting - spade pole , and with his knife slightly split the end , to insert t\n",
            "it in an instant , seized the boat - knife , and impaling the letter on it , se\n",
            " of ye for a prayer book and a pen - knife , and cut the big chains .\" \" Knife \n",
            " knife , and cut the big chains .\" \" Knife ? Aye , aye ,\" cried Queequeg , and \n",
            "or a poltroon . Snatching the boat - knife from its sheath , he suspended its s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text1)\n",
        "text1.concordance(\"monster\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GccaIaPQovuN",
        "outputId": "fd9df884-3cc7-45d1-ea57-ce1e1244b0a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Moby Dick by Herman Melville 1851>\n",
            "Displaying 25 of 49 matches:\n",
            "des cometh within the chaos of this monster ' s mouth , be it beast , boat , or\n",
            "nter into the dreadful gulf of this monster ' s ( whale ' s ) mouth , are immed\n",
            "time with a lance ; but the furious monster at length rushed on the boat ; hims\n",
            " . Such a portentous and mysterious monster roused all my curiosity . Then the \n",
            "and flank with the most exasperated monster . Long usage had , for this Stubb ,\n",
            "ACK ).-- Under this head I reckon a monster which , by the various names of Fin\n",
            "arned the history of that murderous monster against whom I and all the others h\n",
            "ocity , cunning , and malice in the monster attacked ; therefore it was , that \n",
            "iathan is restricted to the ignoble monster primitively pursued in the North ; \n",
            " and incontestable character of the monster to strike the imagination with unwo\n",
            "mberment . Then , in darting at the monster , knife in hand , he had but given \n",
            "e rock ; instead of this we saw the monster sailing off with the utmost gravity\n",
            "e at Constantinople , a great sea - monster was captured in the neighboring Pro\n",
            " Of what precise species this sea - monster was , is not mentioned . But as he \n",
            "man reasoning , Procopius ' s sea - monster , that for half a century stove the\n",
            "hale ,\" as he called the fictitious monster which he declared to be incessantly\n",
            "d his intention to hunt that mortal monster in person . But such a supposition \n",
            "ng us on and on , in order that the monster might turn round upon us , and rend\n",
            "d famous , and most deadly immortal monster , Don ;-- but that would be too lon\n",
            "oluntarily lifted his voice for the monster , though for some little time past \n",
            "s rescuing Andromeda from the sea - monster or whale . Where did Guido get the \n",
            " huge corpulence of that Hogarthian monster undulates on the surface , scarcely\n",
            "nd is drawn just balancing upon the monster ' s spine ; and standing in that pr\n",
            " of cutting - in ) hove over to the monster as if to a quay ; and a boat , hurr\n",
            "eet in length . They fancy that the monster to which these arms belonged ordina\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using .similar()"
      ],
      "metadata": {
        "id": "WIwkGzsVn9eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text1)"
      ],
      "metadata": {
        "id": "oo92GzTQp6KV",
        "outputId": "ab363a15-855d-48a1-be57-6468276afa91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260819"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"monster\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dkg8Fut_n_gB",
        "outputId": "d4ff82de-0529-4fbc-8d88-69a56928cdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whale ship world sea whales boat pequod other sun leviathan thing king\n",
            "water head captain air crew cabin body more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"king\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmlZNkVEoFjd",
        "outputId": "0ff7d9c6-9241-46de-faea-1456e08e5b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whale ship sea boat man line pequod water head captain time crew rope\n",
            "harpooneer cry world lord day monster land\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"knife\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi0qqLtooNxE",
        "outputId": "2c23031b-8619-42a8-a238-6389d563f4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head and side way ship time face hand whale more boat men place duty\n",
            "leg officers marines trumpet heart body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"son\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8z0RhYaoPBs",
        "outputId": "b5de9d64-4ddf-4304-f093-1df16ea91380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "side head part heart body hand boat sort matter time sense wife cabin\n",
            "whale book eye god lord sea mouth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"ship\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqVlSeM7oR6Y",
        "outputId": "3bb58447-213d-4ef4-fef7-7654f5312767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "whale boat sea world captain way head time other man crew pequod line\n",
            "deck body fishery air side water voyage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DCMlA8U3m7ap"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PqlJAKK0JKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1b477d-f97b-429c-ed99-b8fbdb874052"
      },
      "source": [
        "print(text4)\n",
        "len(text4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Inaugural Address Corpus>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152901"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc05kKoa1UOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad73b2a-a39d-4463-d676-7a453789ac07"
      },
      "source": [
        "print(text2)\n",
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Text: Sense and Sensibility by Jane Austen 1811>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141576"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx2fI346fnPD"
      },
      "source": [
        "text2.dispersion_plot([\"Elinor\", \"Marianne\", \"Edward\", \"Willoughby\", \"Brandon\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziYklSVdf_dD"
      },
      "source": [
        "len(set(text4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2Z8Gg-iiajT"
      },
      "source": [
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"liberty\", \"is\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05eHJd7-icgR"
      },
      "source": [
        "text2.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"is\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXY-SyXdiiMC"
      },
      "source": [
        "text2.dispersion_plot([\"society\", \"husband\", \"feeling\", \"marriage\", \"independent\", \"Dashwood\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H364Zrc61uQA"
      },
      "source": [
        "len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sil_VPA71yAm"
      },
      "source": [
        "len(set(text2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANRzeLmb13Ec"
      },
      "source": [
        "len(set(text2))/len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi4KTZHYjDyH"
      },
      "source": [
        "len(set(text3)) / len(text3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN-VR57AjGp-"
      },
      "source": [
        "len(set(text3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ1zOz0cjjXi"
      },
      "source": [
        "len(set(text3)-set(stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_T3BbrymeKz"
      },
      "source": [
        "fdist1 = FreqDist(text1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_oqEJh28Dv"
      },
      "source": [
        "fdist1.most_common(50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Qcf8OT3DE0"
      },
      "source": [
        "fdist2 = FreqDist(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYYuUHg3tJc"
      },
      "source": [
        "len(fdist2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf0rqLgN3F4H"
      },
      "source": [
        "fdist2.most_common(50) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yzQUowg3PuL"
      },
      "source": [
        "Find 50 most frequent words for Sense and Sensibility that are not stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWwLAiyB3Wze"
      },
      "source": [
        "fdist2[',']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49eASpO76cSW"
      },
      "source": [
        "counter = 0\n",
        "for item in fdist2.most_common():\n",
        "  if item[0] not in stopwords:\n",
        "    print(item)\n",
        "    counter += 1\n",
        "    if (counter == 49):\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvSK7Yr77NRM"
      },
      "source": [
        "170/len(text2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}