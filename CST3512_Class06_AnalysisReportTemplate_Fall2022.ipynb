{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9S38P4zcoKwC",
        "9OL-7iOkWDHU",
        "Vv_ZMmzENlWC",
        "ZQMGcXBUwGA0",
        "iv4rE81xmwC2",
        "UskbroIclq-F",
        "-T6hey5olyjV",
        "1FDZi8qkRGVK",
        "lDgAIU1bMSew",
        "5hdPjaqv95iS",
        "wtYc4gD6WPxg",
        "ziTVnVfTW6GJ",
        "AXetTnlKwY03",
        "QXEuUmCPW-K2",
        "f6pRUqRNmG6a",
        "pv2tZ6u-Kss8",
        "85nmdmmWXCFU",
        "OzgpRLetmNC6",
        "FDErOKbFWsC0",
        "L7B2vCbxpX1q",
        "miV2hbqMZMry",
        "O726wlw5wftc",
        "GkGDWJwVyHj6",
        "GL1udqjwy_Pc",
        "sXvBqFvkyOGq",
        "eVHfyoJZzS2k",
        "oUMDdDcnWcTd",
        "is50vYoPmmFc",
        "3rENQFIhJDd-",
        "5CMqlhKOmbGz",
        "wHV19-ryWjf9",
        "xP8m-_Nul-I-",
        "jLl0LSY9Y1xf",
        "qVPmeZkUvH7R",
        "jm__ALNBXQ_j",
        "m8KaWsB3QWfS",
        "1TBdrxQiVhd3",
        "FEkfZqOVVlPy",
        "DyOv4390X7H8",
        "OB4kGFOsVRKT",
        "BPAbobGNJhkK",
        "mc0SugpcS_bs",
        "BaySwbV-5453",
        "UqeHIRhlZ4in",
        "TxR42FTocivm",
        "eir9uRYn4yoL"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CST3512/blob/main/CST3512_Class06_AnalysisReportTemplate_Fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CST3512 Class 06  \n",
        "**template to use for lab class project**    "
      ],
      "metadata": {
        "id": "pMTSf2LtnljV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis Report Template    \n",
        "\n",
        "\n",
        "**A shared resource for the organization of data science analysis and report presentation.**\n",
        "\n",
        "*Prepared by CUNY CityTech CST3512 class, Spring 2022*    \n",
        "*Updated 26-Nov-2022; 04-Dec-2022* \n",
        "   \n",
        "\n"
      ],
      "metadata": {
        "id": "Ar1dlppwcrPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTLINE**    \n",
        "\n",
        "1. (Abstract) Executive Summary     \n",
        "\n",
        "2. Introduction    \n",
        "\n",
        "3. Background   \n",
        "    - Problem Statement   \n",
        "    - Research Questions    \n",
        "\n",
        "4. Methdology / Approach    \n",
        "    - Analytic Aproach  \n",
        "    - Tools and Techniques    \n",
        "    - Data Sources    \n",
        "    - Data Wrangling    \n",
        "    - Results of Analysis    \n",
        "    - Reports and Plots    \n",
        "5. Results\n",
        "    - Findings    \n",
        "    - One or Themes (insighst based on findings)    \n",
        "    - Implications (the 'so what?')     \n",
        "    - Recommendation(s)   \n",
        "6. Conclusion    \n",
        "    - Plan of Action and Milestones\n",
        "    - Next Steps (additional-research?)   \n",
        "\n",
        "\n",
        "* APPENDICES    \n",
        "I.     Footnotes    \n",
        "II.    Glossary of terms    \n",
        "III.   Bibliography / Additional Reading   \n"
      ],
      "metadata": {
        "id": "0PqttnLniYOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. (Abstract) Executive Summary    \n",
        "\n"
      ],
      "metadata": {
        "id": "WFcvmzu84RLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An abstract is a very short summary of all the sections of your research paper—the introduction, objectives, materials and methods, results, and conclusion. It ends by emphasising the novelty or relevance of your study, or by posing questions for future research. The abstract should cover all important aspects of the study, so that the reader can quickly decide if the paper is of their interest or not.    \n",
        "\n",
        "A good introduction sets the context for your study and clearly distinguishes between the knowns and the unknowns in a research topic.\n",
        "\n",
        "Often, the introduction mentions the materials and methods used in a study and outlines the hypotheses tested. Both the abstract and the introduction have this in common. So, what are the key differences between the two sections?    \n",
        "\n",
        "Key differences between an abstract and the introduction:    \n",
        " - The word limit for an abstract is usually 250 words or less. In contrast, the typical word limit for an introduction is 500 words or more.    \n",
        " - When writing the abstract, it is essential to use keywords to make the paper more visible to search engines. This is not a significant concern when writing the introduction.    \n",
        " - The abstract features a summary of the results and conclusions of your study, while the introduction does not. The abstract, unlike the introduction, may also suggest future directions for research.    \n",
        " - While a short review of previous research features in both the abstract and the introduction, it is more elaborate in the latter.    \n",
        " - All references to previous research in the introduction come with citations. The abstract does not mention specific studies, although it may briefly outline previous research.    \n",
        " - The abstract always comes before the introduction in a research paper.    \n",
        " - Every paper does not need an abstract. However, an introduction is an essential component of all research papers.    \n",
        "\n",
        "*from: [Differentiating between the abstract and the introduction of a research paper](https://scientific-publishing.webshop.elsevier.com/manuscript-preparation/differentiating-between-and-introduction-research-paper/)*    "
      ],
      "metadata": {
        "id": "E0uy9CcSc_9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An executive summary is exactly what the name suggests – a summary. It is essentially a quick overview of all the most important information in the report. This part of the report primarily focuses on the key topics and most important data within it.     \n",
        "\n",
        "The people most interested in reading the executive summary are typically the ones who don’t have time to read the whole report and want a quick overview of the most important data and information. These include:    \n",
        "\n",
        " - **Project stakeholders** – The individuals or organizations that are actively involved in a project.    \n",
        " - **C-level Executives** – The chief executive officer and their direct reports (**senior executives**) in a business.    \n",
        " - **Management** personnel (**decision-makers**) – The highest-ranking employees in your company (manager, partner, general partner, etc.)    \n",
        " - **Investors** – This could be bank officials who want a quick recap of of performance or forecasts so they can make an easier investment decision.    \n",
        " - **Venture Capitalists** – Investors who provide capital in exchange for equity stakes.    \n",
        "\n",
        "While an executive summary is a rather short section, it doesn’t mean that it’s easy to write. You will have to pay extra attention to every single sentence in order to avoid unnecessary information.   \n",
        "\n",
        "To know whether you have written a good executive summary, you can ask yourself, “Are the stakeholders going to have all the information they need to make decisions?”  If the answer is yes, you have done a good job.    \n",
        "\n",
        "*summarized from: ['How to Write an Executive Summary for a Report: Step By Step Guide with Examples' | Databox Blog](https://databox.com/how-to-write-executive-summary-report)*    "
      ],
      "metadata": {
        "id": "RxGGgFPlXcCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Introduction    "
      ],
      "metadata": {
        "id": "xBkFoa4zabY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The introduction of your research paper is presented before the background. Some factors that differentiate the background from the introduction:    \n",
        "\n",
        " - The introduction only contains preliminary data about the research topic and does not state the purpose of the study. On the contrary, the background clarifies the importance of the study in detail.    \n",
        " - The introduction provides an overview of the research topic from a broader perspective, while the background provides a detailed understanding of the topic.    \n",
        " - The introduction should end with the mention of the research questions, aims, and objectives of the study. In contrast, the background follows no such format and only provides essential context to the study.    \n",
        "\n",
        "*summarized from: [What is the Background of a Study and How Should it be Written? | Elsevier Author Services](https://scientific-publishing.webshop.elsevier.com/manuscript-preparation/what-background-study-and-how-should-it-be-written/)*    "
      ],
      "metadata": {
        "id": "sH_P86gPbtUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Background    \n",
        "\n"
      ],
      "metadata": {
        "id": "SVGcavwo4RH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The background of a study establishes the context underlying the research. It contains:    \n",
        "- the rationale\n",
        "- the key problem statement\n",
        "- a brief overview of research questions that are addressed in the rest of the paper.    \n",
        "\n",
        "The background forms the crux of the study because it introduces an unaware audience to the research and its importance in a clear and logical manner. At times, the background may even explore whether the study builds on or refutes findings from previous studies.   \n",
        "Any relevant information that the readers need to know before delving into the study should be made available to them in the background.    \n",
        "\n",
        "The length and detail presented in the background varies for different research papers, depending on the complexity and novelty of the research topic. At times, a simple background suffices, even if the study is complex.\n",
        "Before writing and adding details in the background, take a note of these additional points:    \n",
        "\n",
        " - Start with a strong beginning: Begin the background by defining the research topic and then identify the target audience.    \n",
        " - Cover key components: Explain all theories, concepts, terms, and ideas that may feel unfamiliar to the target audience thoroughly.    \n",
        " - Take note of important prerequisites: Go through the relevant literature in detail. Take notes while reading and cite the sources.    \n",
        " - Maintain a balance: Make sure that the background is focused on important details, but also appeals to a broader audience.    \n",
        " - Include historical data: Current issues largely originate from historical events or findings. If the research borrows information from a historical context, add relevant data in the background.    \n",
        " - Explain novelty: If the research study or methodology is unique or novel, provide an explanation that helps to understand the research better.    \n",
        " - Increase engagement: To make the background engaging, build a story around the central theme of the research.    \n",
        "\n",
        "*summarized from: [What is the Background of a Study and How Should it be Written? | Elsevier Author Services](https://scientific-publishing.webshop.elsevier.com/manuscript-preparation/what-background-study-and-how-should-it-be-written/)*    \n"
      ],
      "metadata": {
        "id": "VCSRwr5na3RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Problem Statement    \n",
        "\n"
      ],
      "metadata": {
        "id": "pba2uEEx4Q_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a data scientist you will routinely discover or be presented with problems to solve. Your initial objective should be to determine if your problem is in fact a Data Science problem — and if so, what kind. There is great value in being able to translate a business idea or question into a clearly formulated problem statement. And being able to effectively communicate whether or not that problem can be solved by applying appropriate Machine Learning algorithms.    \n",
        "\n",
        "A true data science problem may:    \n",
        "\n",
        " - Categorize or group data    \n",
        " - Identify patterns    \n",
        " - Identify anomalies    \n",
        " - Show correlations    \n",
        " - Predict outcomes    \n",
        "\n",
        "A good data science problem should be specific and conclusive. For example:    \n",
        "\n",
        " - As personal wealth increases, how do key health markers change?    \n",
        " - Where in California do most people with heart disease live?    \n",
        "\n",
        "Conversely, a vague and unmeasurable problem may not be a good fit for a data science solution. For example:    \n",
        "\n",
        " - What is the link between finances and health?    \n",
        " - Are people in California healthier?    \n",
        "\n",
        "*summarized from: ['Defining A Data Science Problem' | TowardDataScience](https://towardsdatascience.com/defining-a-data-science-problem-4cbf15a2a461)*    \n"
      ],
      "metadata": {
        "id": "9_jrPndZef-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Research Questions    "
      ],
      "metadata": {
        "id": "1Qz45e0YamoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Developing research questions is an absolute necessity in completing any research project. The questions you ask help to shape the type of analysis that you need to conduct.    \n",
        "\n",
        "The type of questions you ask in the context of analytics and data science are similar to those found in traditional quantitative research. Yet data science, like any other field, has its own distinct traits.    \n",
        "\n",
        "Some common data science problems focus on answering one or more of six questions:    \n",
        "\n",
        " 1. Descriptive    \n",
        " 2. Exploratory    \n",
        " 3. Inferential    \n",
        " 4. Predictive    \n",
        " 5. Causal    \n",
        " 6. Mechanistic    \n",
        "\n",
        "*summarized from: [Data Science Research Questions](https://educationalresearchtechniques.com/2017/04/07/data-science-research-questions/)*     \n"
      ],
      "metadata": {
        "id": "E2I2bydSf_4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Descriptive    \n",
        "A descriptive question seeks to describe a characteristic of the dataset. With descriptive questions, there is no need for a hypothesis as you are not trying to infer, establish a relationship, or generalize to a broader context. You simply want to know a trait of the dataset.    "
      ],
      "metadata": {
        "id": "UH1uBnQ3gnoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Exploratory       \n",
        "Exploratory questions seek to identify things that may be “interesting” in the dataset. Examples of things that may be interesting include trends, patterns, and or relationships among variables. Exploratory questions generate hypotheses.    \n"
      ],
      "metadata": {
        "id": "cunQexFAgruz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. Inferential    \n",
        "Inferential questions are an extension of exploratory questions. What this means is that the exploratory question is formally tested by developing an inferential question. Often, the difference between an exploratory and inferential question is the following:    \n",
        "\n",
        " - Exploratory questions are usually developed first    \n",
        " - Exploratory questions generate inferential questions    \n",
        " - Inferential questions are tested often on a different dataset from exploratory questions     \n"
      ],
      "metadata": {
        "id": "OrOC8ExThibi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. Predictive    \n",
        "Predictive analytics uses mathematical modeling tools to generate predictions about an unknown fact, characteristic, or event. “It’s about taking the data that you know exists and building a mathematical model from that data to help you make predictions about somebody [or something] not yet in that data set.    \n",
        "\n",
        "There are some typical types of predictive models used for solving these problems:  \n",
        "\n",
        " a. Linear Regression    \n",
        " b. Text Mining     \n",
        " c. Optimal Estimation    \n",
        " d. Clustering Models    \n",
        " e. Neural Networks    \n",
        "\n",
        "\n",
        "*summarized from: ['Predictive Analytics: What It Is & Why It's Important' | Northeastern University ](https://www.northeastern.edu/graduate/blog/predictive-analytics/)*    "
      ],
      "metadata": {
        "id": "5esOPGXygweF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####a. Linear Regression    \n",
        "\n",
        "Linear regression is one of the most famous and historic modeling tools. This model considers all the known data points on a graph and creates a straight line that travels through the center of those data points. This line represents the smallest possible distance between all the points on the graph. A linear regression mathematical modeling tool can then base predictions about nonexistent data off of the relationship between this line and the existing data points.    \n",
        "\n",
        "*summarized from: ['Predictive Analytics: What It Is & Why It's Important' | Northeastern University ](https://www.northeastern.edu/graduate/blog/predictive-analytics/)*    "
      ],
      "metadata": {
        "id": "6eRTFpHejsUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####b. Text Mining    \n",
        "Whereas linear regression uses only numeric data, mathematical models can also be used to make predictions about non-numerical factors. Text mining is a perfect example.     \n",
        "\n",
        "Text mining is part of predictive analytics in the sense that analytics is all about finding the information I previously knew nothing about. In this scenario, the tool takes data points in the form of text-based words or phrases and searches a giant database for those specific points.    "
      ],
      "metadata": {
        "id": "X7frEKMyj5xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####c. Optimal Estimation     \n",
        "Optimal estimation is a modeling technique that is used to make predictions based on observed factors. This model has been used in analytics for over 50 years and has laid the groundwork for many of the other predictive tools used today. Past applications of this method include determining “how to best recalibrate equipment on a manufacturing floor…[and] estimating where a bullet might go when shot,” as well as in other aspects of the defense industry.    \n",
        "\n",
        "*summarized from: ['Predictive Analytics: What It Is & Why It's Important' | Northeastern University ](https://www.northeastern.edu/graduate/blog/predictive-analytics/)*    "
      ],
      "metadata": {
        "id": "qEEsXbLkkMeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####d. Clustering Models     \n",
        "Clustering models are focused on finding different groups with similar qualities or elements within the data. Many mathematical modeling tools fall within this category, including:    \n",
        "\n",
        " - K-Means    \n",
        " - Hierarchical Clustering    \n",
        " - TwoStep    \n",
        " - Density-Based Scan Clustering    \n",
        " - Gaussian Clustering Model    \n",
        " - Kohonen    \n",
        " \n",
        "*summarized from: ['Predictive Analytics: What It Is & Why It's Important' | Northeastern University ](https://www.northeastern.edu/graduate/blog/predictive-analytics/)*    "
      ],
      "metadata": {
        "id": "5AhmdTngkeY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####e. Neural Networks     \n",
        "Neural networks are complex algorithms inspired by the structure of the human brain. They process historical and current data and identify complex relationships within the data to predict the future, similar to how the human brain can spot trends and patterns.    \n",
        "\n",
        "A  typical neural network is composed of artificial neurons, called units, arranged in different layers. The neural network uses input units to learn about and process data. On the other hand, output units are on the opposite side and outline how the neural network should respond to the input units. Between the two are hidden layers, which are layers of mathematical functions that produce a specific output.    \n",
        "\n",
        "*summarized from: ['Predictive Analytics: What It Is & Why It's Important' | Northeastern University ](https://www.northeastern.edu/graduate/blog/predictive-analytics/)*    "
      ],
      "metadata": {
        "id": "OC5N555tkuJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. Causual    \n",
        "Causal questions address if a change in one variable directly affects another. In analytics, A/B testing is one form of data collection that can be used to develop causal questions. For example, we may develop two version of a website and see which one generates more sales.    \n"
      ],
      "metadata": {
        "id": "RiRzm_54gz9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6. Mechanistic    \n",
        "Mechanistic questions deal with how one variable affects another. This is different from causal questions that focus on if one variable affects another. Continuing with the website example, we may take a closer look at the two different websites and see what it was about them that made one more succesful in generating sales. It may be that one had more banners than another or fewer pictures. Perhaps there were different products offered on the home page.    "
      ],
      "metadata": {
        "id": "ikC2aJlVg4CV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Methodology / Approach    \n",
        "\n",
        "Generally, the research methodology will be driven by the problem statement as described above.  While a specific problem may lend itself to a particular general approach, it is important to document the steps taken at a level of detail which not only explains each step to a lay reader but provides enough information and reference detail for another data scientist to replicate the research and reach similar findings.   \n",
        "\n",
        "The *science* part of **data science** requires that scientific methods be applied, including the requirement that research can be replicated.    \n",
        "\n"
      ],
      "metadata": {
        "id": "jokE8gKI4Q4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analytic Approach    \n",
        "\n"
      ],
      "metadata": {
        "id": "PVKQGKjN4QqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tools and Techniques     "
      ],
      "metadata": {
        "id": "ldD7A7HqoH5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Sources     "
      ],
      "metadata": {
        "id": "byNrMNCJoM5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Wrangling"
      ],
      "metadata": {
        "id": "9S38P4zcoKwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each problem and the data sources considered will raise unique considerations but there are several typical steps to data extraction, discovery, and transformation which are helpful to consider.    \n",
        "\n",
        "The following is derived from the work of Anmol Tomar in ['Every Data Analysis in 10 steps!  Adding structure to your data analysis!' | Codex](https://medium.com/codex/every-data-analysis-in-10-steps-960dc7e7f00b)    \n",
        "\n",
        "**This notebook extends the ten (10) steps in the original article by Anmol Tomar into 13 steps (to include steps related to duplicate rows and the transformation/wrangling of data elements.**    \n"
      ],
      "metadata": {
        "id": "qk0ELZprvSk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####File Upload"
      ],
      "metadata": {
        "id": "A7fnpEhcvSk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMDB Dataset**    \n",
        "\n",
        "For  illustration purposes, this analysis uses the Kaggle IMDB dataset for the top 1000 movies to understand the features/traits of top IMDB movies by applying a 10 step-process.    *The Kaggle file used in this notebook differs from the source file used by Anmol Tomar in the article referenced above.*  \n",
        "\n",
        "A copy of the file is hosted on Professor Patrick's GitHub and can be accsed with `!curl` to upload a copy to the `content` folder in Colab.    \n"
      ],
      "metadata": {
        "id": "3C2GL6O8vSk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl \"https://raw.githubusercontent.com/ProfessorPatrickSlatraigh/data/main/IMDB_top_1000.csv\" -o imdb_top_1000.csv"
      ],
      "metadata": {
        "id": "CzjSM-alvSk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Import Packages    \n",
        "\n",
        "The load, discovery, and transformation steps will only require that the `pandas` and `matplotlib` packages be imported.    \n",
        "\n",
        "To add functionality to data tables in Colab, import `data_table` from `google.colab`."
      ],
      "metadata": {
        "id": "SJ8-Oi67vSk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import our packages \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "JIjYuZe2vSk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Read the Data into a Dataframe    \n"
      ],
      "metadata": {
        "id": "M8lw_FVsvSk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Reading the data from the current working directory    "
      ],
      "metadata": {
        "id": "G9ajesP1vSk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "df_movies =  pd.read_csv('imdb_top_1000.csv')"
      ],
      "metadata": {
        "id": "FMPtPewZvSk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Another means of reading the raw `.csv` content directly into a dataframe without the need to copy the file to the `content\\` folder"
      ],
      "metadata": {
        "id": "tq-Q8hBsvSk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(pd.read_fwf)"
      ],
      "metadata": {
        "id": "P4CcM6v3vSk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the pandas `.read_fwf` to read the raw data file contents directly from a web server\n",
        "df_movies = pd.read_fwf('https://raw.githubusercontent.com/ProfessorPatrickSlatraigh/data/main/IMDB_top_1000.csv', index=0)"
      ],
      "metadata": {
        "id": "ilM3GriJvSk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Summarize the Columns    \n",
        "\n"
      ],
      "metadata": {
        "id": "dpeHZNbxvSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of the columns   \n",
        "df_movies.describe(include = 'all')"
      ],
      "metadata": {
        "id": "NXZclUo1vSk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A detailed pivot table can be viewed as well to peruse the data.    "
      ],
      "metadata": {
        "id": "4Co_kn0HvSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query the detail in the dataframe\n",
        "df_movies"
      ],
      "metadata": {
        "id": "k3hyJ7cqvSk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Data Types    \n",
        "\n",
        "The next step is to do a sanity check of the data types of the columns of the dataframe. If there are some incorrect data types they can be corrected in this step."
      ],
      "metadata": {
        "id": "L12Lj9IgvSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the datatypes \n",
        "print(df_movies.dtypes)"
      ],
      "metadata": {
        "id": "z8iPnZqfvSk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The field `Duration` is a string (object data type) with the runtime in minutes followed by the term 'min'.  A new column can be generated for `runtime` as a float by assigning the result of a split of `Duration` and the transformation of the first element in the resulting list into a float. "
      ],
      "metadata": {
        "id": "RZ3jvVRevSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the numeric value of movie Duration to a new column Runtime \n",
        "df_movies[['Runtime', 'Unit']] = df_movies[\"Duration\"].str.split(' ', 1, expand=True)\n",
        "df_movies['Runtime'] = df_movies['Runtime'].astype(int)\n",
        "del df_movies['Unit']"
      ],
      "metadata": {
        "id": "ZCzGKwiRvSk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the dataframe after the transformation   \n",
        "df_movies"
      ],
      "metadata": {
        "id": "PrvJrgrnvSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another approach would simply replace the `Duration` column in place with it's float value\n",
        "# Transformting `Duration` into int \n",
        "\n",
        "# >>> Remove the comment from the following line of code to try it\n",
        "# df_movies['Duration'] = df_movies['Duration'].str.replace(' min','').astype(int)\n"
      ],
      "metadata": {
        "id": "JhcPK0EDvSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review the data types of each column again\n",
        "df_movies.dtypes"
      ],
      "metadata": {
        "id": "e2sBr4j6vSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3. Find Missing Values    \n",
        "\n",
        "The next step is to find the number of missing values across the columns of the dataframe. It’s important to understand the count of nulls so determine how best to treat them.    "
      ],
      "metadata": {
        "id": "r-JSe5nFvSk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find nulls or missing values\n",
        "df_movies.isnull().sum()"
      ],
      "metadata": {
        "id": "KSfQiA26vSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4. Missing Values Treatment    \n",
        "\n",
        "Using the count of missing values and any other descriptive statistics applicable, the next step is to treat the columns with missing values.    \n",
        "\n",
        "For illustration purposes, the nulls are filled with the mean value of the columns, although there are more sophisticated methods of missing value treatment.   \n"
      ],
      "metadata": {
        "id": "kmZ47kM0vSk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for potential values to use in replacing missing data \n",
        "\n",
        "print('The mean Metascore is ', df_movies['Metascore'].mean())\n",
        "\n",
        "print('The most frequent Certificate value is ', df_movies['Certificate'].mode())\n"
      ],
      "metadata": {
        "id": "LkwB8lo2vSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing nulls with mean for numeric values and mode for categorical values\n",
        "\n",
        "df_movies['Metascore'].fillna(df_movies['Metascore'].mean())\n",
        "\n",
        "df_movies['Certificate'].fillna(df_movies['Certificate'].mode())\n"
      ],
      "metadata": {
        "id": "R4M0oy5tvSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies.head()\n"
      ],
      "metadata": {
        "id": "zjaxXD8yvSk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####5. Find and Count Duplicates"
      ],
      "metadata": {
        "id": "H0bdSxv_vSk8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step requires some context and/or intuition regarding the data.  On first inspection, there may be data which appears to be redundant but is actually categorical data.  Most data sources will not be normalized so this is often the case.  Later in this workbook there is an exercise on Transformations and Feature Extraction which discusses the fact that some categorical data may have data entry or other errors which require their own special treatment.    \n",
        "\n",
        "Finding duplicates and wrangingling categorical data can sometimes be an iterative process of discovery, trial and error, then treatement.    \n",
        "\n",
        "The initial activity in dealing with duplicates includes:    \n",
        "\n",
        "1. Finding duplicates    \n",
        "2. Counting duplicates    \n",
        "3. Displaying duplicate rows with `loc`\n",
        "3. Determing duplicates treatment     \n",
        "\n",
        "The following snippets of code give examples of 1 and 2 above:    \n",
        "\n"
      ],
      "metadata": {
        "id": "O2UMozmIvSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicates across the dataframe   \n",
        "# Use the `.duplicated()` method\n",
        "df_movies.duplicated()"
      ],
      "metadata": {
        "id": "KMBWVIeSvSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To consider certain columns such as `Title` for identifying duplicates\n",
        "df_movies.duplicated(subset=['Title'])"
      ],
      "metadata": {
        "id": "zcGTVbk5vSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas `duplicated()` returns a boolean Series. However, it is not practical to see a list of True and False when we need to perform some data analysis. The Pandas `loc` data selector can be used to extract those duplicate rows:"
      ],
      "metadata": {
        "id": "5Hp5fr6WvSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of `loc` allows us to see the rows that were identified by duplicated()\n",
        "df_movies.loc[df_movies.duplicated(), :]"
      ],
      "metadata": {
        "id": "Wx_z7xxMvSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####6. Duplicates Treatment"
      ],
      "metadata": {
        "id": "UGTA5HiWvSk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fundamental concepts in duplicate treament to consider:\n",
        "\n",
        "1. Determing duplicates to retain with `keep`    \n",
        "2. Dropping duplicates      \n"
      ],
      "metadata": {
        "id": "H7rggV23vSk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retain duplicates with `keep`**\n",
        "\n",
        "There is an argument `keep` in Pandas duplicated() to determine which duplicates to mark. `keep` defaults to 'first', which means the first occurrence gets kept, and all others get identified as duplicates.   \n"
      ],
      "metadata": {
        "id": "ZctPNcJvvSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `keep` defaults to 'first'\n",
        "df_movies.loc[df_movies.duplicated(keep='first'), :]"
      ],
      "metadata": {
        "id": "1eC6BWrDvSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An argument can be used to change it to 'last' keep the last occurrence and mark all others as duplicates.  "
      ],
      "metadata": {
        "id": "AIsvAVq7vSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `keep` defaults to 'last'\n",
        "df_movies.loc[df_movies.duplicated(keep='last'), :]"
      ],
      "metadata": {
        "id": "XcbjPkv_vSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a third option we can use `keep=False`. It marks all duplicates as True and allows viewing all duplicate rows."
      ],
      "metadata": {
        "id": "bwe9CiUHvSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a third option we can use 'keep=False'. It marks all duplicates as True\n",
        "df_movies.loc[df_movies.duplicated(keep=False), :]"
      ],
      "metadata": {
        "id": "xd3kebwFvSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop duplicates**    \n",
        "\n",
        "The Pandas built-in method `drop_duplicates()` will drop duplicate rows."
      ],
      "metadata": {
        "id": "RITSliAtvSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the change in the number of rows before and after dropping duplicates\n",
        "# It is not performed in place by default,\n",
        "df_movies.drop_duplicates()"
      ],
      "metadata": {
        "id": "wS-sx88lvSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, this method returns a new DataFrame with duplicate rows removed. We can set the argument inplace=True to remove duplicates from the original DataFrame."
      ],
      "metadata": {
        "id": "OkfoSXT3vSk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the change in the number of rows before and after dropping duplicates\n",
        "# It is not performed in place by default, it can be changed it to in place by `inplace=True`df.drop_duplicates(inplace=True)\n",
        "df_movies.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "N4xQriUovSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The argument keep can be set for `drop_duplicates()` as well to determine which duplicates to keep. It defaults to `'first'` to keep the first occurrence and drop all other duplicates.    \n",
        "\n",
        "\n",
        "Similarly, `drop_duplicates` can be set `keep` to 'last' to keep the last occurrence and drop other duplicates."
      ],
      "metadata": {
        "id": "83rBwjL2vSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use keep='last' to keep the last occurrence \n",
        "df_movies.drop_duplicates(keep='last')"
      ],
      "metadata": {
        "id": "xallw6NuvSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`drop_duplicates` can also be set with `keep` assigned the value **False** to drop all duplicates."
      ],
      "metadata": {
        "id": "hTk2dhDxvSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To drop all duplicates\n",
        "df_movies.drop_duplicates(keep=False)"
      ],
      "metadata": {
        "id": "lcoKmRYAvSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considering certain columns for dropping duplicates**    \n",
        "\n",
        "\n",
        "Similarly, to consider certain columns for dropping duplicates, pass a list of columns to the argument subset:    \n"
      ],
      "metadata": {
        "id": "hueHLi5MvSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, we can consider a certain columns for dropping duplicates\n",
        "df_movies.drop_duplicates(subset=['Title'])"
      ],
      "metadata": {
        "id": "N9dw9aTjvSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and do that in-place."
      ],
      "metadata": {
        "id": "3DQJPtfrvSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a certain columns for dropping duplicates in-place\n",
        "df_movies.drop_duplicates(subset=['Title'], inplace=True)"
      ],
      "metadata": {
        "id": "1j5bFwrGvSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoking the `duplicated()` method will confirm that duplicate rows have been dropped. "
      ],
      "metadata": {
        "id": "ZFm9hRKNvSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of `loc` allows us to see the rows that were identified by duplicated()\n",
        "df_movies.loc[df_movies.duplicated(), :]"
      ],
      "metadata": {
        "id": "iOKhChwEvSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7. Analyze for Outliers    \n",
        "\n",
        "The next step is to check for outliers. There are multiple ways of checking the outliers, the graphical method vis presented here. Two continuous variables (`Metascore` and `Runtime`) have been select to be checked for outliers by evaluating a histogram for each column.    "
      ],
      "metadata": {
        "id": "ZfeYQx_gvSk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Metascores \n",
        "plt.hist(df_movies['Metascore'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Kk5Qd6NNvSk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the minimum `Metascore` \n",
        "df_movies['Metascore'].min()\n",
        "# output : ?"
      ],
      "metadata": {
        "id": "ls3ij9eqvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Runtimes \n",
        "plt.hist(df_movies['Runtime'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z5y2r52gvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the minimum `Runtime` \n",
        "df_movies['Runtime'].min()\n",
        "# output : ?"
      ],
      "metadata": {
        "id": "GGEsx3nqvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the maximum `Runtime` \n",
        "df_movies['Runtime'].max()\n",
        "# output : ?"
      ],
      "metadata": {
        "id": "_q_NXHqfvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Outlier Treatment\n",
        "\n",
        "\n",
        "The next step is to treat the outliers observed in the previous step. There are different ways of treating the outliers such as:     \n",
        "1. Capping the minimum and maximum value limits\n",
        "2. Removing the rows with outlier values   \n",
        "\n",
        "\n",
        "Although there is nothing off with the distribution of Metascores, for illustration purposes, the minimum `Metascore` value is capped at 65.    \n",
        "\n",
        "\n",
        "And with respect to the Runtimes, any rows with a `Runtime` in excess of 200 is deleted as are rows with a `Runtime` of less than 60.   \n",
        "\n"
      ],
      "metadata": {
        "id": "M9e4N-G9vSk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capping the minimum Metascore to 65\n",
        "df_movies.loc[df_movies['Metascore'] < 65,'Metascore'] = 65\n",
        "\n",
        "#check the minimum `Metascore` \n",
        "df_movies['Metascore'].min()\n",
        "# output : 65.0"
      ],
      "metadata": {
        "id": "AUL377UlvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The range of `Metascore` values can be evaluated again with a histogram.    \n"
      ],
      "metadata": {
        "id": "wMs75T8IvSk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Metascores \n",
        "plt.hist(df_movies['Metascore'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6h2M19P8vSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop runtimes of less than 60 or greater than 200.    \n",
        "\n",
        "*note the use of the* `|` *set operation to combine the results of each conditional test.*"
      ],
      "metadata": {
        "id": "-KxOYN59vSk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping any rows where Runtime exceeeds 200 or is below 60\n",
        "df_movies.drop(df_movies[(df_movies.Runtime > 200) | (df_movies.Runtime < 60)].index, inplace=True)  \n"
      ],
      "metadata": {
        "id": "fDqZsdtrvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the minimum `Runtime` \n",
        "df_movies['Runtime'].min()\n",
        "# output : 60"
      ],
      "metadata": {
        "id": "kRAaptezvSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the maximum `Runtime` \n",
        "df_movies['Runtime'].max()\n",
        "# output : 200?"
      ],
      "metadata": {
        "id": "1GVQjAo2vSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of Runtimes \n",
        "plt.hist(df_movies['Runtime'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1yY9GPJ2vSk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####9. Ask 'Who?'    \n",
        "\n",
        "\n",
        "The next step to answer are questions related to people, members, stakeholders, etc. \n",
        "\n",
        "In films, there are actors, directors, and cast members.  Some 'who' questions to raise could include:  \n",
        "\n",
        "* Who has directed the most number of top IMDB movies? (univariate)    \n",
        "\n",
        "\n",
        "* Who has acted in most top IMDB movies? (univariate)    \n",
        "\n",
        "\n",
        "* Which Actor-Director combination has the most top IMDB movies? (bivariate)    \n",
        "\n",
        "\n",
        "* Who provided the most music in top IMDB movies ? (Data not available)    \n",
        "\n",
        "\n",
        "And More …    \n",
        "\n"
      ],
      "metadata": {
        "id": "Kk1d_BvHvSk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*to use the code in the following snippet, extraction of data values from the `Cast` column would be required as a preliminary transformation.*\n"
      ],
      "metadata": {
        "id": "YgyfulEqvSlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### >>> The sample code here relies on columns in the original dataset used by Anmol Tomar\n",
        "### >>> The columns required by the code in this snippet are not available in the Kaggle dataset\n",
        "\n",
        "## Who has directed the most number of top IMDB movies ?\n",
        "# df_movies.groupby(['Director']).agg({'Series_Title':'count'}).reset_index().rename(columns = {'Series_Title':'count'}).\\\n",
        "# sort_values('count',ascending = False).head(5)\n",
        "\n",
        "## Who has acted in the most number of top movies \n",
        "# df_movies.groupby(['Star1']).agg({'Series_Title':'count'}).reset_index().rename(columns = {'Series_Title':'count'}).\\\n",
        "# sort_values('count',ascending = False).head(5)\n",
        "\n",
        "## Director - Actor works best \n",
        "# df_movies.groupby(['Director','Star1'])['Series_Title'].count().reset_index().\\\n",
        "# rename(columns = {'Series_Title':'Count'}).sort_values('Count',ascending = False).head(5)    \n"
      ],
      "metadata": {
        "id": "UCKS8KYUvSlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####10. Ask 'When?'    \n",
        "\n",
        "\n",
        "The next step is to answer questions related to the time dimension (year, quarter, month, week, day, time-of-day, hour, minute, etc.)    \n",
        "\n",
        "Considering film data, the following type of question could be asked:    \n",
        "\n",
        "\n",
        "* Find the years with most movies in IMDB top 1000 ? (univariate)    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hNBlXTOWvSlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*to use the code in the following snippet, extraction of data values from the `Title` column would be required as a preliminary transformation.*"
      ],
      "metadata": {
        "id": "75HW40iWvSlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### >>> The sample code here relies on columns in the original dataset used by Anmol Tomar\n",
        "### >>> The columns required by the code in this snippet are not available in the Kaggle dataset\n",
        "\n",
        "## Finding years with most movies in top 1000\n",
        "# year_dis = df_movies.groupby('Released_Year')['Series_Title'].count().reset_index().\\\n",
        "# rename(columns = {'Series_Title':'Count'}).sort_values('Count',ascending = False).head(10)\n",
        "\n",
        "# plt.bar(year_dis['Released_Year'].astype(str), year_dis['Count'], width = 0.5)\n",
        "# plt.xlabel('Years')\n",
        "# plt.ylabel('Number of Movies')\n",
        "# plt.title('Years with most movies in IMDB top 1000')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "hdm5cveRvSlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####11. Ask 'Where?'    \n",
        "\n",
        "The next step is to look at the things from the location or place perspective, for example, country, state, regions etc.  Geolocation can also be used in determing the 'where' attribute of data. For location features a variety of data elements may be used, including:    \n",
        "\n",
        "* Formal country, region, jurisdiction decriptors (see ISO codes)    \n",
        "\n",
        "* Postal codes\n",
        "\n",
        "* Longitude and Lattitude (GPS coordinates) \n",
        ">o may be express as degrees(°), minutes('), seconds(\")    \n",
        ">o may be expressed as decimal values (which can be translated to °, ', \")  \n",
        "> *note that there are 360° degrees around the earth, each minute' is 1/60th of a degree, and each second\" is 1/60th of a minute -- akin to minutes and seconds as fractions of hours when measuring time* \n",
        "\n",
        "* Mappings    \n",
        ">o phone number country codes and areas codes may generally map to location    \n",
        ">o IP address may map to location (consider VPN impacts)    \n",
        "\n",
        "\n",
        "For film data, the following question could be posed:    \n",
        "\n",
        "\n",
        "* Find countries with most movies in IMDB top 1000.\n",
        "\n",
        "\n",
        "The dataset does not have the data to answer this question.   \n",
        "Research should be as exhaustive as possible and not limited based on data availability.  Additional sources or enrichment approaches should be considered to answer pertinent questions.    \n",
        "\n"
      ],
      "metadata": {
        "id": "Fbs6hEpCvSlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####12. Ask 'What?  Which?  How?'    \n",
        "\n",
        "\n",
        "The next and final step is formulating questions about aspects not covered in the first nine steps. These questions are not related to people, place, or time but everything apart from these. Formulating such questions can be quite subjective and takes some time and experience to develop intuition and a facility.\n",
        "\n",
        "With respect to film data, such question might include:    \n",
        "\n",
        "* Which genres are featured most in the top 1000?\n",
        "\n",
        "\n",
        "* What is the duration of the top movies?\n",
        "\n",
        "\n",
        "* What is the correlation between the rating and gross earning?\n",
        "\n",
        "\n",
        "and more…\n",
        "\n",
        "For illustration purposes, the first question is approached using the following code:\n",
        "\n"
      ],
      "metadata": {
        "id": "fuAWiM27vSlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Which genres of film are featured most in top 1000 ? \n",
        "genre_dis = df_movies.groupby('Genre')['Title'].count().reset_index().\\\n",
        "rename(columns = {'Title':'Count'}).sort_values('Count',ascending = False).head(5)\n",
        "fig, ax = plt.subplots()\n",
        "plt.bar(genre_dis['Genre'], genre_dis['Count'], width = 0.5)\n",
        "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qThbbzw4vSlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A6nMa2M4vSlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can refine the exercises in this section through the inclusion of the requisite transformations to wrangle the data required for additional analysis.   \n"
      ],
      "metadata": {
        "id": "2gEhvqmlvSlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SHuR_viJvSlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####13. Transformations and Features Extraction    \n",
        "\n",
        "While some data may need to be sourced from additional research, there can be data embedded in compound/complex data types which through a process of analysis, and transformation may be extracted as useful features.  This process can utilize different tools and techniques, including:    \n",
        "\n",
        "* **Regex** (Regular Expressions)    \n",
        "\n",
        "* **NLP** (Natural Language Processing)    \n",
        "\n",
        "* **Image** Detection (for image blobs)  \n",
        "\n",
        "\n",
        "Once initial feature data is generated, it may need to be treated using the approach used for the raw data sourced, including:    \n",
        "\n",
        "*  Find and treat missing values    \n",
        "\n",
        "*  Find and treat outliers    \n",
        "\n",
        "*  Find and treat duplicates    \n",
        "\n",
        "*  Standardize/scrub categorical values (may be required to clean data)   \n",
        "\n"
      ],
      "metadata": {
        "id": "4R4yB-nevSlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MpsUVaRZvSlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results of Analysis    \n"
      ],
      "metadata": {
        "id": "GObi-953oSZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reports and Plots     \n"
      ],
      "metadata": {
        "id": "31QtLaSaoVuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Results     \n"
      ],
      "metadata": {
        "id": "pTdP-iHEolx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Findings    \n"
      ],
      "metadata": {
        "id": "9Mlks0rJ4QxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Themes    \n",
        "\n"
      ],
      "metadata": {
        "id": "SMJqnnHq4QTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implications    \n",
        "\n"
      ],
      "metadata": {
        "id": "Wyy9CAVY48-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Recommendations    \n"
      ],
      "metadata": {
        "id": "Yn8HoXIp5AMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Conclusion     \n"
      ],
      "metadata": {
        "id": "gmAtXSQnosvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plan of Action and Milestones     \n"
      ],
      "metadata": {
        "id": "5Aaw-Bk1oy1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Next Steps    \n"
      ],
      "metadata": {
        "id": "GOh4OKlz5Ewa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgSofQ0DR9_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix I - Footnotes    \n"
      ],
      "metadata": {
        "id": "0I48gB7t5IhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aWKvP93UR7df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix II - Glossary of Terms    \n"
      ],
      "metadata": {
        "id": "J05taHqH5d-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1PBy-yNbR6qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix III - Bibliography / Additional Reading    \n"
      ],
      "metadata": {
        "id": "BbO9vRS75hfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9r2AzaojPc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RESOURCES    \n",
        "**<h2>Data Sources to Consider</h2>**   \n",
        "\n",
        "* **General/Broad Data Sources** - Data sources across disciplines, time periods, and geographies\n",
        "* **Government and Political** -- Many global datasets; UN financial institution with investments and data on developing countries     \n",
        "* **Business and Commerce** - Better Business Bureau provides several APIs to research businesses, customer reviews, and complaints    \n",
        "* **Curated Lists of Job Search APIs** -- links to various APIs for job boards and other job search resources.    \n",
        "* **Finance** - Time-series market data and a variety of data sources about public companies    \n",
        "* **Social Media** - Social media postings and data from Twitter, etc.   \n",
        "* **Literary and Arts** - Open-source text of works of literature    \n",
        "* **Crime and Safety** - Aggregated law enforcement data from across the United States    \n",
        "* **Health and Wellness Data** - Various data sources     \n",
        "* **Weather and Climate** - Various data sources    \n",
        "* **Travel and Transport** - AirBnB, travel, and transport data sources    \n",
        "* **Traffic** - Data sources for information on traffic conditions -- some data may be in [Keyhole Markup Language **KML**](https://en.wikipedia.org/wiki/Keyhole_Markup_Language)  or  [Keyhole Markup Zipped **KMZ**](https://en.wikipedia.org/wiki/KMZ) formats.     \n",
        "* **Geolocation** - Data sources on Named Entity : Longitude/Lattitude maps, and other location-specific data   \n",
        "   \n",
        "\n",
        "*note: **Google** curates a [catalog of open data sources](google.com/publicdata/directory)*\n"
      ],
      "metadata": {
        "id": "HmsMn9RCjQgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>General/Broad Data Sources</u>    \n"
      ],
      "metadata": {
        "id": "9OL-7iOkWDHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data World    \n",
        "\n",
        "Data aggregator and tool provide with (free) limited Community Edition, (paid) Professional Edition, and (paid) Enterprise Plans at:  [data.world](https://data.world/pricing)"
      ],
      "metadata": {
        "id": "Vv_ZMmzENlWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Google Dataset Search    \n",
        "\n",
        "Facility provided by Google to search specifically for open data sets, available at:    \n",
        "\n",
        "`https://datasetsearch.research.google.com/`    \n",
        "\n"
      ],
      "metadata": {
        "id": "ZQMGcXBUwGA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Google Catalog of Open Data    \n",
        "\n",
        "place Google Catalog of Open Data info and links here.\n",
        "\n",
        "See `https://www.google.com/publicdata/directory` for the home page.    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iv4rE81xmwC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Kaggle     \n",
        "\n",
        "The following dataset contains five years of data regarding Netflix's stock prices. Ranging from February 5th, 2018 - February 5th, 2022.\n",
        "\n",
        "https://www.kaggle.com/datasets/jainilcoder/netflix-stock-price-prediction"
      ],
      "metadata": {
        "id": "UskbroIclq-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Our World in Data (OWID)    \n",
        "\n",
        "\n",
        "See the OWID Frequently Asked Questions at: https://ourworldindata.org/faqs    \n"
      ],
      "metadata": {
        "id": "-T6hey5olyjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Scholarly Research\n",
        "\n",
        "*from CUNY Mina Rees Library*\n",
        "\n",
        "\n",
        "See: https://libguides.gc.cuny.edu/c.php?g=405353&p=4857784 \n",
        "\n",
        "*...or search for 'CUNY, Library Guides, Digital Tools and Techniques, Scholarly Research APIs'*    \n"
      ],
      "metadata": {
        "id": "1FDZi8qkRGVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Statista     \n",
        "\n",
        "Paid service for customized research data and online commerce data at: [statista.com.](https://www.statista.com)    \n",
        "\n",
        "a portal for all things statistical including data that can be downloaded in a variety of file formats, reports, and infographics.    \n",
        "\n",
        "Statista is geared to students as well as professionals. Unlike many of our other resources, Statista has business and marketing data.    \n",
        "\n",
        "Access within the CityTech network may be available at:    \n",
        "`http://citytech.ezproxy.cuny.edu:2048/login?url=http://www.statista.com`\n",
        "\n"
      ],
      "metadata": {
        "id": "lDgAIU1bMSew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####UCI - University of California Irvine    \n",
        "\n",
        "[UCI Machine Learning Repository](https://archive-beta.ics.uci.edu/) is a popular platform for finding datasets for any data science project. Just like Kaggle, you can search for any dataset here by searching for your project topic. You will also find popular and new datasets here that you can use to practice as a beginner in data science.    \n"
      ],
      "metadata": {
        "id": "5hdPjaqv95iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Government and Political Data</u>    "
      ],
      "metadata": {
        "id": "wtYc4gD6WPxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Worldwide**"
      ],
      "metadata": {
        "id": "ziTVnVfTW6GJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####United Nations (UNdata)       \n",
        "\n",
        "\n",
        "\n",
        "**UNdata API** provides dynamic, programmatic access to data within the UNdata platform. Developers can use the API to dynamically query UNdata to obtain the latest data and display the result on a Web page, download to local storage for further processing, etc.\n",
        "\n",
        "**UNdata API** is powered by Eurostat’s SDMX Reference Infrastructure (SDMX-RI). The API is implemented as a REST and SOAP Web Service that can be used to query the datamarts using the SDMX standard. The API is governed by UNdata Terms of Use.\n",
        "\n",
        "**How To Use UNdata API**\n",
        "If you want to query data from UNData API we have two options for you. You can use REST API or the SOAP web service. In any of these cases it is recommended that you get familiarized with the main SDMX artifacts such as DataFlow, Codelist, Agency, Structure, etc.     \n",
        "\n",
        "\n",
        "Details at:  http://data.un.org/Host.aspx?Content=API    \n",
        "\n"
      ],
      "metadata": {
        "id": "AXetTnlKwY03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**United States of America**"
      ],
      "metadata": {
        "id": "QXEuUmCPW-K2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data.gov    \n",
        "\n",
        "The [Data.gov](https://data.gov ) program was started by the President of the United States in 2009 to provide open access to unclassified government data. These data are produced by all departments including:    \n",
        "\n",
        "* Executive Branch    \n",
        "* White House    \n",
        "* Cabinet Level Departments    \n",
        "* and other levels of governments    \n",
        "\n",
        "\n",
        "The datasets uploaded here can be used for Data Science projects like:\n",
        "* Economic Growth \n",
        "* Environmental Changes\n",
        "* Quality of Life and many more.     \n",
        "\n",
        "Like the data.gov you can also get data from other countries for the same data science project ideas on a different country, such as:    \n",
        "\n",
        "* **[data.gov.uk](https://data.gov.uk)**    \n",
        "* **[open.canada.ca](https://open.canada.ca)**    \n",
        "\n"
      ],
      "metadata": {
        "id": "f6pRUqRNmG6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####U.S. Census Bureau Data     \n",
        "\n",
        "Like other countries, the US Census is also held in every ten years. The data collected by the US Census is available at [census.gov](https://census.gov). This dataset can be used if you want to do any marketing or advertising related data science project. You can use this data for classification related projects also, where you can classify people according to:    \n",
        "\n",
        "* Age    \n",
        "* Income    \n",
        "* Household Size    \n",
        "* Gender    \n",
        "* Education    \n",
        "\n"
      ],
      "metadata": {
        "id": "pv2tZ6u-Kss8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**New York**"
      ],
      "metadata": {
        "id": "85nmdmmWXCFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NYC Open Data    \n",
        "\n",
        "*accessible with Socrata*\n",
        "\n",
        "\n",
        "Example - 311 Service Requests from 2010 to Present :\n",
        "\n",
        "https://dev.socrata.com/foundry/data.cityofnewyork.us/erm2-nwe9 \n",
        "\n",
        "Socrata Developer's Account :\n",
        "\n",
        "https://dev.socrata.com/\n",
        "\n",
        "Accessing NYC Open Data on LVNGD Blog*\n",
        "\n",
        "https://lvngd.com/blog/accessing-nyc-open-data-with-python-and-the-socrata-open-data-api/\n",
        "\n",
        "*note: the LVNGD blog is outdated with respect to some small details but presents the concepts clearly.*     \n",
        " \n"
      ],
      "metadata": {
        "id": "OzgpRLetmNC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[See the NYC Open Data Gallery](https://opendata.cityofnewyork.us/projects/) for great ideas about data science projects using open data.**    \n"
      ],
      "metadata": {
        "id": "m1O8HJxRcUBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Business and Commercial Data</u>"
      ],
      "metadata": {
        "id": "FDErOKbFWsC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Better Business Bureau (BBB)    \n",
        "\n",
        "Provides information on businesses in the U.S. at [several APIs](https://apievangelist.com/2016/08/04/thinking-about-the-better-business-bureau-api-in-context-of-the-overall-api-economy/), including:    \n",
        "\n",
        "* **[Organization Search](https://developers.bbb.org/documentation/endpoints/organization-search/)** - This endpoint will allow you to search for Organizations that are known to the BBB.  Organizations may include businesses or charities.    \n",
        "* **[Organization Collections](https://developers.bbb.org/documentation/endpoints/organization-collections/)** - BBB Partners will need to be able to identify collections of Organizations that they care about and be able to manage those collections and receive Organization info and updates on them as needed.     \n",
        "* **[Bulk Retrieval](https://developers.bbb.org/documentation/endpoints/bulk-retrieval/)** - This section defines the message specification that will be sent back to the API partners who request a bulk data transfer.     \n",
        "* **[Push Notifications](https://developers.bbb.org/documentation/endpoints/push-notifications/)** - BBB Partners have the need to be notified when BBB data updates occur for the records or a collection of records they are currently subscribed to.     \n",
        "* **[BBB Investigation/Application for Accreditation](https://developers.bbb.org/documentation/endpoints/bbb-investigation-application-for-accreditation/)** - This endpoint allows submission of organizations for BBB investigation and application for BBB accreditation.     \n",
        "\n"
      ],
      "metadata": {
        "id": "L7B2vCbxpX1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####UN ILOSTAT    \n",
        "**[International Labour Statistics](https://ilostat.ilo.org/)**    \n",
        "\n",
        "**from the United Nations**    \n",
        "\n"
      ],
      "metadata": {
        "id": "miV2hbqMZMry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####U.S. Bureau of Labor Statistics (BLS)    \n",
        "\n",
        "The Bureau of Labor Statistics' (**BLS**) Public Data Application Programming Interface (**API**) gives the public access to economic data from all **BLS** programs. It is the Bureau's hope that talented developers and programmers will use the **BLS** Public Data API to create original, inventive applications with published **BLS** data.\n",
        "\n",
        "The **BLS** Public Data API is currently available in two versions. Version 2.0 requires registration and allows users to access more data more frequently. Users may add calculations and annual averages to requests, and series description information is available for many **BLS** surveys. Version 1.0 is a more limited API that does not require registration and is open for public use.\n",
        "\n",
        "Using **BLS** API Signatures, developers and programmers can retrieve published historical timeseries data in JSON data-interchange or XLSX format. The **BLS** Public API utilizes two HTTP request-response mechanisms to retrieve data: GET and POST. GET requests data from a specified source. POST submits data to a specified resource to be processed. The BLS Public Data API uses GET to request a single piece of information and POST for all other requests. \n",
        "\n",
        "*note: more online at https://www.bls.gov/*"
      ],
      "metadata": {
        "id": "O726wlw5wftc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Curated Lists of Job Search APIs</u>    \n"
      ],
      "metadata": {
        "id": "GkGDWJwVyHj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####'Job Search API Basics' by [Google](https://cloud.google.com/talent-solution/job-search/v3/docs/basics)     "
      ],
      "metadata": {
        "id": "GL1udqjwy_Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####'90 Job Search APIs' by [AImultiple](https://aimultiple.com/job-search-api)      \n",
        "\n"
      ],
      "metadata": {
        "id": "sXvBqFvkyOGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####'Monster Job Search API Toolkit' by [Programmable Web](https://www.programmableweb.com/api/monster-web-services-toolkit-rpc-api-v1)      \n"
      ],
      "metadata": {
        "id": "eVHfyoJZzS2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####[Indeed Developer Portal](https://developer.indeed.com/)     \n"
      ],
      "metadata": {
        "id": "TA5c58Sbz2HM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "PzxcgjEvYrkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Financial Data</u>"
      ],
      "metadata": {
        "id": "oUMDdDcnWcTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####EDGAR (U.S. SEC)    \n",
        "\n",
        "`https://www.sec.gov/edgar/search-and-access`\n",
        "\n",
        "You can search information collected by the SEC using a variety of search tools. EDGAR full text search. New versatile tool lets you search for keywords and phrases in over 20 years of EDGAR filings, and filter by date, company, person, filing category, or location.; Boolean and advanced searching, including addresses.\n",
        "  \n"
      ],
      "metadata": {
        "id": "is50vYoPmmFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####World Bank    \n",
        "\n",
        "The world bank data is an international financial institution run by the United Nations. It is a statutory body that provides loans to developing countries. You can get the dataset made available by world bank from [data.worldbank.org](https://data.worldbank.org).    \n",
        "\n",
        "You can use the data made available by world bank for Data Science projects, such as:    \n",
        "\n",
        "* Agricultural and Rural Development analysis\n",
        "* Economic Growth Analysis\n",
        "* Science and Technological growth \n",
        "* The poverty level of a country"
      ],
      "metadata": {
        "id": "3rENQFIhJDd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Yahoo Finance       \n",
        "\n",
        "\n",
        "Real time low latency Yahoo Finance API for stock market, crypto currencies, and currency exchange.\n",
        "\n",
        "Nearly a dozen sources for different metrics and data related to financial markets at: https://www.yahoofinanceapi.com/ \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CMqlhKOmbGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Literary and Arts Data</u>    "
      ],
      "metadata": {
        "id": "wHV19-ryWjf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Project Gutenberg     \n",
        "\n",
        "To search for books with available text: https://www.gutenberg.org/ebooks/ \n",
        "\n",
        "See [Jonathan Reeve's site](https://jonreeve.com/2017/06/project-gutenberg-the-database/) for information about text mining using Project Gutenberg.  \n"
      ],
      "metadata": {
        "id": "xP8m-_Nul-I-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Crime and Safety Data</u>"
      ],
      "metadata": {
        "id": "jLl0LSY9Y1xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FBI Uniform Crime Reporting (UCR)    \n",
        "\n",
        "Aggregate data from law enforcement across the United States at: https://www.fbi.gov/services/cjis/ucr    \n"
      ],
      "metadata": {
        "id": "qVPmeZkUvH7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Social Media Data</u>"
      ],
      "metadata": {
        "id": "jm__ALNBXQ_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Twitter    \n",
        "\n",
        "Twitter APIs handle enormous amounts of data. \n",
        "\n",
        "The Twitter ensures this data is secured for developers and users alike is through authentication. \n",
        "\n",
        "There are a few methods for authentication:\n",
        "\n",
        "*   OAuth 1.0a User Context\n",
        "*   OAuth 2.0 Bearer Token\n",
        "*   Basic Authentication\n",
        "\n",
        "\n",
        "https://developer.twitter.com/en/docs/authentication/overview \n",
        "\n",
        "https://developer.twitter.com/en/docs/apps/overview \n",
        "\n",
        "https://developer.twitter.com/en/docs/twitter-api \n"
      ],
      "metadata": {
        "id": "m8KaWsB3QWfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Health and Wellness Data</u>    \n",
        "\n"
      ],
      "metadata": {
        "id": "1TBdrxQiVhd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The Human Mortality Database   \n",
        "\n",
        "See the information online at: https://www.mortality.org/"
      ],
      "metadata": {
        "id": "FEkfZqOVVlPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Climate and Weather Data</u>    "
      ],
      "metadata": {
        "id": "DyOv4390X7H8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Global Carbon Project     \n",
        "\n",
        "See the catalog of Global Carbon Project data sources at: https://www.globalcarbonproject.org/products/internetresources.htm#Data    \n"
      ],
      "metadata": {
        "id": "OB4kGFOsVRKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####NASA (National Aeronautic and Space Administration)    \n",
        "\n",
        "All of the datasets available at [nasa.gov](https://nasa.gov) have been continuously growing with the advancements of satellites and communication technologies. Now Nasa generates terabytes of data every day, which is equivalent to millions of mp3 files.    \n",
        "\n",
        "You can use datasets available at Nasa for Data Science projects such as:    \n",
        "\n",
        "* Astronomy and Space Analysis\n",
        "* Climate change analysis\n",
        "* Life Sciences\n",
        "* Geology"
      ],
      "metadata": {
        "id": "BPAbobGNJhkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####NOAA GeoPlatform\n",
        "\n",
        "[NOAA's Geoplatform](https://www.climate.gov/maps-data/all) - Data, maps, analytics on weather for U.S. National Oceanographic and Atmospheric Administration.     \n"
      ],
      "metadata": {
        "id": "mc0SugpcS_bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Open Weather Map \n",
        "\n",
        "[OpenWeatherMap](https://openweathermap.org/weathermap) - Dynamic weather maps.    \n",
        "OpenWeatherMap for data about the weather. [Documentation](http://openweathermap.org/current#geo).     \n",
        "\n",
        "OpenWeatherMap contains parameters as part of the URL, including an `appid` which is a key that is used to limit the number of calls that can be issued by a single application. \n",
        "\n",
        "*(Note: The `%20` is a transformation for the space (` `) character in URLs.)*\n",
        "\n",
        "[**You will need to get an API key**](https://openweathermap.org/price)    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BaySwbV-5453"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Virtual Crossing Weather\n",
        "\n",
        "[Virtual Crossing](https://virtualcrossing.com)     \n",
        "\n",
        "Visual Crossing is a leading provider of weather data and enterprise analysis tools to data scientists, business analysts, professionals, and academics. Founded in 2003, their mission has always been to empower data consumers and analysts to make better decisions based on high-quality, easy-to-access data.    \n",
        "\n",
        "[**You will need to get a (free) API key**](https://www.visualcrossing.com/weather-data-editions).    \n",
        "\n"
      ],
      "metadata": {
        "id": "UqeHIRhlZ4in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Travel and Transport</u>    "
      ],
      "metadata": {
        "id": "TxR42FTocivm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####AirBnB  \n",
        "\n",
        "Data provided by [**AirBnB**](http://insideairbnb.com)    \n"
      ],
      "metadata": {
        "id": "BmoxVbGScnFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Traffic</u>    \n",
        "\n",
        "Data sources for information on traffic conditions -- some data may be in [Keyhole Markup Language **KML**](https://en.wikipedia.org/wiki/Keyhole_Markup_Language)  or  [Keyhole Markup Zipped **KMZ**](https://en.wikipedia.org/wiki/KMZ) formats.     "
      ],
      "metadata": {
        "id": "eir9uRYn4yoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---  "
      ],
      "metadata": {
        "id": "D4dCOY804_bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<u>Map and Geographic Data</u>\n"
      ],
      "metadata": {
        "id": "FGv6lsxwYRwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####[Natural Earth Vector repos](https://github.com/nvkelso/natural-earth-vector/)    \n",
        "Provides a variety of well-curated `geojson` and other vector files for mapping.      \n",
        "This resource is maintained by volunteers, open source, and free.   \n",
        "The **GitHub** repos are maintained and explained by [Natural Earth](https://www.naturalearthdata.com/features/)      "
      ],
      "metadata": {
        "id": "nxrRK2QYVZrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####OpenDataSoft    \n",
        "\n",
        "[Map data for USA states and counties](https://public.opendatasoft.com/explore/dataset/georef-united-states-of-america-county/)      "
      ],
      "metadata": {
        "id": "lOyzuS16_6Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####GeoJSON Viewer/Builder from MapBox     \n",
        "\n",
        "Use this free online resource to inspect, test, and refine `geojson` files:    \n",
        "\n",
        " o [GeoJSON.io by MapBox](https://geojson.io/#map=2/0/20)\n"
      ],
      "metadata": {
        "id": "GLKQpNHntqiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####GeoJson and KML Data     \n",
        "GeoJason and KML data for the United States in available in varying levels of detail by state, county, and congressional district from the following:    \n",
        "* [**GEOJSON AND KML DATA FOR THE UNITED STATES**](https://eric.clst.org/tech/usgeojson/)    \n"
      ],
      "metadata": {
        "id": "2E1W3IVU1lAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BaseMaps     \n",
        "\n",
        "Leaflet basemaps are available from the following:    \n",
        "\n",
        "* [**Github: leaflet-extras**](https://leaflet-extras.github.io/leaflet-providers/preview/)      \n"
      ],
      "metadata": {
        "id": "8OjpilMq1ogP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####geoIP Resolution    \n",
        "\n",
        "**geoIP resolution** - takes the IP of a device and returns back its location.\n",
        "\n",
        "You will need to get an API here from here: https://ipstack.com/    \n"
      ],
      "metadata": {
        "id": "xnJGW5zHR1hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Geo-Location APIs      \n",
        "\n",
        "*This section is a work-in-progress*    "
      ],
      "metadata": {
        "id": "QA4tll5iRv7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**leaflet API**    \n",
        "\n",
        "Leaflet API documentation on objects, methods, parameters, and more available from the following:    \n",
        "\n",
        "* [**Leaflet - an open-source JavaScript library for mobile-friendly interactive maps**](https://leafletjs.com/reference-1.6.0.html#path-option)   "
      ],
      "metadata": {
        "id": "Xp6XDGqh1tfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "wejW5SAiviba"
      }
    }
  ]
}