{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CST3512/blob/main/CST3512_Class18_TimeSeries_AirPassenger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qv1jTpsGbYr"
      },
      "source": [
        "#Time Series  Analysis - Airline Passengers       \n",
        "\n",
        "*from a [blog entry, Medium article](https://medium.com/@stallonejacob/time-series-forecast-a-basic-introduction-using-python-414fcb963000), and [GitHub repos](https://github.com/jacobstallone/Time_Series_ARIMA--Blog-and-code-) entitled 'Time Series ARIMA' by Jacob Stallone, on November 9, 2019.*     \n",
        "\n",
        "Data is courtesy of [Jacob Stallone's copy](https://github.com/jacobstallone/Time_Series_ARIMA--Blog-and-code-/blob/master/AirPassengers.csv) of an [original Kaggle source](https://www.kaggle.com/datasets/chirag19/air-passengers).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Time Series Forecast : A basic introduction using Python**    \n",
        "\n",
        "\n",
        "\n",
        "Time series data is an important source for information and strategy used in various businesses. From a conventional finance industry to education industry, they play a major role in understanding a lot of details on specific factors with respect to time. I recently learnt the importance of Time series data in the telecommunication industry and wanted to brush up on my time series analysis and forecasting information. \n",
        "\n",
        "\n",
        "This notebook works through a simple example using Python.    \n",
        "\n",
        "\n",
        "Time series forecasting is basically the machine learning modeling for Time Series data (years, days, hour, etc.) for predicting future values using Time Series modeling. This approach helps if your data in serially correlated.  \n",
        "\n"
      ],
      "metadata": {
        "id": "jb8p8XG7VGGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lkQg9-tUgas6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekPp6hqzGbYz"
      },
      "source": [
        "##Housekeeping: Import Modules, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekSIStSKGbY0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np    \n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 15, 6\n",
        "\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIYbRGl8GbY1"
      },
      "source": [
        "https://matplotlib.org/users/customizing.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVPFx4wGGbY2"
      },
      "source": [
        "The dataset is from kaggle , Let's explore it using a copy that Jacob Stallone posts on his GitHub.   Pull that file into the current working directory with `!curl` as follows.    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl \"https://raw.githubusercontent.com/jacobstallone/Time_Series_ARIMA--Blog-and-code-/master/AirPassengers.csv\"  -o AirPassengers.csv"
      ],
      "metadata": {
        "id": "Co7ZqmCyGg2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76604402-46fc-4177-c5a0-55e0104a6fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1746  100  1746    0     0   8953      0 --:--:-- --:--:-- --:--:--  8953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read the dataset into a Pandas dataframe called `data`."
      ],
      "metadata": {
        "id": "j8P_QZvyJSn8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jGRVLuaGbY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4166d2b8-6585-46ca-bef8-54787ab4c2b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Month  #Passengers\n",
            "0  1949-01          112\n",
            "1  1949-02          118\n",
            "2  1949-03          132\n",
            "3  1949-04          129\n",
            "4  1949-05          121\n",
            "\n",
            " Data Types:\n",
            "Month          object\n",
            "#Passengers     int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('AirPassengers.csv')\n",
        "print(data.head())\n",
        "print('\\n Data Types:')\n",
        "print(data.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B5XBCUD7gZFK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98D7PcAgGbY4"
      },
      "source": [
        "##Dataset Description "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuJdkDRNGbY4"
      },
      "source": [
        "The data contains a particular month and number of passengers travelling in that month. The data type here is object (month).     \n",
        "\n",
        "Convert it into a Time series object and use the Month column as the index.    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBEx8ZHNGbY5"
      },
      "source": [
        "\n",
        "\n",
        "Timestamps are useful objects for comparisons.     \n",
        "\n",
        "Create a timestamp object with the `pd.to_datetime()` function and a string specifying the date. These timestamps are useful for logical filtering with dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsURr0wPGbY6"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "con=data['Month']\n",
        "data['Month']=pd.to_datetime(data['Month'])\n",
        "data.set_index('Month', inplace=True)\n",
        "#check datatype of index\n",
        "data.index"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data type is `datetime64[ns]`. For this notebook example it is read into a series rather than a dataframe but the approach works regardless of whether the data is in a series or a dataframe.   "
      ],
      "metadata": {
        "id": "Sl7iOZD9WGrI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9v6yzqbGbY7"
      },
      "outputs": [],
      "source": [
        "#convert to time series:\n",
        "ts = data['#Passengers']\n",
        "ts.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZNGnRzWGbY7"
      },
      "source": [
        "###Explore properties of the date-time based index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6V6UDGeGbY8"
      },
      "outputs": [],
      "source": [
        "#1. Specify the index as a string constant:\n",
        "ts['1949-01-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVM6LaMRGbY8"
      },
      "outputs": [],
      "source": [
        "#2. Import the datetime library and use 'datetime' function:\n",
        "from datetime import datetime\n",
        "ts[datetime(1949,1,1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsjlSw3HGbY8"
      },
      "outputs": [],
      "source": [
        "#1. Specify the entire range:\n",
        "ts['1949-01-01':'1949-05-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-YA4pZPGbY9"
      },
      "outputs": [],
      "source": [
        "#2. Use ':' if one of the indices is at an end\n",
        "ts[:'1949-05-01']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu4acX5UGbY9"
      },
      "outputs": [],
      "source": [
        "#All rows of 1962:\n",
        "ts['1949']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "84-XPu1BgW3G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcy9BeskGbY-"
      },
      "source": [
        "##**Stationarity**    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Stationarity** is a very important concept in Time Series Analysis. In order to apply a time series model, it is important for the Time series to be stationary; in other words all its statistical properties (mean,variance) remain constant over time. This is done basically because if you take a certain behavior over time, it is important that this behavior is same in the future in order for us to forecast the series.     \n",
        "\n",
        "There are a lot of statistical theories to explore stationary series than non-stationary series.  In practice we can assume the series to be stationary if it has constant statistical properties over time and these properties can be:\n",
        "* constant **mean**\n",
        "* constant **variance**\n",
        "* an **auto-co-variance** that does not depend on time       \n",
        "\n",
        "\n",
        "These details can be easily retrieved using stat commands in python.\n",
        "The best way to understand you stationarity in a Time Series is by visually inspecting the plot.    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Some additional housekeeping to import plot converters*    "
      ],
      "metadata": {
        "id": "IFyOoA0EKHm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQweF5_JGbY-"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbFa3wffGbY-"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNOiQYUHGbY_"
      },
      "source": [
        "It’s clear from the plot that there is an overall increase in the trend and with some seasonality in it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Rqv9t14WgU1M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHdPb116GbY_"
      },
      "source": [
        "###Stationarity Testing "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Stallone has provided a function for testing **stationarity** which will be used often in this Time Series notebook. A description of the concepts in the function follows.    \n",
        "\n",
        "**Plotting Rolling Statistics** - The function will plot the moving mean or moving Standard Deviation. This is still a visual method.     \n",
        "\n",
        "*NOTE: moving mean and moving standard deviation — At any instant ‘t’, we take the mean/std of the last year which in this case is 12 months.*\n",
        "\n",
        "**Dickey-Fuller Test** - This is one of the statistical tests for checking stationarity. First we consider the null hypothesis: the time series is non- stationary. The result from the rest will contain the test statistic and critical value for different confidence levels. The idea is to have Test statistics less than critical value, in this case we can reject the null hypothesis and say that this Time series is indeed stationary.    \n",
        "\n",
        "\n",
        "More details for Dickey-Fuller Test.\n",
        "\n",
        "\n",
        "Function details:\n",
        "\n",
        "* **mean**    \n",
        "* **Standard Deviation** (instead of variance)    \n",
        "* **Plot Original Series**     \n",
        "* **Plot Mean**     \n",
        "* **Plot std**    \n",
        "* **Plot Dickey-Fuller Test**    \n",
        "\n"
      ],
      "metadata": {
        "id": "aJtwWBZEX_xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Additional housekeeping to import adfuller from statsmodels to test stationarity*"
      ],
      "metadata": {
        "id": "k6Bt9z7cKgop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "    "
      ],
      "metadata": {
        "id": "O9qqK3eJKbdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Based on the deprecation warning in the execution of the last snippet, the following import or some derivative thereof may warrant exection.*"
      ],
      "metadata": {
        "id": "ve7HGfYOLMra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No need to run this import if no deprecation warning on last snippet\n",
        "import pandas.util.testing as tm"
      ],
      "metadata": {
        "id": "-XfC3hhALF1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to perform various tests of stationarity as described above:    \n",
        "\n",
        "* **mean**    \n",
        "* **Standard Deviation** (instead of variance)    \n",
        "* **Plot Original Series**     \n",
        "* **Plot Mean**     \n",
        "* **Plot std**    \n",
        "* **Plot Dickey-Fuller Test**    \n"
      ],
      "metadata": {
        "id": "raVTAp_3Kr8h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU-nClzMGbY_"
      },
      "outputs": [],
      "source": [
        "def test_stationarity(timeseries):\n",
        "    \n",
        "    #Determing rolling statistics\n",
        "    rolmean = pd.Series(timeseries).rolling(window=12).mean()\n",
        "    rolstd = pd.Series(timeseries).rolling(window=12).std()\n",
        "\n",
        "    #Plot rolling statistics:\n",
        "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    #Perform Dickey-Fuller test:\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print(dfoutput)    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the tests of stationarity for the time series `ts` by parsing the time series data into the new `test_stationarity()` function.\n"
      ],
      "metadata": {
        "id": "tbs2_TapK3Be"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNfJHJWTGbZA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "test_stationarity(ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This series is not **stationary** because:    \n",
        "\n",
        "* **mean** is increasing even though the **std** is small.\n",
        "* **Test Stat** is > critical value.\n",
        "\n",
        "\n",
        "*Note: the signed values are compared and the absolute values.*    \n",
        "\n"
      ],
      "metadata": {
        "id": "j-iW_dfiYzp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ElM278KegStP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bmAuOwDGbZA"
      },
      "source": [
        "###Transforming the Times Series for Stationarity    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAKING THE TIME SERIES STATIONARY**   \n",
        "\n",
        "\n",
        "There are two major factors that make a time series non-stationary. They are:    \n",
        "* Trend: non-constant **mean**    \n",
        "* Seasonality: **variation pattern** at certain periods or time-frames    \n",
        "\n",
        "\n",
        "The basic idea is to model the trend and seasonality in this series, so we can remove it and make the series stationary. Then we can go ahead and apply statistical forecasting to the stationary series. Then. finally convert the forecasted values into original by applying the trend and seasonality constraints back to those that we previously separated.\n",
        "\n",
        "In summary, the approach is:\n",
        "\n",
        "1. understand and model the trend\n",
        "2. remove the trend    \n",
        "3. understand and model the seasonality\n",
        "4. remove the seasonality    \n",
        "5. understand and model the underlying result\n",
        "6. bulid a model which constucts predictions by - \n",
        "    * using the #5 model    \n",
        "    * layering in seasonality to enrich #5 model results\n",
        "    * layering in trend model to enrich those results\n",
        "\n",
        "\n",
        "Let’s start by working on the trend piece.    \n",
        "\n"
      ],
      "metadata": {
        "id": "lF3AsS_jZ_Iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Bo9GODBUgLL7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5wWqiQuGbZA"
      },
      "source": [
        "###**Trend Analysis**    \n",
        "\n",
        "The first step is to reduce the trend using some transformation, as we can see here that there is a strong positive trend. These transformation can be log, sq-rt, cube root etc . Basically it penalizes larger values more than the smaller. In this case we will use the logarithmic transformation.    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying a `log()` function to dampen large values.    "
      ],
      "metadata": {
        "id": "Yx49yqhTLsYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGFrzBKwGbZB"
      },
      "outputs": [],
      "source": [
        "ts_log = np.log(ts)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is some noise in realizing the forward trend here. There are some methods to model these trends and then remove them from the series. Some of the common ones are:    \n",
        "\n",
        "* **Smoothing** - using rolling/moving average\n",
        "* **Aggression** - by taking the mean for a certain time period (year/month)\n",
        "\n",
        "\n",
        "The example which follows in this notebook uses **Smoothing**.    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s84iOuT2auoC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_sem-cRGbZB"
      },
      "source": [
        "###Smoothing the Series    \n",
        "\n",
        "\n",
        "\n",
        "**Smoothing**     \n",
        "\n",
        "In smoothing we usually take the past few periods or instances (rolling estimates).  Two methods of smoothing are considered in this notebook:  **Moving Average**, and **Exponentially Weighted Moving Average (EWM)**.    \n",
        "\n",
        "\n",
        "**Moving Average** -    \n",
        "\n",
        "First take n consecutive values (depending on the frequency if it is 1 year of monthly data, then take 12 values) and sum the values for that range of period then divide by the number of periods in the range. Pandas has a function for rolling estimates.    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdPiErkqGbZB"
      },
      "source": [
        "####Applying a Moving Average    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deriving a 12-month moving average of the data.     \n",
        "\n",
        "*note that any moving average of `n` periods will result in the creation of `null` values for the first `n-1` periods.*    \n"
      ],
      "metadata": {
        "id": "7rYZ8OdJMOIF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed6ZT2PnGbZC"
      },
      "outputs": [],
      "source": [
        "moving_avg = pd.Series(ts_log).rolling(12).mean()\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log)\n",
        "plt.plot(moving_avg, color='red')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now subtract the rolling mean from the original series to calculate the period change in the moving average.    "
      ],
      "metadata": {
        "id": "uxhCLMVQMohm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dvMBllOGbZC"
      },
      "outputs": [],
      "source": [
        "ts_log_moving_avg_diff = ts_log - moving_avg\n",
        "ts_log_moving_avg_diff.head(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop the `null` or `NaN` values which resulted from calculating `n` number of periods in the moving average.    \n"
      ],
      "metadata": {
        "id": "w_aBe9GzMwog"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgHyykDjGbZC"
      },
      "outputs": [],
      "source": [
        "ts_log_moving_avg_diff.dropna(inplace=True)\n",
        "ts_log_moving_avg_diff.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason there are null values is because we take the average of first 12 so 11 values are null. We can also see that in the visual representation. Thus it is dropped for further analysis. Now let’s parse it to the function to check for stationarity.    \n",
        "\n",
        "\n",
        "Invoke the `test_stationarity()` function defined earlier, using the newly calculated differences in the moving average of log function (`ts_log_moving_avg_diff`)    \n"
      ],
      "metadata": {
        "id": "_boDtmrJM-C6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2nHWDJEGbZC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "test_stationarity(ts_log_moving_avg_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice two things:    \n",
        "* The rolling values are varying slightly but there is no specific trend.\n",
        "* The test statistics is smaller than the 5 % critical values.     \n",
        "\n",
        "\n",
        "That tells us that we are 95% confident that this series is stationary.    \n",
        "\n",
        "\n",
        "In this example we can easily take a time period (12 months for a year), but there are situations where the time period range is more complex like stock price etc. So we use the exponentially weighted moving average (there are other weighted moving averages but for starters, lets use this). The previous values are assigned with a decay factor. Pandas again comes to the rescue with some awesome functions for it, like:"
      ],
      "metadata": {
        "id": "gEvjHOsXeiOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the exponentially-weighted moving average using the `ewm()` method."
      ],
      "metadata": {
        "id": "fbduCJK3NZVR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Boo5qwyYGbZD"
      },
      "source": [
        "###### Exponentially weighted moving average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrBW8DrsGbZD"
      },
      "outputs": [],
      "source": [
        "expwighted_avg = ts_log.ewm(halflife=12).mean()\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log)\n",
        "plt.plot(expwighted_avg,color='red')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameter (halflife) is assumed to be 12, but that really depends on the characteristics of the data in the domain. Let’s check stationarity now."
      ],
      "metadata": {
        "id": "4ZhdMpgTexR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the difference in the exponentially-weighted moving average from the log of the time series (`ts_log_ewma_diff`) and testing for stationarity."
      ],
      "metadata": {
        "id": "yI0G2ILUNvh9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WunFxlHaGbZD"
      },
      "outputs": [],
      "source": [
        "ts_log_ewma_diff = ts_log - expwighted_avg\n",
        "plt.figure(figsize=(20,10))\n",
        "test_stationarity(ts_log_ewma_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is stationary because:    \n",
        "\n",
        "* Rolling values have less variations in **mean** and **standard deviation** in magnitude.    \n",
        "* the **Test Statistic** is smaller than 1% of the critical value. So we can say we are almost 99% confident that this is stationary.    \n",
        "\n"
      ],
      "metadata": {
        "id": "RfQZRgU1f5xa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "It5nL5UFgFrP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8gAJE0CGbZD"
      },
      "source": [
        "###Analyzing and Treating Seasonality      \n",
        "\n",
        "**(Along with the Trend)**     \n",
        "\n",
        "\n",
        "Previously we saw just trend part of the time series, now we will see both trend and seasonality. Most Time series have trends along with seasonality. There are two common methods to remove trend and seasonality, they are:    \n",
        "* **Differencing** - by taking difference using time lag    \n",
        "* **Decomposition** -  model both trend and seasonality, then remove them\n",
        "\n",
        "**Differencing**    \n",
        "\n",
        "First take the difference of the value at a particular time with that of the previous time. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating a difference based on a shift in period. "
      ],
      "metadata": {
        "id": "AwIjCrV5OTVb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "575aMwOHGbZD"
      },
      "outputs": [],
      "source": [
        "#Take first difference:\n",
        "ts_log_diff = ts_log - ts_log.shift()\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBrGwIODGbZE"
      },
      "outputs": [],
      "source": [
        "ts_log_diff.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop missing (`NaN`) values and parse it using our stationary testing function.    "
      ],
      "metadata": {
        "id": "TbuSwbv2Of8d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-LhrBccGbZE"
      },
      "outputs": [],
      "source": [
        "ts_log_diff.dropna(inplace=True)\n",
        "plt.figure(figsize=(20,10))\n",
        "test_stationarity(ts_log_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is stationary because:    \n",
        "* the **mean** and **std variations** have small variations with time.    \n",
        "* **Test Statistic** is less than 10% of the critical values, so we can be 90 % confident that this is stationary.    "
      ],
      "metadata": {
        "id": "dA1fT3cVj-NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IAGWXd5skInj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decomposing**    \n",
        "\n",
        "Here we model both the trend and the seasonality, then the remaining part of the time series is returned. Pandas has a  function for it. Let’s check it out."
      ],
      "metadata": {
        "id": "GGMVg65EkMtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Additional housekeeping to import `seasonal_decompose` for analysis and treatment of seasonality.*    \n",
        " "
      ],
      "metadata": {
        "id": "LOsxQGQ9OnOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "decomposition = seasonal_decompose(ts_log)"
      ],
      "metadata": {
        "id": "7ra9lLGUO1os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze a decomposition of the `trend`, it's `seasonal` component and the `residual` value.    \n"
      ],
      "metadata": {
        "id": "roxsf_8fO3e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7r9l4qUGbZE"
      },
      "outputs": [],
      "source": [
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(411)\n",
        "plt.plot(ts_log, label='Original')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(412)\n",
        "plt.plot(trend, label='Trend')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(413)\n",
        "plt.plot(seasonal,label='Seasonality')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(414)\n",
        "plt.plot(residual, label='Residuals')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove the trend and seasonality from the Time series and now we can use the residual values. Let’s check stationarity."
      ],
      "metadata": {
        "id": "RTDk23HpktTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing the seasonality residual values and invoking the `test_stationarity` function defined earlier using residuals (`ts_log_decompose`) as an argument.    \n"
      ],
      "metadata": {
        "id": "iHa9mjuIPPrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s3iZHcbGbZE"
      },
      "outputs": [],
      "source": [
        "ts_log_decompose = residual\n",
        "ts_log_decompose.dropna(inplace=True)\n",
        "plt.figure(figsize=(20,10))\n",
        "test_stationarity(ts_log_decompose)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This is stationary because:    \n",
        "* **Test Statistic** is lower than 1% critical values.    \n",
        "* the **mean** and **std variations** have small variations with time.   \n",
        "\n"
      ],
      "metadata": {
        "id": "K7eJtS0Kk-Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y-N71DhXk_Ox"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONvB79TWGbZF"
      },
      "source": [
        "##Forecasting a Time Series (AR-I-MA) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLvL-BgnGbZF"
      },
      "source": [
        "###ACF & PACF  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About ACF and PACF**    \n",
        "\n",
        "\n",
        "*from [Significance of ACF and PACF in Time Series Analysis, TowardDataScience](https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8)*    \n",
        "\n",
        "\n",
        "\n",
        "**ACF** is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values. We plot these values along with the confidence band and tada! We have an ACF plot. In simple terms, it describes how well the present value of the series is related with its past values. A time series can have components like trend, seasonality, cyclic and residual. ACF considers all these components while finding correlations hence it’s a ‘complete auto-correlation plot’.    \n",
        "\n",
        "\n",
        "**PACF** is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value hence ‘partial’ and not ‘complete’ as we remove already found variations before we find the next correlation. So if there is any hidden information in the residual which can be modeled by the next lag, we might get a good correlation and we will keep that next lag as a feature while modeling. Remember while modeling we don’t want to keep too many features which are correlated as that can create multicollinearity issues. Hence we need to retain only the relevant features.    \n",
        "\n"
      ],
      "metadata": {
        "id": "0aSQtcg_REnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional housekeeping to import plot methods `plot_acf()` and `plot_pacf()`.   "
      ],
      "metadata": {
        "id": "zC_5vNatP9_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
      ],
      "metadata": {
        "id": "ztNeMQWJQPkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot of ACF, PACF for the difference in log of time series (`ts_log_diff`).      "
      ],
      "metadata": {
        "id": "EqMdv79sQR3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DETuhe04GbZF"
      },
      "outputs": [],
      "source": [
        "plot_acf(ts_log_diff, lags =20)\n",
        "plot_pacf(ts_log_diff, lags =20)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Housekeeping to import `ARIMA` for Auto-Regressive in Moving Average analysis.    \n"
      ],
      "metadata": {
        "id": "8r4txnAsR2xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "#ACF and PACF plots:\n",
        "from statsmodels.tsa.stattools import acf, pacf  "
      ],
      "metadata": {
        "id": "9TeZJRMXSJic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqxvCzcAGbZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "lag_acf = acf(ts_log_diff, nlags=12)\n",
        "lag_pacf = pacf(ts_log_diff, nlags=12, method='ols')\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "#Plot ACF:    \n",
        "plt.subplot(121)    \n",
        "plt.plot(lag_acf)\n",
        "plt.axhline(y=0,linestyle='--',color='gray')\n",
        "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
        "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
        "plt.title('Autocorrelation Function')\n",
        "\n",
        "#Plot PACF:\n",
        "plt.subplot(122)\n",
        "plt.plot(lag_pacf)\n",
        "plt.axhline(y=0,linestyle='--',color='gray')\n",
        "plt.axhline(y=-1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
        "plt.axhline(y=1.96/np.sqrt(len(ts_log_diff)),linestyle='--',color='gray')\n",
        "plt.title('Partial Autocorrelation Function')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Auto-Regressive Model**   \n",
        "\n",
        "\n",
        "**Auto regressive (AR) process** - a time series is said to be AR when present value of the time series can be obtained using previous values of the same time series *i.e.* the present value is weighted average of its past values. Stock prices and global temperature rise can be thought of as an AR processes.    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CsmErR43SPtZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Sd_Jy0CGbZF"
      },
      "outputs": [],
      "source": [
        "#AR model\n",
        "model = ARIMA(ts_log, order=(2,1,0))\n",
        "results_AR = model.fit(disp=-1)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log_diff)\n",
        "plt.plot(results_AR.fittedvalues, color='red')\n",
        "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues - ts_log_diff)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Moving Average Model**    \n",
        "\n",
        "**Moving average (MA) process** -  a process where the present value of series is defined as a linear combination of past errors. We assume the errors to be independently distributed with the normal distribution.     \n"
      ],
      "metadata": {
        "id": "y8sjS-VcSXWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1fT374IGbZG"
      },
      "outputs": [],
      "source": [
        "#MA model\n",
        "model = ARIMA(ts_log, order=(0,1,2))\n",
        "results_MA = model.fit(disp=-1)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log_diff)\n",
        "plt.plot(results_MA.fittedvalues, color='red')\n",
        "plt.title('RSS: %.4f'% sum((results_MA.fittedvalues - ts_log_diff)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto-Regressive in Moving Average Model**    \n"
      ],
      "metadata": {
        "id": "13_Wv4p2SdIp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESUqdeBXGbZG"
      },
      "outputs": [],
      "source": [
        "#ARIMA model\n",
        "model = ARIMA(ts_log, order=(2,1,2))\n",
        "results_ARIMA = model.fit(disp=-1)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts_log_diff)\n",
        "plt.plot(results_ARIMA.fittedvalues, color='red')\n",
        "plt.title('RSS: %.4f'% sum((results_ARIMA.fittedvalues - ts_log_diff)**2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculated Targets (Predictions)**    \n"
      ],
      "metadata": {
        "id": "bmQmbVC8Sllb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oC7wn1kGbZG"
      },
      "outputs": [],
      "source": [
        "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy = True)\n",
        "print(predictions_ARIMA_diff.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYjotHB9GbZG"
      },
      "outputs": [],
      "source": [
        "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
        "print(predictions_ARIMA_diff_cumsum.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7cIogk8GbZG"
      },
      "outputs": [],
      "source": [
        "predictions_ARIMA_log = pd.Series(ts_log.iloc[0], index = ts_log.index)\n",
        "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value = 0)\n",
        "predictions_ARIMA_log.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT20V3nnGbZH"
      },
      "outputs": [],
      "source": [
        "predictions_ARIMA = np.exp(predictions_ARIMA_log)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(ts)\n",
        "plt.plot(predictions_ARIMA)\n",
        "plt.title('RMSE: %.4f'% np.sqrt(sum((predictions_ARIMA - ts)**2)/len(ts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forecasting with 95% Confidence**    \n"
      ],
      "metadata": {
        "id": "B_4TiO17S3Bn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrmsPStFGbZH"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20,5))\n",
        "fig = results_ARIMA.plot_predict(start='1959-01-01', end='1964-01-01',ax=ax)\n",
        "legend = ax.legend(loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diP4SfyOGbZH"
      },
      "source": [
        "###Forecast for next 12 months"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use ARIMA to forecast 12 periods and raise values using `np.exp()` then populate a dataframe (`prediction_df`) with those forecasted values.    \n"
      ],
      "metadata": {
        "id": "vqIAueu9TSdG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VSReIU0GbZH"
      },
      "outputs": [],
      "source": [
        "results = results_ARIMA.forecast(steps = 12)\n",
        "converted_results = [(np.exp(x)) for x in [i for i in results]]\n",
        "prediction_df = pd.DataFrame(converted_results)\n",
        "prediction_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Eq8LZusUT4Ct"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}