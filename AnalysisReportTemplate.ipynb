{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnalysisReportTemplate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorPatrickSlatraigh/CST3512/blob/main/AnalysisReportTemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis Report Template    \n",
        "\n",
        "\n",
        "**A shared resource for the organization of data science analysis and report presentation.**\n",
        "\n",
        "Prepared by CUNY CityTech CST3512 class, Spring 2022.   \n",
        "\n"
      ],
      "metadata": {
        "id": "Ar1dlppwcrPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OUTLINE**    \n",
        "\n",
        "* Executive Summary   \n",
        "* Background   \n",
        "* Problem Statement   \n",
        "* Approach    \n",
        "* Findings    \n",
        "* Model/Analysis Design   \n",
        "* One or Themes (deeper findings)    \n",
        "* Implications (so what?)     \n",
        "* Recommendation(s)   \n",
        "* Next Steps (implement/additional-research)   \n",
        "\n",
        "\n",
        "* APPENDICES    \n",
        "1.   Footnotes    \n",
        "2.   Glossary of terms    \n",
        "3.   Additional reading   \n"
      ],
      "metadata": {
        "id": "0PqttnLniYOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Executive Summary    \n",
        "\n"
      ],
      "metadata": {
        "id": "WFcvmzu84RLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Background    \n",
        "\n"
      ],
      "metadata": {
        "id": "SVGcavwo4RH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem Statement    \n",
        "\n"
      ],
      "metadata": {
        "id": "pba2uEEx4Q_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Approach    \n",
        "\n"
      ],
      "metadata": {
        "id": "jokE8gKI4Q4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Findings    \n"
      ],
      "metadata": {
        "id": "9Mlks0rJ4QxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model/Design Analysis    \n",
        "\n"
      ],
      "metadata": {
        "id": "PVKQGKjN4QqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Themes    \n",
        "\n"
      ],
      "metadata": {
        "id": "SMJqnnHq4QTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implications    \n",
        "\n"
      ],
      "metadata": {
        "id": "Wyy9CAVY48-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Recommendations    \n"
      ],
      "metadata": {
        "id": "Yn8HoXIp5AMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Next Steps    \n"
      ],
      "metadata": {
        "id": "GOh4OKlz5Ewa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgSofQ0DR9_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix 1 - Footnotes    \n"
      ],
      "metadata": {
        "id": "0I48gB7t5IhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "aWKvP93UR7df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix 2 - Glossary of Terms    \n"
      ],
      "metadata": {
        "id": "J05taHqH5d-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1PBy-yNbR6qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix 3 - Additional Reading    \n"
      ],
      "metadata": {
        "id": "BbO9vRS75hfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q9r2AzaojPc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix 4 - RESOURCES - Data Sources to Consider   \n",
        "\n",
        "* **Kaggle** - Data sources across disciplines, time periods, and geographies\n",
        "* **UCI** -- University of California - Irvine, hundreds of ML datasets\n",
        "* **OWID** - Our World in Data\n",
        "* **United Nations** -- Many global datasets\n",
        "* **Project Gutenberg** - Open-source text of works of literature\n",
        "* **Data.gov** - A variety of sources of data on government measures    \n",
        "* **U.S. Bureau of Labor Statistics (BLS)** -- Labor and employment data in the United States    \n",
        "* **NYC Open Data** - Multiple data sources from New York City Government\n",
        "* **FBI Uniform Crime Reporting (UCR)** -- Aggregated law enforcement data from across the United States\n",
        "* **Yahoo Finance** - Time-series market data and a variety of data sources about public companies\n",
        "* **EDGAR** - Security and Exchange Commission (SEC) data of corporate filings    \n",
        "* **BBB** - Better Business Bureau provides several APIs to research businesses, customer reviews, and complaints     \n",
        "* **Twitter** - Social media postings and data from Twitter   \n",
        "* **Scholarly Research** - from CUNY library services   \n",
        "* **DataWorld** - data aggregator and tool platform (free and paid)    \n",
        "* **Statista** - data aggregator (paid, not free)      \n",
        "* **Health Data** - Various data sources\n",
        "* **Weather** - Various data sources    \n",
        "* **Geolocation** - Data sources on Named Entity : Longitude/Lattitude maps, and other location-specific data   \n",
        "* **Climate** -- Global Carbon Project and other sources for climate data     \n",
        "\n",
        "\n",
        "*note: **Google** curates a [catalog of open data sources](google.com/publicdata/directory)*\n"
      ],
      "metadata": {
        "id": "HmsMn9RCjQgC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Kaggle     \n",
        "\n",
        "The following dataset contains five years of data regarding Netflix's stock prices. Ranging from February 5th, 2018 - February 5th, 2022.\n",
        "\n",
        "https://www.kaggle.com/datasets/jainilcoder/netflix-stock-price-prediction"
      ],
      "metadata": {
        "id": "UskbroIclq-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###UCI - University of California Irvine    \n",
        "\n",
        "[UCI Machine Learning Repository](https://archive-beta.ics.uci.edu/) is a popular platform for finding datasets for any data science project. Just like Kaggle, you can search for any dataset here by searching for your project topic. You will also find popular and new datasets here that you can use to practice as a beginner in data science.    \n"
      ],
      "metadata": {
        "id": "5hdPjaqv95iS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Our World in Data (OWID)    \n",
        "\n",
        "\n",
        "See the OWID Frequently Asked Questions at: https://ourworldindata.org/faqs    \n"
      ],
      "metadata": {
        "id": "-T6hey5olyjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###United Nations (UNdata)       \n",
        "\n",
        "\n",
        "\n",
        "**UNdata API** provides dynamic, programmatic access to data within the UNdata platform. Developers can use the API to dynamically query UNdata to obtain the latest data and display the result on a Web page, download to local storage for further processing, etc.\n",
        "\n",
        "**UNdata API** is powered by Eurostat’s SDMX Reference Infrastructure (SDMX-RI). The API is implemented as a REST and SOAP Web Service that can be used to query the datamarts using the SDMX standard. The API is governed by UNdata Terms of Use.\n",
        "\n",
        "**How To Use UNdata API**\n",
        "If you want to query data from UNData API we have two options for you. You can use REST API or the SOAP web service. In any of these cases it is recommended that you get familiarized with the main SDMX artifacts such as DataFlow, Codelist, Agency, Structure, etc.     \n",
        "\n",
        "\n",
        "Details at:  http://data.un.org/Host.aspx?Content=API    \n",
        "\n"
      ],
      "metadata": {
        "id": "AXetTnlKwY03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Project Gutenberg     \n",
        "\n",
        "To search for books with available text: https://www.gutenberg.org/ebooks/ \n",
        "\n",
        "See [Jonathan Reeve's site](https://jonreeve.com/2017/06/project-gutenberg-the-database/) for information about text mining using Project Gutenberg.  \n"
      ],
      "metadata": {
        "id": "xP8m-_Nul-I-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data.gov    \n",
        "\n",
        "https://data.gov       \n",
        "\n"
      ],
      "metadata": {
        "id": "f6pRUqRNmG6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###U.S. Bureau of Labor Statistics (BLS)    \n",
        "\n",
        "The Bureau of Labor Statistics' (**BLS**) Public Data Application Programming Interface (**API**) gives the public access to economic data from all **BLS** programs. It is the Bureau's hope that talented developers and programmers will use the **BLS** Public Data API to create original, inventive applications with published **BLS** data.\n",
        "\n",
        "The **BLS** Public Data API is currently available in two versions. Version 2.0 requires registration and allows users to access more data more frequently. Users may add calculations and annual averages to requests, and series description information is available for many **BLS** surveys. Version 1.0 is a more limited API that does not require registration and is open for public use.\n",
        "\n",
        "Using **BLS** API Signatures, developers and programmers can retrieve published historical timeseries data in JSON data-interchange or XLSX format. The **BLS** Public API utilizes two HTTP request-response mechanisms to retrieve data: GET and POST. GET requests data from a specified source. POST submits data to a specified resource to be processed. The BLS Public Data API uses GET to request a single piece of information and POST for all other requests. \n",
        "\n",
        "*note: more online at https://www.bls.gov/*"
      ],
      "metadata": {
        "id": "O726wlw5wftc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NYC Open Data    \n",
        "\n",
        "*accessible with Socrata*\n",
        "\n",
        "\n",
        "Example - 311 Service Requests from 2010 to Present :\n",
        "\n",
        "https://dev.socrata.com/foundry/data.cityofnewyork.us/erm2-nwe9 \n",
        "\n",
        "Socrata Developer's Account :\n",
        "\n",
        "https://dev.socrata.com/\n",
        "\n",
        "Accessing NYC Open Data on LVNGD Blog*\n",
        "\n",
        "https://lvngd.com/blog/accessing-nyc-open-data-with-python-and-the-socrata-open-data-api/\n",
        "\n",
        "*note: the LVNGD blog is outdated with respect to some small details but presents the concepts clearly.*     \n",
        " \n"
      ],
      "metadata": {
        "id": "OzgpRLetmNC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FBI Uniform Crime Reporting (UCR)    \n",
        "\n",
        "Aggregate data from law enforcement across the United States at: https://www.fbi.gov/services/cjis/ucr    \n"
      ],
      "metadata": {
        "id": "qVPmeZkUvH7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yahoo Finance       \n",
        "\n",
        "\n",
        "Real time low latency Yahoo Finance API for stock market, crypto currencies, and currency exchange.\n",
        "\n",
        "Nearly a dozen sources for different metrics and data related to financial markets at: https://www.yahoofinanceapi.com/ \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CMqlhKOmbGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EDGAR (U.S. SEC)    \n",
        "\n",
        "place SEC Edgar info and links in this section    \n"
      ],
      "metadata": {
        "id": "is50vYoPmmFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Better Business Bureau (BBB)    \n",
        "\n",
        "Provides information on businesses in the U.S. at [several APIs](https://apievangelist.com/2016/08/04/thinking-about-the-better-business-bureau-api-in-context-of-the-overall-api-economy/), including:    \n",
        "\n",
        "* **[Organization Search](https://developers.bbb.org/documentation/endpoints/organization-search/)** - This endpoint will allow you to search for Organizations that are known to the BBB.  Organizations may include businesses or charities.    \n",
        "* **[Organization Collections](https://developers.bbb.org/documentation/endpoints/organization-collections/)** - BBB Partners will need to be able to identify collections of Organizations that they care about and be able to manage those collections and receive Organization info and updates on them as needed.     \n",
        "* **[Bulk Retrieval](https://developers.bbb.org/documentation/endpoints/bulk-retrieval/)** - This section defines the message specification that will be sent back to the API partners who request a bulk data transfer.     \n",
        "* **[Push Notifications](https://developers.bbb.org/documentation/endpoints/push-notifications/)** - BBB Partners have the need to be notified when BBB data updates occur for the records or a collection of records they are currently subscribed to.     \n",
        "* **[BBB Investigation/Application for Accreditation](https://developers.bbb.org/documentation/endpoints/bbb-investigation-application-for-accreditation/)** - This endpoint allows submission of organizations for BBB investigation and application for BBB accreditation.     \n",
        "\n"
      ],
      "metadata": {
        "id": "L7B2vCbxpX1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Twitter    \n",
        "\n",
        "Twitter APIs handle enormous amounts of data. \n",
        "\n",
        "The Twitter ensures this data is secured for developers and users alike is through authentication. \n",
        "\n",
        "There are a few methods for authentication:\n",
        "\n",
        "*   OAuth 1.0a User Context\n",
        "*   OAuth 2.0 Bearer Token\n",
        "*   Basic Authentication\n",
        "\n",
        "\n",
        "https://developer.twitter.com/en/docs/authentication/overview \n",
        "\n",
        "https://developer.twitter.com/en/docs/apps/overview \n",
        "\n",
        "https://developer.twitter.com/en/docs/twitter-api \n"
      ],
      "metadata": {
        "id": "m8KaWsB3QWfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Scholarly Research\n",
        "\n",
        "*from CUNY Mina Rees Library*\n",
        "\n",
        "\n",
        "See: https://libguides.gc.cuny.edu/c.php?g=405353&p=4857784 \n",
        "\n",
        "*...or search for 'CUNY, Library Guides, Digital Tools and Techniques, Scholarly Research APIs'*    \n"
      ],
      "metadata": {
        "id": "1FDZi8qkRGVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data World    \n",
        "\n",
        "Data aggregator and tool provide with (free) limited Community Edition, (paid) Professional Edition, and (paid) Enterprise Plans at:  [data.world](https://data.world/pricing)"
      ],
      "metadata": {
        "id": "Vv_ZMmzENlWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Statista     \n",
        "\n",
        "Paid service for customized research data and online commerce data at: [statisa.com.](https://www.statista.com)    \n",
        "\n"
      ],
      "metadata": {
        "id": "lDgAIU1bMSew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Health Data**    \n",
        "\n"
      ],
      "metadata": {
        "id": "1TBdrxQiVhd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Human Mortality Database   \n",
        "\n",
        "See the information online at: https://www.mortality.org/"
      ],
      "metadata": {
        "id": "FEkfZqOVVlPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Weather \n",
        "\n",
        "1.    [OpenWeatherMap](https://openweathermap.org/weathermap) - Dynamic weather maps.    \n",
        "OpenWeatherMap for data about the weather. [Documentation](http://openweathermap.org/current#geo).     \n",
        "\n",
        "OpenWeatherMap contains parameters as part of the URL, including an `appid` which is a key that is used to limit the number of calls that can be issued by a single application. \n",
        "\n",
        "*(Note: The `%20` is a transformation for the space (` `) character in URLs.)*\n",
        "\n",
        "[**You will need to get an API key**](https://openweathermap.org/price)    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BaySwbV-5453"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2.    [NOAA's Geoplatform](https://www.climate.gov/maps-data/all) - Data, maps, analytics on weather for U.S. National Oceanographic and Atmospheric Administration.     \n"
      ],
      "metadata": {
        "id": "mc0SugpcS_bs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Google Catalog of Open Data    \n",
        "\n",
        "place Google Catalog of Open Data info and links here.\n",
        "\n",
        "See `google.com/publicdata/directory` for the home page."
      ],
      "metadata": {
        "id": "iv4rE81xmwC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DataWorld.com    \n",
        "\n",
        "[Data.World](https://data.world) is a collection of available data sources for research and exercises.    \n"
      ],
      "metadata": {
        "id": "RCc1YgVQbEvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Geo-Location APIs**      "
      ],
      "metadata": {
        "id": "QA4tll5iRv7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###geoIP Resolution    \n",
        "\n",
        "**geoIP resolution** - takes the IP of a device and returns back its location.\n",
        "\n",
        "You will need to get an API here from here: https://ipstack.com/    \n"
      ],
      "metadata": {
        "id": "xnJGW5zHR1hu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Climate**    \n",
        "\n"
      ],
      "metadata": {
        "id": "qJKBjgiEVN-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Global Carbon Project     \n",
        "\n",
        "See the catalog of Global Carbon Project data sources at: https://www.globalcarbonproject.org/products/internetresources.htm#Data    \n"
      ],
      "metadata": {
        "id": "OB4kGFOsVRKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Appendix 5 - Data Discovery Approach"
      ],
      "metadata": {
        "id": "OJ5RDAm6SBXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each problem and the data sources considered will raise unique considerations but there are several typical steps to data extraction, discovery, and transformation which are helpful to consider.    \n",
        "\n",
        "The following is derived from the work of Anmol Tomar in CodeX, entitled \"[Every Data Analysis in 10 steps!  Adding structure to your data analysis!](https://medium.com/codex/every-data-analysis-in-10-steps-960dc7e7f00b)\"\n",
        "\n",
        "**This notebook extends the ten (10) steps in the original article by Anmol Tomar into 13 steps (to include steps related to duplicate rows and the transformation/wrangling of data elements.**    \n"
      ],
      "metadata": {
        "id": "UGMvsEJYSHV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###File Upload"
      ],
      "metadata": {
        "id": "pygOTl3wWVxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMDB Dataset**    \n",
        "\n",
        "For  illustration purposes, this analysis uses the Kaggle IMDB dataset for the top 1000 movies to understand the features/traits of top IMDB movies by applying a 10 step-process.    *The Kaggle file used in this notebook differs from the source file used by Anmol Tomar in the article referenced above.*  \n",
        "\n",
        "A copy of the file is hosted on Professor Patrick's GitHub and can be accsed with `!curl` to upload a copy to the `content` folder in Colab.    \n"
      ],
      "metadata": {
        "id": "T_eWdtgSS3Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl \"https://raw.githubusercontent.com/ProfessorPatrickSlatraigh/data/main/IMDB_top_1000.csv\" -o imdb_top_1000.csv"
      ],
      "metadata": {
        "id": "E0WhUYu4Vvxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Packages    \n",
        "\n",
        "The load, discovery, and transformation steps will only require that the `pandas` and `matplotlib` packages be imported.    \n",
        "\n",
        "To add functionality to data tables in Colab, import `data_table` from `google.colab`."
      ],
      "metadata": {
        "id": "6UWCV11SWS7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import our packages \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "BrWmty-JTQaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Read the Data into a Dataframe    \n"
      ],
      "metadata": {
        "id": "tmcsf6BMWsor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data\n",
        "df_movies =  pd.read_csv('imdb_top_1000.csv')"
      ],
      "metadata": {
        "id": "A3uuHsALW40Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Summarize the Columns    \n",
        "\n"
      ],
      "metadata": {
        "id": "14FSXQL0W8Q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary of the columns   \n",
        "df_movies.describe(include = 'all')"
      ],
      "metadata": {
        "id": "zAJV5RJwXA8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A detailed pivot table can be viewed as well to peruse the data.    "
      ],
      "metadata": {
        "id": "2ybNwMiiZAl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# query the detail in the dataframe\n",
        "df_movies"
      ],
      "metadata": {
        "id": "vAVYKKHIYmU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Data Types    \n",
        "\n",
        "The next step is to do a sanity check of the data types of the columns of the dataframe. If there are some incorrect data types they can be corrected in this step."
      ],
      "metadata": {
        "id": "1kwBCvrgYHAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the datatypes \n",
        "print(df_movies.dtypes)"
      ],
      "metadata": {
        "id": "VTv3ecNnZhgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The field `Duration` is a string (object data type) with the runtime in minutes followed by the term 'min'.  A new column can be generated for `runtime` as a float by assigning the result of a split of `Duration` and the transformation of the first element in the resulting list into a float. "
      ],
      "metadata": {
        "id": "nsJ6aaA-b3OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the numeric value of movie Duration to a new column Runtime \n",
        "df_movies[['Runtime', 'Unit']] = df_movies[\"Duration\"].str.split(' ', 1, expand=True)\n",
        "df_movies['Runtime'] = df_movies['Runtime'].astype(int)\n",
        "del df_movies['Unit']"
      ],
      "metadata": {
        "id": "9MS8rNJFc3zF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the dataframe after the transformation   \n",
        "df_movies"
      ],
      "metadata": {
        "id": "G6WPznU5djF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another approach would simply replace the `Duration` column in place with it's float value\n",
        "# Transformting `Duration` into int \n",
        "\n",
        "# >>> Remove the comment from the following line of code to try it\n",
        "# df_movies['Duration'] = df_movies['Duration'].str.replace(' min','').astype(int)\n"
      ],
      "metadata": {
        "id": "dmfXl_qwgHYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Review the data types of each column again\n",
        "df_movies.dtypes"
      ],
      "metadata": {
        "id": "AtyXzTYegyxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Find Missing Values    \n",
        "\n",
        "The next step is to find the number of missing values across the columns of the dataframe. It’s important to understand the count of nulls so determine how best to treat them.    "
      ],
      "metadata": {
        "id": "DIKYq0WGhBYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find nulls or missing values\n",
        "df_movies.isnull().sum()"
      ],
      "metadata": {
        "id": "UNaeaIcmhTbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Missing Values Treatment    \n",
        "\n",
        "Using the count of missing values and any other descriptive statistics applicable, the next step is to treat the columns with missing values.    \n",
        "\n",
        "For illustration purposes, the nulls are filled with the mean value of the columns, although there are more sophisticated methods of missing value treatment.   \n"
      ],
      "metadata": {
        "id": "9PUaQFA-h5Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing nulls with mean for numeric values and mode for categorical values\n",
        "\n",
        "df_movies['Metascore'].fillna(df_movies['Metascore'].mean())\n",
        "\n",
        "df_movies['Certificate'].fillna(df_movies['Certificate'].mode())\n"
      ],
      "metadata": {
        "id": "Qh1_gHzwh7ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies.head()\n"
      ],
      "metadata": {
        "id": "x_f_BvBoinYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Find and Count Duplicates"
      ],
      "metadata": {
        "id": "X1IQZ8-WL6UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step requires some context and/or intuition regarding the data.  On first inspection, there may be data which appears to be redundant but is actually categorical data.  Most data sources will not be normalized so this is often the case.  Later in this workbook there is an exercise on Transformations and Feature Extraction which discusses the fact that some categorical data may have data entry or other errors which require their own special treatment.    \n",
        "\n",
        "Finding duplicates and wrangingling categorical data can sometimes be an iterative process of discovery, trial and error, then treatement.    \n",
        "\n",
        "The initial activity in dealing with duplicates includes:    \n",
        "\n",
        "1. Finding duplicates    \n",
        "2. Counting duplicates    \n",
        "3. Displating duplicate rows with `loc`\n",
        "3. Determing duplicates treatment     \n",
        "\n",
        "The following snippets of code give examples of 1 and 2 above:    \n",
        "\n"
      ],
      "metadata": {
        "id": "XjZxpavOT8Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding duplicates across the dataframe   \n",
        "# Use the `.duplicated()` method\n",
        "df_movies.duplicated()"
      ],
      "metadata": {
        "id": "Wr639zzkVHD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To consider certain columns such as `Title` for identifying duplicates\n",
        "df_movies.duplicated(subset=['Title'])"
      ],
      "metadata": {
        "id": "i4qvsqJRhtjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas `duplicated()` returns a boolean Series. However, it is not practical to see a list of True and False when we need to perform some data analysis. The Pandas `loc` data selector can be used to extract those duplicate rows:"
      ],
      "metadata": {
        "id": "sg4ofFvbSg_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of `loc` allows us to see the rows that were identified by duplicated()\n",
        "df_movies.loc[df_movies.duplicated(), :]"
      ],
      "metadata": {
        "id": "gCwtLOcKSm9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Duplicates Treatment"
      ],
      "metadata": {
        "id": "0jQC4ajhL_J5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fundamental concepts in duplicate treament to consider:\n",
        "\n",
        "1. Determing duplicates to retain with `keep`    \n",
        "2. Dropping duplicates      \n"
      ],
      "metadata": {
        "id": "zGTr2QPESP4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retain duplicates with `keep`**\n",
        "\n",
        "There is an argument `keep` in Pandas duplicated() to determine which duplicates to mark. `keep` defaults to 'first', which means the first occurrence gets kept, and all others get identified as duplicates.   \n"
      ],
      "metadata": {
        "id": "uLCoY_3-Talt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `keep` defaults to 'first'\n",
        "df_movies.loc[df_movies.duplicated(keep='first'), :]"
      ],
      "metadata": {
        "id": "jFuho0LmTgZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An argument can be used to change it to 'last' keep the last occurrence and mark all others as duplicates.  "
      ],
      "metadata": {
        "id": "Ilusn23QTugZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `keep` defaults to 'last'\n",
        "df_movies.loc[df_movies.duplicated(keep='last'), :]"
      ],
      "metadata": {
        "id": "Bi0OlssgTmqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a third option we can use `keep=False`. It marks all duplicates as True and allows viewing all duplicate rows."
      ],
      "metadata": {
        "id": "KhZ9_HaRUA2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There is a third option we can use 'keep=False'. It marks all duplicates as True\n",
        "df_movies.loc[df_movies.duplicated(keep=False), :]"
      ],
      "metadata": {
        "id": "EPCb8mvJUFcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop duplicates**    \n",
        "\n",
        "The Pandas built-in method `drop_duplicates()` will drop duplicate rows."
      ],
      "metadata": {
        "id": "UcXHHiYcT1SB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the change in the number of rows before and after dropping duplicates\n",
        "# It is not performed in place by default,\n",
        "df_movies.drop_duplicates()"
      ],
      "metadata": {
        "id": "AEcFgCDAUaHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, this method returns a new DataFrame with duplicate rows removed. We can set the argument inplace=True to remove duplicates from the original DataFrame."
      ],
      "metadata": {
        "id": "VoALZWAwUjSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note the change in the number of rows before and after dropping duplicates\n",
        "# It is not performed in place by default, it can be changed it to in place by `inplace=True`df.drop_duplicates(inplace=True)\n",
        "df_movies.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "dkPHoQ_5UwJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The argument keep can be set for `drop_duplicates()` as well to determine which duplicates to keep. It defaults to `'first'` to keep the first occurrence and drop all other duplicates.    \n",
        "\n",
        "\n",
        "Similarly, `drop_duplicates` can be set `keep` to 'last' to keep the last occurrence and drop other duplicates."
      ],
      "metadata": {
        "id": "yxKkDWLzVAR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use keep='last' to keep the last occurrence \n",
        "df_movies.drop_duplicates(keep='last')"
      ],
      "metadata": {
        "id": "k0FPmfomVJJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`drop_duplicates` can also be set with `keep` assigned the value **False** to drop all duplicates."
      ],
      "metadata": {
        "id": "WodkptrgVS4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To drop all duplicates\n",
        "df_movies.drop_duplicates(keep=False)"
      ],
      "metadata": {
        "id": "KQKTRHVMVdBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considering certain columns for dropping duplicates**    \n",
        "\n",
        "\n",
        "Similarly, to consider certain columns for dropping duplicates, pass a list of columns to the argument subset:    \n"
      ],
      "metadata": {
        "id": "ZFWw-DjdVigP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, we can consider a certain columns for dropping duplicates\n",
        "df_movies.drop_duplicates(subset=['Title'])"
      ],
      "metadata": {
        "id": "pn_f7CMhVoy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and do that in-place."
      ],
      "metadata": {
        "id": "V53p9fLfV1tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider a certain columns for dropping duplicates in-place\n",
        "df_movies.drop_duplicates(subset=['Title'], inplace=True)"
      ],
      "metadata": {
        "id": "QV9yELBVV4fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoking the `duplicated()` method will confirm that duplicate rows have been dropped. "
      ],
      "metadata": {
        "id": "ZzXipNxKYDvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of `loc` allows us to see the rows that were identified by duplicated()\n",
        "df_movies.loc[df_movies.duplicated(), :]"
      ],
      "metadata": {
        "id": "ngJDLufzYLJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Analyze for Outliers    \n",
        "\n",
        "The next step is to check for outliers. There are multiple ways of checking the outliers, the graphical method vis presented here. Two continuous variables (`Metascore` and `Runtime`) have been select to be checked for outliers by evaluating a histogram for each column.    "
      ],
      "metadata": {
        "id": "c7CRu70-jb2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Metascores \n",
        "plt.hist(df_movies['Metascore'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_3D4aRxZjdAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the minimum `Metascore` \n",
        "df_movies['Metascore'].min()\n",
        "# output : ?"
      ],
      "metadata": {
        "id": "8Qzn8BwhnvOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Runtimes \n",
        "plt.hist(df_movies['Runtime'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bWG3ilD3jwKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the minimum `Runtime` \n",
        "df_movies['Runtime'].min()\n",
        "# output : ?"
      ],
      "metadata": {
        "id": "k7EDome8nzYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the maximum `Runtime` \n",
        "df_movies['Runtime'].max()\n",
        "# output : ?"
      ],
      "metadata": {
        "id": "6DNwh1Ron_33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Outlier Treatment\n",
        "\n",
        "\n",
        "The next step is to treat the outliers observed in the previous step. There are different ways of treating the outliers such as:     \n",
        "1. Capping the minimum and maximum value limits\n",
        "2. Removing the rows with outlier values   \n",
        "\n",
        "\n",
        "Although there is nothing off with the distribution of Metascores, for illustration purposes, the minimum `Metascore` value is capped at 65.    \n",
        "\n",
        "\n",
        "And with respect to the Runtimes, any rows with a `Runtime` in excess of 200 is deleted as are rows with a `Runtime` of less than 60.   \n",
        "\n"
      ],
      "metadata": {
        "id": "IBl-AtQFkAai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Capping the minimum Metascore to 65\n",
        "df_movies.loc[df_movies['Metascore'] < 65,'Metascore'] = 65\n",
        "\n",
        "#check the minimum `Metascore` \n",
        "df_movies['Metascore'].min()\n",
        "# output : 65.0"
      ],
      "metadata": {
        "id": "-kJvPiczkuxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The range of `Metascore` values can be evaluated again with a histogram.    \n"
      ],
      "metadata": {
        "id": "XERinlJYYjyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Metascores \n",
        "plt.hist(df_movies['Metascore'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BEtw9G_ZYu39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drop runtimes of less than 60 or greater than 200.    \n",
        "\n",
        "*note the use of the* `|` *set operation to combine the results of each conditional test.*"
      ],
      "metadata": {
        "id": "Mb-AsVTXY0Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping any rows where Runtime exceeeds 200 or is below 60\n",
        "df_movies.drop(df_movies[(df_movies.Runtime > 200) | (df_movies.Runtime < 60)].index, inplace=True)  \n"
      ],
      "metadata": {
        "id": "W9ielgEJl8GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the minimum `Runtime` \n",
        "df_movies['Runtime'].min()\n",
        "# output : 60"
      ],
      "metadata": {
        "id": "6pIR_4fZoIOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the maximum `Runtime` \n",
        "df_movies['Runtime'].max()\n",
        "# output : 200?"
      ],
      "metadata": {
        "id": "NgWLv7nyoTij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of Runtimes \n",
        "plt.hist(df_movies['Runtime'],bins = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4Q9UAspZmXvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9. Ask 'Who?'    \n",
        "\n",
        "\n",
        "The next step to answer are questions related to people, members, stakeholders, etc. \n",
        "\n",
        "In films, there are actors, directors, and cast members.  Some 'who' questions to raise could include:  \n",
        "\n",
        "* Who has directed the most number of top IMDB movies? (univariate)    \n",
        "\n",
        "\n",
        "* Who has acted in most top IMDB movies? (univariate)    \n",
        "\n",
        "\n",
        "* Which Actor-Director combination has the most top IMDB movies? (bivariate)    \n",
        "\n",
        "\n",
        "* Who provided the most music in top IMDB movies ? (Data not available)    \n",
        "\n",
        "\n",
        "And More …    \n",
        "\n"
      ],
      "metadata": {
        "id": "YXncGAO6pfLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*to use the code in the following snippet, extraction of data values from the `Cast` column would be required as a preliminary transformation.*\n"
      ],
      "metadata": {
        "id": "lQwa4C8wquzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### >>> The sample code here relies on columns in the original dataset used by Anmol Tomar\n",
        "### >>> The columns required by the code in this snippet are not available in the Kaggle dataset\n",
        "\n",
        "## Who has directed the most number of top IMDB movies ?\n",
        "# df_movies.groupby(['Director']).agg({'Series_Title':'count'}).reset_index().rename(columns = {'Series_Title':'count'}).\\\n",
        "# sort_values('count',ascending = False).head(5)\n",
        "\n",
        "## Who has acted in the most number of top movies \n",
        "# df_movies.groupby(['Star1']).agg({'Series_Title':'count'}).reset_index().rename(columns = {'Series_Title':'count'}).\\\n",
        "# sort_values('count',ascending = False).head(5)\n",
        "\n",
        "## Director - Actor works best \n",
        "# df_movies.groupby(['Director','Star1'])['Series_Title'].count().reset_index().\\\n",
        "# rename(columns = {'Series_Title':'Count'}).sort_values('Count',ascending = False).head(5)    \n"
      ],
      "metadata": {
        "id": "bPSe912Mpxvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10. Ask 'When?'    \n",
        "\n",
        "\n",
        "The next step is to answer questions related to the time dimension (year, quarter, month, week, day, time-of-day, hour, minute, etc.)    \n",
        "\n",
        "Considering film data, the following type of question could be asked:    \n",
        "\n",
        "\n",
        "* Find the years with most movies in IMDB top 1000 ? (univariate)    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aTen4oxarnHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*to use the code in the following snippet, extraction of data values from the `Title` column would be required as a preliminary transformation.*"
      ],
      "metadata": {
        "id": "VwE9jyYssYyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### >>> The sample code here relies on columns in the original dataset used by Anmol Tomar\n",
        "### >>> The columns required by the code in this snippet are not available in the Kaggle dataset\n",
        "\n",
        "## Finding years with most movies in top 1000\n",
        "# year_dis = df_movies.groupby('Released_Year')['Series_Title'].count().reset_index().\\\n",
        "# rename(columns = {'Series_Title':'Count'}).sort_values('Count',ascending = False).head(10)\n",
        "\n",
        "# plt.bar(year_dis['Released_Year'].astype(str), year_dis['Count'], width = 0.5)\n",
        "# plt.xlabel('Years')\n",
        "# plt.ylabel('Number of Movies')\n",
        "# plt.title('Years with most movies in IMDB top 1000')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "3Po6QyiMqgdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11. Ask 'Where?'    \n",
        "\n",
        "The next step is to look at the things from the location or place perspective, for example, country, state, regions etc.  Geolocation can also be used in determing the 'where' attribute of data. For location features a variety of data elements may be used, including:    \n",
        "\n",
        "* Formal country, region, jurisdiction decriptors (see ISO codes)    \n",
        "\n",
        "* Postal codes\n",
        "\n",
        "* Longitude and Lattitude (GPS coordinates) \n",
        ">o may be express as degrees(°), minutes('), seconds(\")    \n",
        ">o may be expressed as decimal values (which can be translated to °, ', \")  \n",
        "> *note that there are 360° degrees around the earth, each minute' is 1/60th of a degree, and each second\" is 1/60th of a minute -- akin to minutes and seconds as fractions of hours when measuring time* \n",
        "\n",
        "* Mappings    \n",
        ">o phone number country codes and areas codes may generally map to location    \n",
        ">o IP address may map to location (consider VPN impacts)    \n",
        "\n",
        "\n",
        "For film data, the following question could be posed:    \n",
        "\n",
        "\n",
        "* Find countries with most movies in IMDB top 1000.\n",
        "\n",
        "\n",
        "The dataset does not have the data to answer this question.   \n",
        "Research should be as exhaustive as possible and not limited based on data availability.  Additional sources or enrichment approaches should be considered to answer pertinent questions.    \n",
        "\n"
      ],
      "metadata": {
        "id": "adAccQcNsdgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12. Ask 'What?  Which?  How?'    \n",
        "\n",
        "\n",
        "The next and final step is formulating questions about aspects not covered in the first nine steps. These questions are not related to people, place, or time but everything apart from these. Formulating such questions can be quite subjective and takes some time and experience to develop intuition and a facility.\n",
        "\n",
        "With respect to film data, such question might include:    \n",
        "\n",
        "* Which genres are featured most in the top 1000?\n",
        "\n",
        "\n",
        "* What is the duration of the top movies?\n",
        "\n",
        "\n",
        "* What is the correlation between the rating and gross earning?\n",
        "\n",
        "\n",
        "and more…\n",
        "\n",
        "For illustration purposes, the first question is approached using the following code:\n",
        "\n"
      ],
      "metadata": {
        "id": "Iz0se6PdtOw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Which genres of film are featured most in top 1000 ? \n",
        "genre_dis = df_movies.groupby('Genre')['Title'].count().reset_index().\\\n",
        "rename(columns = {'Title':'Count'}).sort_values('Count',ascending = False).head(5)\n",
        "fig, ax = plt.subplots()\n",
        "plt.bar(genre_dis['Genre'], genre_dis['Count'], width = 0.5)\n",
        "plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qyzK4zcEuK32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mEitLngMuq3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can refine the exercises in this appendix through the inclusion of the requisite transformations to wrangle the data required for additional analysis.   \n"
      ],
      "metadata": {
        "id": "2QnNfKqKueQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "z6yO_9P9li_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###13. Transformations and Features Extraction    \n",
        "\n",
        "While some data may need to be sourced from additional research, there can be data embedded in compound/complex data types which through a process of analysis, and transformation may be extracted as useful features.  This process can utilize different tools and techniques, including:    \n",
        "\n",
        "* **Regex** (Regular Expressions)    \n",
        "\n",
        "* **NLP** (Natural Language Processing)    \n",
        "\n",
        "* **Image** Detection (for image blobs)  \n",
        "\n",
        "\n",
        "Once initial feature data is generated, it may need to be treated using the approach used for the raw data sourced, including:    \n",
        "\n",
        "*  Find and treat missing values    \n",
        "\n",
        "*  Find and treat outliers    \n",
        "\n",
        "*  Find and treat duplicates    \n",
        "\n",
        "*  Standardize/scrub categorical values (may be required to clean data)   \n",
        "\n"
      ],
      "metadata": {
        "id": "RdA1Lc3KPfqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6KcrFhvepRm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**EXERCISE**    \n",
        "\n",
        "\n",
        "Use regular expressions to derive the following features which can be used in the code provided by the original Anmol Tomar analysis, then execute the commented-out code to perform the analysis on the features extracted:    \n",
        "\n",
        "\n",
        "1. **Director** (used when asking 'Who?')    \n",
        "\n",
        "2. **Star1** (used when asking 'Who?')      \n",
        "\n",
        "3. **Released_Year**  (used when asking 'When?')  \n",
        "\n",
        "\n",
        "In addition, the following feature name should be changed in the original source code because the dataset used here refers to a feature `Title` not 'Series_Title`:    \n",
        "\n",
        "* **Series_Title** (should be `Title`) \n",
        "\n"
      ],
      "metadata": {
        "id": "9tWRA0MIPCWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ur_4EPb2TR9v"
      }
    }
  ]
}